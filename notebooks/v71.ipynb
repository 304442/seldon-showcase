{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Seldon Core 2: Advanced MLOps Platform Showcase üöÄ\n**Experience production-ready MLOps with Seldon Core 2's complete capabilities**\n\n## üåü **Why Seldon Core 2?**\nSeldon Core 2 is the **next-generation MLOps platform** that transforms how organizations deploy, manage, and scale machine learning models in production. Built for enterprise-grade workloads, it provides everything needed for a complete ML infrastructure.\n\n**üèÜ Industry Leading:**\n- Trusted by Fortune 500 companies for mission-critical ML workloads\n- Open-source with enterprise support and cloud-native architecture\n- CNCF Sandbox project with strong community and contributor base\n- Compatible with all major cloud providers and on-premises deployments\n\n## üéØ What You'll Experience\nThis showcase demonstrates Seldon Core 2's **four key value propositions** through a complete product classification system:\n\n### üîß **Flexibility** \nDeploy diverse models (transformers, classifiers) using Server and Model CRDs with efficient multi-model serving\n\n### üìã **Standardization**\nCreate ML pipelines with consistent CRDs and Open Inference Protocol V2 for unified model/pipeline interactions\n\n### üëÅÔ∏è **Observability** \nReal-time monitoring with Prometheus metrics and Grafana dashboards for comprehensive insights\n\n### ‚ö° **Optimization**\nSafe A/B testing with traffic splitting, multi-model serving efficiency, and production deployment strategies\n\n## üèóÔ∏è Architecture Overview\n**Complete MLOps Infrastructure in Action:**\n- **üîß Multi-Model Serving**: MLServer (5 replicas) + Triton (2 replicas) for diverse workloads\n- **ü§ñ ML Models**: Feature transformer + V1/V2 classifiers with shared resource optimization\n- **üîó Pipeline Orchestration**: End-to-end ML workflows with Kafka data flow and tensor mapping\n- **üß™ A/B Testing**: Safe model updates with 90/10 traffic splitting and real-time analysis\n- **üìä Monitoring**: Real-time metrics and comprehensive observability\n- **üåê Production Access**: Direct browser access to all services with external IP routing\n- **‚öñÔ∏è Load Balancing**: Intelligent request distribution with health checking and auto-scaling\n- **üîí Security**: mTLS encryption, RBAC integration, and audit trail compliance\n\n**Prerequisites**: Kubernetes cluster with Seldon Core 2 and monitoring stack installed\n\n**Note**: For advanced data science monitoring features (drift detection, explainability), see the separate `advanced_data_science_monitoring.ipynb` notebook.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport subprocess\nimport time\nimport requests\nimport numpy as np\nfrom IPython.display import display, Markdown, Code\nfrom typing import Optional, List, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nconfig = {\n    \"namespace\": \"seldon-mesh\",\n    \"gateway_ip\": None,\n    \"gateway_port\": \"80\"\n}\n\n# Track deployed resources\ndeployed = {\"servers\": [], \"models\": [], \"pipelines\": [], \"experiments\": []}\n\ndef run(cmd, timeout=30): \n    \"\"\"Execute command\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n        return result\n    except Exception as e:\n        return subprocess.CompletedProcess(cmd, 1, \"\", str(e))\n\ndef log(msg, level=\"INFO\"): \n    \"\"\"Simple logging\"\"\"\n    display(Markdown(f\"**{msg}**\"))\n\n# Get gateway configuration\nresult = run(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\nif result.returncode == 0 and result.stdout:\n    try:\n        svc_data = json.loads(result.stdout)\n        ingress = svc_data.get(\"status\", {}).get(\"loadBalancer\", {}).get(\"ingress\", [])\n        if ingress and ingress[0].get(\"ip\"):\n            config[\"gateway_ip\"] = ingress[0].get(\"ip\")\n            log(f\"Gateway IP: {config['gateway_ip']}\")\n    except:\n        config[\"gateway_ip\"] = \"localhost\"\n        \nlog(\"üöÄ Starting Seldon Core 2 MLOps Platform Showcase\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üîß Act 1: Flexibility - Multi-Model Deployment\n\n**Seldon Core 2's flexibility allows you to deploy diverse model types and serve multiple models efficiently on shared infrastructure.**\n\n## Key Features:\n- **Multi-Model Serving**: Deploy multiple models on shared servers\n- **Multiple Runtimes**: MLServer (Python/SKLearn), Triton (GPU/TensorRT)\n- **Smart Scheduling**: Intelligent model placement across server replicas\n- **Hot Swapping**: Update models without downtime\n\nLet's deploy servers and models to demonstrate this flexibility.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy servers for multi-model serving\nservers_config = {\n    \"mlserver\": 5,\n    \"triton\": 2\n}\n\nfor server_name, replica_count in servers_config.items():\n    server_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: {server_name}\n  namespace: {config['namespace']}\nspec:\n  replicas: {replica_count}\n  serverConfig: {server_name}\"\"\"\n    \n    with open(f\"{server_name}.yaml\", \"w\") as f: \n        f.write(server_yaml)\n    \n    result = run(f\"kubectl apply -f {server_name}.yaml\")\n    if result.returncode == 0:\n        deployed[\"servers\"].append(server_name)\n        log(f\"‚úÖ Deployed {server_name} server with {replica_count} replicas\")\n\nlog(f\"Servers deployed: {len(deployed['servers'])}\")"
  },
  {
   "cell_type": "code",
   "source": "# Deploy models\nmodels_config = [\n    {\n        \"name\": \"feature-transformer\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"replicas\": 2\n    },\n    {\n        \"name\": \"product-classifier-v1\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"replicas\": 3\n    },\n    {\n        \"name\": \"product-classifier-v2\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"replicas\": 3\n    }\n]\n\n# Deploy models\nfor model in models_config:\n    model_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: {model['name']}\n  namespace: {config['namespace']}\nspec:\n  storageUri: \"{model['uri']}\"\n  requirements:\n{chr(10).join(f'  - {req}' for req in model['requirements'])}\n  replicas: {model['replicas']}\n  server: {model['server']}\"\"\"\n    \n    with open(f\"{model['name']}.yaml\", \"w\") as f:\n        f.write(model_yaml)\n    \n    result = run(f\"kubectl apply -f {model['name']}.yaml\")\n    if result.returncode == 0:\n        deployed[\"models\"].append(model['name'])\n        log(f\"‚úÖ Deployed model: {model['name']}\")\n\nlog(f\"Models deployed: {len(deployed['models'])}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Test model inference\ndef test_inference(name, data):\n    url = f\"http://{config['gateway_ip']}:{config['gateway_port']}/v2/models/{name}/infer\"\n    payload = {\"inputs\": [{\"name\": \"predict\", \"shape\": [len(data), len(data[0])], \"datatype\": \"FP32\", \"data\": data}]}\n    headers = {\"Content-Type\": \"application/json\", \"Seldon-Model\": name}\n    \n    try:\n        response = requests.post(url, json=payload, headers=headers, timeout=10)\n        if response.status_code == 200:\n            result = response.json()\n            outputs = result.get(\"outputs\", [{}])\n            prediction = outputs[0].get(\"data\", []) if outputs else []\n            log(f\"‚úÖ {name}: {prediction[:3]}\")\n            return True\n    except:\n        pass\n    return False\n\n# Test deployed models\nsample_data = [[5.1, 3.5, 1.4, 0.2]]\nfor model_name in deployed[\"models\"]:\n    test_inference(model_name, sample_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üìã Act 2: Standardization - ML Pipelines\n\n**Seldon Core 2 provides standardized pipeline orchestration using consistent CRDs and Open Inference Protocol V2.**\n\n## Key Features:\n- **Pipeline CRDs**: Define complex ML workflows declaratively\n- **Data Flow**: Kafka-based streaming between pipeline steps\n- **Tensor Mapping**: Flexible data routing between models\n- **Open Standards**: V2 inference protocol for all endpoints",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy pipelines\npipelines_config = [\n    {\n        \"name\": \"product-pipeline-v1\",\n        \"steps\": [\n            {\"name\": \"feature-transformer\"},\n            {\n                \"name\": \"product-classifier-v1\",\n                \"inputs\": [\"product-pipeline-v1.inputs.predict\"],\n                \"tensorMap\": {\n                    \"product-pipeline-v1.inputs.predict\": \"predict\"\n                }\n            }\n        ]\n    },\n    {\n        \"name\": \"product-pipeline-v2\",\n        \"steps\": [\n            {\"name\": \"feature-transformer\"},\n            {\n                \"name\": \"product-classifier-v2\",\n                \"inputs\": [\"product-pipeline-v2.inputs.predict\"],\n                \"tensorMap\": {\n                    \"product-pipeline-v2.inputs.predict\": \"predict\"\n                }\n            }\n        ]\n    }\n]\n\nfor pipeline_config in pipelines_config:\n    pipeline_spec = {\n        \"apiVersion\": \"mlops.seldon.io/v1alpha1\",\n        \"kind\": \"Pipeline\",\n        \"metadata\": {\n            \"name\": pipeline_config[\"name\"],\n            \"namespace\": config[\"namespace\"]\n        },\n        \"spec\": {\n            \"steps\": pipeline_config[\"steps\"],\n            \"output\": {\"steps\": [pipeline_config[\"steps\"][-1][\"name\"]]}\n        }\n    }\n    \n    with open(f\"{pipeline_config['name']}.yaml\", \"w\") as f:\n        f.write(json.dumps(pipeline_spec, indent=2))\n    \n    result = run(f\"kubectl apply -f {pipeline_config['name']}.yaml\")\n    if result.returncode == 0:\n        deployed[\"pipelines\"].append(pipeline_config[\"name\"])\n        log(f\"‚úÖ Deployed pipeline: {pipeline_config['name']}\")\n\nlog(f\"Pipelines deployed: {len(deployed['pipelines'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üëÅÔ∏è Act 3: Observability - Real-Time Monitoring\n\n**Seldon Core 2 provides comprehensive observability with Prometheus metrics and distributed tracing.**\n\n## Key Features:\n- **Auto-generated Metrics**: Request rates, latencies, success rates\n- **Model-Level Granularity**: Per-model and per-pipeline metrics\n- **Prometheus Integration**: Ready-to-use queries for monitoring\n- **Custom Metrics**: Business KPIs and model performance tracking",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Generate metrics through inference requests\nlog(\"Generating metrics through 100 inference requests...\")\n\nrequest_count = 0\nfor i in range(25):\n    for endpoint in deployed[\"models\"][:2] + deployed[\"pipelines\"]:\n        try:\n            url = f\"http://{config['gateway_ip']}:{config['gateway_port']}/v2/models/{endpoint}/infer\"\n            payload = {\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[5.1, 3.5, 1.4, 0.2]]}]}\n            headers = {\"Content-Type\": \"application/json\", \"Seldon-Model\": f\"{endpoint}.pipeline\" if endpoint in deployed[\"pipelines\"] else endpoint}\n            \n            response = requests.post(url, json=payload, headers=headers, timeout=5)\n            if response.status_code == 200:\n                request_count += 1\n        except:\n            pass\n    \n    if i % 5 == 0:\n        print(f\"Progress: {request_count} requests...\", end=\"\\r\")\n\nprint()\nlog(f\"‚úÖ Generated {request_count} requests for metrics\")\n\n# Display Prometheus queries\ndisplay(Markdown(f\"\"\"\n## üìä Prometheus Queries\n\nCopy these queries into your Prometheus/Grafana:\n\n**Request Rate:**\n```promql\nrate(seldon_model_infer_total{{namespace=\"{config['namespace']}\"}}[5m])\n```\n\n**Latency P95:**\n```promql\nhistogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\"{config['namespace']}\"}}[5m]))\n```\n\n**Success Rate:**\n```promql\nsum(rate(seldon_model_infer_total{{namespace=\"{config['namespace']}\", code=\"200\"}}[5m])) / \nsum(rate(seldon_model_infer_total{{namespace=\"{config['namespace']}\"}}[5m])) * 100\n```\n\n**Per-Model Requests:**\n```promql\nsum by (model_name) (rate(seldon_model_infer_total{{namespace=\"{config['namespace']}\"}}[5m]))\n```\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ‚ö° Act 4: Optimization - A/B Testing\n\n**Seldon Core 2 enables safe model deployment through A/B testing and traffic management.**\n\n## Key Features:\n- **Traffic Splitting**: Route percentages of traffic to different models\n- **Safe Rollouts**: Test new models with minimal risk\n- **Instant Rollback**: Revert to previous version immediately\n- **Resource Efficiency**: Multiple versions share infrastructure",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy A/B experiment\nexperiment_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Experiment\nmetadata:\n  name: product-ab-test\n  namespace: {config['namespace']}\nspec:\n  default: product-pipeline-v1\n  resourceType: pipeline\n  candidates:\n    - name: product-pipeline-v1\n      weight: 90\n    - name: product-pipeline-v2\n      weight: 10\"\"\"\n\nwith open(\"experiment.yaml\", \"w\") as f:\n    f.write(experiment_yaml)\n\nresult = run(\"kubectl apply -f experiment.yaml\")\nif result.returncode == 0:\n    deployed[\"experiments\"].append(\"product-ab-test\")\n    log(\"‚úÖ Deployed A/B experiment: 90% v1, 10% v2\")\n\n# Test traffic splitting\ntime.sleep(10)  # Wait for experiment to be ready\n\nlog(\"Testing A/B traffic distribution with 50 requests...\")\nv1_count = v2_count = 0\n\nfor i in range(50):\n    try:\n        url = f\"http://{config['gateway_ip']}:{config['gateway_port']}/v2/models/product-pipeline-v1/infer\"\n        payload = {\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[5.1, 3.5, 1.4, 0.2]]}]}\n        headers = {\"Content-Type\": \"application/json\", \"Seldon-Model\": \"product-pipeline-v1.pipeline\"}\n        \n        response = requests.post(url, json=payload, headers=headers, timeout=5)\n        if response.status_code == 200:\n            route = response.headers.get(\"X-Seldon-Route\", \"\")\n            if \"v2\" in route:\n                v2_count += 1\n            else:\n                v1_count += 1\n    except:\n        pass\n\ntotal = v1_count + v2_count\nif total > 0:\n    v1_pct = (v1_count / total) * 100\n    v2_pct = (v2_count / total) * 100\n    log(f\"üìä Traffic Distribution: V1={v1_count} ({v1_pct:.0f}%), V2={v2_count} ({v2_pct:.0f}%)\")\n    \ndisplay(Markdown(f\"\"\"\n## üéõÔ∏è Traffic Management Commands\n\n**Update to 50/50 split:**\n```bash\nkubectl patch experiment product-ab-test -n {config['namespace']} --type='merge' -p='\n{{\n  \"spec\": {{\n    \"candidates\": [\n      {{\"name\": \"product-pipeline-v1\", \"weight\": 50}},\n      {{\"name\": \"product-pipeline-v2\", \"weight\": 50}}\n    ]\n  }}\n}}'\n```\n\n**Promote V2 to 100%:**\n```bash\nkubectl patch experiment product-ab-test -n {config['namespace']} --type='merge' -p='\n{{\n  \"spec\": {{\n    \"default\": \"product-pipeline-v2\",\n    \"candidates\": [\n      {{\"name\": \"product-pipeline-v2\", \"weight\": 100}}\n    ]\n  }}\n}}'\n```\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üèÜ Summary\n\n**You've successfully demonstrated Seldon Core 2's key capabilities:**\n\n‚úÖ **Flexibility**: Deployed multiple models on MLServer and Triton  \n‚úÖ **Standardization**: Created ML pipelines with V2 protocol  \n‚úÖ **Observability**: Generated metrics for Prometheus monitoring  \n‚úÖ **Optimization**: Implemented A/B testing with traffic splitting  \n\n## üìö Next Steps\n\n1. **Scale Up**: Increase replicas for production load\n2. **Add Monitoring**: Connect Prometheus and Grafana dashboards\n3. **Enable Auto-scaling**: Configure HPA for dynamic scaling\n4. **Advanced Features**: Explore drift detection, explainability, and more\n\n## üßπ Clean Up Resources\n\nRun the cell below to remove all deployed resources when you're done.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clean up resources\ndef cleanup_resources():\n    log(\"Cleaning up deployed resources...\")\n    \n    # Delete experiments first\n    for exp in deployed[\"experiments\"]:\n        run(f\"kubectl delete experiment {exp} -n {config['namespace']} --ignore-not-found=true\")\n    \n    # Delete pipelines\n    for pipeline in deployed[\"pipelines\"]:\n        run(f\"kubectl delete pipeline {pipeline} -n {config['namespace']} --ignore-not-found=true\")\n    \n    # Delete models\n    for model in deployed[\"models\"]:\n        run(f\"kubectl delete model {model} -n {config['namespace']} --ignore-not-found=true\")\n    \n    # Delete servers\n    for server in deployed[\"servers\"]:\n        run(f\"kubectl delete server {server} -n {config['namespace']} --ignore-not-found=true\")\n    \n    # Clean up YAML files\n    import glob\n    for yaml_file in glob.glob(\"*.yaml\"):\n        try:\n            import os\n            os.remove(yaml_file)\n        except:\n            pass\n    \n    log(\"‚úÖ Cleanup complete!\")\n\n# Uncomment to clean up\n# cleanup_resources()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}