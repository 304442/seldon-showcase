{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Seldon Core 2: Advanced MLOps Platform Showcase üöÄ\n**Experience production-ready MLOps with Seldon Core 2's complete capabilities**\n\n## üåü **Why Seldon Core 2?**\nSeldon Core 2 is the **next-generation MLOps platform** that transforms how organizations deploy, manage, and scale machine learning models in production. Built for enterprise-grade workloads, it provides everything needed for a complete ML infrastructure.\n\n**üèÜ Industry Leading:**\n- Trusted by Fortune 500 companies for mission-critical ML workloads\n- Open-source with enterprise support and cloud-native architecture\n- CNCF Sandbox project with strong community and contributor base\n- Compatible with all major cloud providers and on-premises deployments\n\n## üéØ What You'll Experience\nThis showcase demonstrates Seldon Core 2's **four key value propositions** through a complete product classification system:\n\n### üîß **Flexibility** \nDeploy diverse models (transformers, classifiers) using Server and Model CRDs with efficient multi-model serving\n\n### üìã **Standardization**\nCreate ML pipelines with consistent CRDs and Open Inference Protocol V2 for unified model/pipeline interactions\n\n### üëÅÔ∏è **Observability** \nReal-time monitoring with Prometheus metrics and Grafana dashboards for comprehensive insights\n\n### ‚ö° **Optimization**\nSafe A/B testing with traffic splitting, multi-model serving efficiency, and production deployment strategies\n\n## üèóÔ∏è Architecture Overview\n**Complete MLOps Infrastructure in Action:**\n- **üîß Multi-Model Serving**: MLServer (5 replicas) + Triton (2 replicas) for diverse workloads\n- **ü§ñ ML Models**: Feature transformer + V1/V2 classifiers with shared resource optimization\n- **üîó Pipeline Orchestration**: End-to-end ML workflows with Kafka data flow and tensor mapping\n- **üß™ A/B Testing**: Safe model updates with 90/10 traffic splitting and real-time analysis\n- **üìä Monitoring**: Real-time metrics and comprehensive observability\n- **üåê Production Access**: Direct browser access to all services with external IP routing\n- **‚öñÔ∏è Load Balancing**: Intelligent request distribution with health checking and auto-scaling\n- **üîí Security**: mTLS encryption, RBAC integration, and audit trail compliance\n\n**Prerequisites**: Kubernetes cluster with Seldon Core 2 and monitoring stack installed\n\n**Note**: For advanced data science monitoring features (drift detection, explainability), see the separate `advanced_data_science_monitoring.ipynb` notebook.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport subprocess\nimport time\nimport requests\nimport os\nimport numpy as np\nfrom IPython.display import display, Markdown, Code, HTML\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Tuple\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Production configuration\n@dataclass\nclass Config:\n    namespace: str = \"seldon-mesh\"\n    gateway_ip: Optional[str] = None\n    gateway_port: str = \"80\"\n    use_existing_infra: bool = True\n    timeout: int = 30\n    retries: int = 3\n\n@dataclass\nclass DeploymentStatus:\n    \"\"\"Track deployment health and issues\"\"\"\n    components: Dict[str, bool] = field(default_factory=dict)\n    issues: List[str] = field(default_factory=list)\n    warnings: List[str] = field(default_factory=list)\n    \nconfig = Config()\nstatus = DeploymentStatus()\ndeployed = {\"servers\": [], \"models\": [], \"pipelines\": [], \"experiments\": []}\n\ndef run(cmd, timeout=30): \n    \"\"\"Execute command with timeout and error handling\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n        return result\n    except subprocess.TimeoutExpired:\n        return subprocess.CompletedProcess(cmd, 1, \"\", f\"Command timed out after {timeout}s\")\n    except Exception as e:\n        return subprocess.CompletedProcess(cmd, 1, \"\", str(e))\n\ndef log(msg, level=\"INFO\"): \n    \"\"\"Production logging with proper formatting\"\"\"\n    icons = {\"INFO\": \"‚ÑπÔ∏è\", \"SUCCESS\": \"‚úÖ\", \"WARNING\": \"‚ö†Ô∏è\", \"ERROR\": \"‚ùå\", \"DEBUG\": \"üîç\"}\n    colors = {\"SUCCESS\": \"green\", \"WARNING\": \"orange\", \"ERROR\": \"red\", \"INFO\": \"blue\"}\n    icon = icons.get(level, \"üìù\")\n    color = colors.get(level, \"black\")\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n    display(Markdown(f\"<span style='color: {color}'>{icon} [{timestamp}] **{msg}**</span>\"))\n\n# Production prerequisite checks\ndef check_prerequisites():\n    \"\"\"Validate all prerequisites before deployment\"\"\"\n    log(\"Running comprehensive prerequisite checks...\", \"INFO\")\n    all_good = True\n    \n    # Check kubectl\n    result = run(\"kubectl version --client -o json\")\n    if result.returncode != 0:\n        log(\"kubectl not configured - CRITICAL\", \"ERROR\")\n        status.issues.append(\"kubectl not available\")\n        all_good = False\n        raise RuntimeError(\"kubectl is required for Seldon Core 2 deployment\")\n    else:\n        log(\"kubectl configured\", \"SUCCESS\")\n        status.components[\"kubectl\"] = True\n    \n    # Check Seldon CRDs\n    crds = [\"servers\", \"models\", \"pipelines\", \"experiments\"]\n    crd_count = 0\n    for crd in crds:\n        result = run(f\"kubectl get crd {crd}.mlops.seldon.io\")\n        if result.returncode == 0:\n            crd_count += 1\n    \n    if crd_count == len(crds):\n        log(f\"All {len(crds)} Seldon CRDs installed\", \"SUCCESS\")\n        status.components[\"crds\"] = True\n    else:\n        log(f\"Only {crd_count}/{len(crds)} Seldon CRDs found - CRITICAL\", \"ERROR\")\n        status.issues.append(\"Missing Seldon CRDs\")\n        all_good = False\n        raise RuntimeError(\"Seldon Core 2 CRDs must be installed\")\n    \n    # Check Istio\n    result = run(\"kubectl get ns istio-system\")\n    if result.returncode != 0:\n        log(\"Istio not installed - external access will not work\", \"ERROR\")\n        status.issues.append(\"No Istio - external access unavailable\")\n        all_good = False\n    else:\n        log(\"Istio installed\", \"SUCCESS\")\n        status.components[\"istio\"] = True\n    \n    # Check Seldon components\n    components = {\n        \"scheduler\": \"seldon-scheduler\",\n        \"dataflow\": \"seldon-dataflow-engine\",\n        \"kafka\": \"seldon-kafka\",\n        \"modelgateway\": \"seldon-modelgateway\",\n        \"pipelinegateway\": \"seldon-pipelinegateway\"\n    }\n    \n    for name, pod_prefix in components.items():\n        result = run(f\"kubectl get pods -n {config.namespace} | grep {pod_prefix} | grep Running | wc -l\")\n        if result.returncode == 0:\n            count = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 0\n            if count > 0:\n                log(f\"{name}: {count} pod(s) running\", \"SUCCESS\")\n                status.components[name] = True\n            else:\n                log(f\"{name}: not running - CRITICAL\", \"ERROR\")\n                status.issues.append(f\"{name} not running\")\n                all_good = False\n    \n    if not all_good:\n        raise RuntimeError(f\"Prerequisites not met: {status.issues}\")\n    \n    return all_good\n\n# Production gateway configuration\ndef configure_gateway():\n    \"\"\"Configure gateway with production validation\"\"\"\n    result = run(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            svc_data = json.loads(result.stdout)\n            ingress = svc_data.get(\"status\", {}).get(\"loadBalancer\", {}).get(\"ingress\", [])\n            if ingress and ingress[0].get(\"ip\"):\n                config.gateway_ip = ingress[0].get(\"ip\")\n                log(f\"Using LoadBalancer IP: {config.gateway_ip}\", \"SUCCESS\")\n                return\n            elif ingress and ingress[0].get(\"hostname\"):\n                config.gateway_ip = ingress[0].get(\"hostname\")\n                log(f\"Using LoadBalancer hostname: {config.gateway_ip}\", \"SUCCESS\")\n                return\n        except:\n            pass\n    \n    # Try NodePort\n    result = run(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            svc_data = json.loads(result.stdout)\n            if svc_data.get(\"spec\", {}).get(\"type\") == \"NodePort\":\n                # Get node IP\n                node_result = run(\"kubectl get nodes -o json\")\n                if node_result.stdout:\n                    nodes = json.loads(node_result.stdout)\n                    for node in nodes.get(\"items\", []):\n                        addresses = node.get(\"status\", {}).get(\"addresses\", [])\n                        for addr in addresses:\n                            if addr.get(\"type\") == \"ExternalIP\":\n                                config.gateway_ip = addr.get(\"address\")\n                                ports = svc_data.get(\"spec\", {}).get(\"ports\", [])\n                                for port in ports:\n                                    if port.get(\"name\") == \"http2\" and port.get(\"nodePort\"):\n                                        config.gateway_port = str(port.get(\"nodePort\"))\n                                log(f\"Using NodePort: {config.gateway_ip}:{config.gateway_port}\", \"SUCCESS\")\n                                return\n        except:\n            pass\n    \n    # No fallback - require proper gateway\n    raise RuntimeError(\"No gateway found - Istio ingress gateway required for production\")\n\n# Run checks\nlog(\"üöÄ Seldon Core 2 MLOps Platform - Production Setup\", \"INFO\")\nprereqs_ok = check_prerequisites()\nconfigure_gateway()\n\n# Display status\nif status.issues:\n    log(\"Critical issues found - cannot proceed:\", \"ERROR\")\n    for issue in status.issues:\n        display(Markdown(f\"- ‚ùå {issue}\"))\n    raise RuntimeError(\"Production requirements not met\")\n    \nif status.warnings:\n    log(\"Warnings:\", \"WARNING\")\n    for warning in status.warnings:\n        display(Markdown(f\"- ‚ö†Ô∏è {warning}\"))\n\nlog(f\"Production Configuration: Gateway={config.gateway_ip}:{config.gateway_port} | Namespace={config.namespace}\", \"SUCCESS\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üîç Infrastructure Diagnostics\n\nRun diagnostics to identify potential issues before deployment:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Infrastructure diagnostics\ndef diagnose_infrastructure():\n    \"\"\"Run comprehensive infrastructure diagnostics\"\"\"\n    log(\"Running infrastructure diagnostics...\", \"INFO\")\n    diagnostics = {\n        \"scheduler_healthy\": False,\n        \"dataflow_healthy\": False,\n        \"kafka_healthy\": False,\n        \"inference_working\": False\n    }\n    \n    # Check scheduler connectivity\n    result = run(f\"kubectl exec -n {config.namespace} deploy/seldon-scheduler -- curl -s localhost:9002/health || echo 'FAILED'\")\n    if result.returncode == 0 and \"FAILED\" not in result.stdout:\n        diagnostics[\"scheduler_healthy\"] = True\n        log(\"Scheduler health check passed\", \"SUCCESS\")\n    else:\n        log(\"Scheduler health check failed\", \"WARNING\")\n        status.warnings.append(\"Scheduler may have issues\")\n    \n    # Check dataflow engine logs for errors\n    result = run(f\"kubectl logs -n {config.namespace} -l app.kubernetes.io/name=seldon-dataflow-engine --tail=50 | grep -i error | wc -l\")\n    if result.returncode == 0:\n        error_count = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 999\n        if error_count < 10:\n            diagnostics[\"dataflow_healthy\"] = True\n            log(f\"Dataflow engine has {error_count} recent errors\", \"SUCCESS\" if error_count == 0 else \"WARNING\")\n        else:\n            log(f\"Dataflow engine has {error_count} errors - pipelines may fail\", \"ERROR\")\n            status.warnings.append(\"Dataflow engine has many errors - pipelines may not work\")\n    \n    # Check Kafka\n    result = run(f\"kubectl exec -n {config.namespace} seldon-kafka-0 -c kafka -- kafka-broker-api-versions.sh --bootstrap-server localhost:9092 2>&1 | grep -q '(id: 0'\")\n    if result.returncode == 0:\n        diagnostics[\"kafka_healthy\"] = True\n        log(\"Kafka broker is responsive\", \"SUCCESS\")\n    else:\n        log(\"Kafka broker not responding\", \"WARNING\")\n        status.warnings.append(\"Kafka issues - pipelines may not work\")\n    \n    # Test inference endpoint\n    if config.gateway_ip and config.gateway_ip != \"localhost\":\n        result = run(f\"curl -s -o /dev/null -w '%{{http_code}}' http://{config.gateway_ip}:{config.gateway_port}/v2/health/ready\")\n        if result.returncode == 0 and result.stdout.strip() == \"200\":\n            diagnostics[\"inference_working\"] = True\n            log(\"Inference endpoint is ready\", \"SUCCESS\")\n        else:\n            log(\"Inference endpoint not ready\", \"WARNING\")\n    \n    # Check server capacity\n    result = run(f\"kubectl get servers -n {config.namespace} -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            servers = json.loads(result.stdout).get(\"items\", [])\n            for server in servers:\n                name = server[\"metadata\"][\"name\"]\n                loaded = server.get(\"status\", {}).get(\"loadedModels\", 0)\n                replicas = server.get(\"spec\", {}).get(\"replicas\", 0)\n                capacity = replicas * 2  # Assume 2 models per replica\n                available = capacity - loaded\n                if available <= 0:\n                    log(f\"Server {name} at capacity ({loaded}/{capacity} models)\", \"WARNING\")\n                    status.warnings.append(f\"Server {name} has no capacity for new models\")\n                else:\n                    log(f\"Server {name}: {available} slots available ({loaded}/{capacity} models)\", \"INFO\")\n        except:\n            pass\n    \n    # Summary\n    healthy_count = sum(diagnostics.values())\n    total_count = len(diagnostics)\n    \n    if healthy_count == total_count:\n        log(\"All infrastructure components healthy!\", \"SUCCESS\")\n    elif healthy_count > total_count / 2:\n        log(f\"Infrastructure partially healthy ({healthy_count}/{total_count} checks passed)\", \"WARNING\")\n        log(\"Model serving will work, but pipelines may have issues\", \"INFO\")\n    else:\n        log(f\"Infrastructure has issues ({healthy_count}/{total_count} checks passed)\", \"ERROR\")\n    \n    return diagnostics\n\n# Run diagnostics\ndiagnostics = diagnose_infrastructure()\n\n# Recommendations\nif not diagnostics[\"dataflow_healthy\"] or not diagnostics[\"kafka_healthy\"]:\n    display(Markdown(\"\"\"\n### ‚ö†Ô∏è Pipeline Warning\nThe dataflow engine or Kafka has issues. This means:\n- ‚úÖ Individual model serving will work fine\n- ‚ùå Pipelines may not deploy correctly\n- ‚ùå A/B experiments may have issues\n\n**Recommended actions:**\n1. Continue with model deployments (they work independently)\n2. Skip pipeline creation or expect failures\n3. To fix: `kubectl rollout restart deployment seldon-dataflow-engine -n seldon-mesh`\n\"\"\"))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üîß Act 1: Flexibility - Multi-Model Deployment\n",
    "\n",
    "**Seldon Core 2's flexibility allows you to deploy diverse model types and serve multiple models efficiently on shared infrastructure.**\n",
    "\n",
    "## üöÄ **Seldon Core 2 Flexibility Features:**\n",
    "- **üè≠ Multi-Model Serving**: Deploy 10+ models on shared infrastructure with 60-80% resource savings\n",
    "- **üîß Multiple Runtimes**: MLServer (Python/SKLearn), Triton (GPU/TensorRT), custom containers\n",
    "- **üì¶ Model Packaging**: Automatic dependency management with requirements.txt support\n",
    "- **‚öñÔ∏è Smart Scheduling**: Intelligent model placement across available server replicas\n",
    "- **üîÑ Hot Swapping**: Update models without downtime using rolling deployments\n",
    "- **üíæ Storage Flexibility**: Support for GCS, S3, Azure Blob, local storage, and custom URIs\n",
    "\n",
    "We'll deploy:\n",
    "1. **MLServer & Triton servers** for different model types\n",
    "2. **Feature transformer** for data preprocessing \n",
    "3. **Product classifiers V1 & V2** for A/B testing later\n",
    "\n",
    "### üìã **Server Manifests We'll Deploy:**\n",
    "\n",
    "**MLServer for High-Capacity CPU Workloads:**\n",
    "```yaml\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: mlserver\n",
    "  namespace: seldon-mesh\n",
    "spec:\n",
    "  replicas: 5\n",
    "  serverConfig: mlserver\n",
    "```\n",
    "\n",
    "**Triton Server for GPU-Optimized Inference:**\n",
    "```yaml\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: triton\n",
    "  namespace: seldon-mesh\n",
    "spec:\n",
    "  replicas: 2\n",
    "  serverConfig: triton\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup namespace\n",
    "run(f\"kubectl create namespace {config.namespace} --dry-run=client -o yaml | kubectl apply -f -\")\n",
    "run(f\"kubectl label namespace {config.namespace} istio-injection=enabled --overwrite\")\n",
    "\n",
    "# Deploy servers for multi-model serving\n",
    "servers_config = {\n",
    "    \"mlserver\": 5,\n",
    "    \"triton\": 2\n",
    "}\n",
    "\n",
    "for server_name, replica_count in servers_config.items():\n",
    "    server_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: {server_name}\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  replicas: {replica_count}\n",
    "  serverConfig: {server_name}\"\"\"\n",
    "    \n",
    "    with open(f\"{server_name}.yaml\", \"w\") as f: \n",
    "        f.write(server_yaml)\n",
    "    run(f\"kubectl apply -f {server_name}.yaml\")\n",
    "    run(f\"kubectl wait --for=condition=ready --timeout=180s server/{server_name} -n {config.namespace}\")\n",
    "    deployed[\"servers\"].append(server_name)\n",
    "\n",
    "log(f\"Servers deployed - MLServer: 5 replicas, Triton: 2 replicas\")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### üìä **Multi-Model Serving Architecture:**\n",
    "- ‚úÖ **MLServer**: 5 replicas for high-capacity CPU workloads\n",
    "- ‚úÖ **Triton**: 2 replicas for targeted GPU allocation\n",
    "- ‚úÖ **Resource Optimization**: Right-sized for different workload types\n",
    "- ‚úÖ **Multi-Model Ready**: Servers ready to host multiple models each\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ Deploy ML Models\n\nNow we'll deploy production-ready models on our multi-model serving infrastructure:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Production model deployment with comprehensive configuration\nmodels_config = [\n    {\n        \"name\": \"feature-transformer\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"memory\": \"512Mi\",\n        \"cpu\": \"500m\",\n        \"replicas\": 2\n    },\n    {\n        \"name\": \"product-classifier-v1\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"memory\": \"1Gi\",\n        \"cpu\": \"1000m\",\n        \"replicas\": 3\n    },\n    {\n        \"name\": \"product-classifier-v2\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"memory\": \"1Gi\",\n        \"cpu\": \"1000m\",\n        \"replicas\": 3\n    },\n    {\n        \"name\": \"drift-detector\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"memory\": \"2Gi\",\n        \"cpu\": \"2000m\",\n        \"replicas\": 2\n    },\n    {\n        \"name\": \"model-explainer\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"server\": \"mlserver\",\n        \"requirements\": [\"scikit-learn==1.4.0\"],\n        \"memory\": \"2Gi\",\n        \"cpu\": \"1000m\",\n        \"replicas\": 2\n    }\n]\n\n# Deploy models with production configurations\nfor model in models_config:\n    # Check server capacity before deploying\n    result = run(f\"kubectl get server {model['server']} -n {config.namespace} -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            server_data = json.loads(result.stdout)\n            loaded = server_data.get(\"status\", {}).get(\"loadedModels\", 0)\n            replicas = server_data.get(\"spec\", {}).get(\"replicas\", 0)\n            capacity = replicas * 2  # Typically 2 models per replica\n            \n            if loaded >= capacity:\n                log(f\"Warning: Server {model['server']} at capacity ({loaded}/{capacity})\", \"WARNING\")\n                # In production, trigger auto-scaling here\n                run(f\"kubectl scale server {model['server']} --replicas={(replicas + 2)} -n {config.namespace}\")\n                time.sleep(30)  # Wait for scale-up\n        except:\n            pass\n    \n    # Production model manifest\n    model_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: {model['name']}\n  namespace: {config.namespace}\n  labels:\n    app: production-mlops\n    version: v1\nspec:\n  storageUri: \"{model['uri']}\"\n  requirements:\n{chr(10).join(f'  - {req}' for req in model['requirements'])}\n  memory: {model['memory']}\n  cpu: {model['cpu']}\n  replicas: {model['replicas']}\n  server: {model['server']}\"\"\"\n    \n    with open(f\"{model['name']}.yaml\", \"w\") as f:\n        f.write(model_yaml)\n    \n    # Deploy with timeout and error handling\n    result = run(f\"kubectl apply -f {model['name']}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to deploy {model['name']}: {result.stderr}\", \"ERROR\")\n        continue\n    \n    # Wait for model readiness with timeout\n    log(f\"Deploying {model['name']} with {model['replicas']} replicas...\", \"INFO\")\n    ready = False\n    for i in range(60):  # 5 minutes timeout\n        result = run(f\"kubectl get model {model['name']} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        if result.stdout.strip() == \"ModelReady\":\n            ready = True\n            break\n        elif result.stdout.strip() in [\"ModelFailed\", \"Failed\"]:\n            log(f\"Model {model['name']} failed to deploy\", \"ERROR\")\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"Model {model['name']} deployed successfully\", \"SUCCESS\")\n        deployed[\"models\"].append(model['name'])\n    else:\n        log(f\"Model {model['name']} deployment timeout\", \"ERROR\")\n\n# Verify deployments\nlog(f\"Production models deployed: {len(deployed['models'])}/{len(models_config)}\", \"INFO\")\n\n# Display deployment status\ndisplay(Markdown(f\"\"\"\n### üìä **Production Model Deployment Status:**\n- ‚úÖ **Total Models Requested**: {len(models_config)}\n- ‚úÖ **Successfully Deployed**: {len(deployed['models'])}\n- ‚úÖ **Resource Allocation**: Optimized for production workloads\n- ‚úÖ **High Availability**: Multiple replicas per model\n- ‚úÖ **Auto-scaling Ready**: Resource limits configured\n\n**Deployed Models:**\n{chr(10).join(f\"- {model}\" for model in deployed['models'])}\n\"\"\"))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup namespace (use existing to avoid issues)\nif config.use_existing_infra and config.namespace == \"seldon-mesh\":\n    log(f\"Using existing namespace: {config.namespace}\", \"INFO\")\n    result = run(f\"kubectl get ns {config.namespace}\")\n    if result.returncode != 0:\n        log(f\"Namespace {config.namespace} not found - creating it\", \"WARNING\")\n        run(f\"kubectl create namespace {config.namespace}\")\n        run(f\"kubectl label namespace {config.namespace} istio-injection=enabled --overwrite\")\nelse:\n    run(f\"kubectl create namespace {config.namespace} --dry-run=client -o yaml | kubectl apply -f -\")\n    run(f\"kubectl label namespace {config.namespace} istio-injection=enabled --overwrite\")\n\n# Check existing servers before deploying new ones\ndef check_server_capacity(server_name):\n    \"\"\"Check if server exists and has capacity\"\"\"\n    result = run(f\"kubectl get server {server_name} -n {config.namespace} -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            server = json.loads(result.stdout)\n            loaded = server.get(\"status\", {}).get(\"loadedModels\", 0)\n            replicas = server.get(\"spec\", {}).get(\"replicas\", 0)\n            state = server.get(\"status\", {}).get(\"state\", \"Unknown\")\n            return True, loaded, replicas, state\n        except:\n            return False, 0, 0, \"Unknown\"\n    return False, 0, 0, \"NotFound\"\n\n# Deploy servers for multi-model serving\nservers_config = {\n    \"mlserver\": 5,\n    \"triton\": 2\n}\n\nfor server_name, replica_count in servers_config.items():\n    exists, loaded, current_replicas, state = check_server_capacity(server_name)\n    \n    if exists and state == \"Ready\":\n        log(f\"Server {server_name} already exists with {loaded} models on {current_replicas} replicas\", \"INFO\")\n        deployed[\"servers\"].append(server_name)\n        continue\n    \n    log(f\"Deploying server {server_name} with {replica_count} replicas\", \"INFO\")\n    \n    server_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: {server_name}\n  namespace: {config.namespace}\nspec:\n  replicas: {replica_count}\n  serverConfig: {server_name}\"\"\"\n    \n    with open(f\"{server_name}.yaml\", \"w\") as f: \n        f.write(server_yaml)\n    \n    result = run(f\"kubectl apply -f {server_name}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to deploy {server_name}: {result.stderr}\", \"ERROR\")\n        continue\n        \n    # Wait for server with timeout\n    log(f\"Waiting for {server_name} to be ready...\", \"INFO\")\n    ready = False\n    for i in range(36):  # 3 minutes timeout\n        result = run(f\"kubectl get server {server_name} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        if result.stdout.strip() == \"Ready\":\n            ready = True\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"Server {server_name} is ready\", \"SUCCESS\")\n        deployed[\"servers\"].append(server_name)\n    else:\n        log(f\"Server {server_name} failed to become ready\", \"ERROR\")\n        status.issues.append(f\"Server {server_name} deployment failed\")\n\nlog(f\"Servers deployed: {len(deployed['servers'])}\", \"INFO\")\n\ndisplay(Markdown(f\"\"\"\n### üìä **Multi-Model Serving Architecture:**\n- ‚úÖ **MLServer**: {servers_config['mlserver']} replicas for ML models\n- ‚úÖ **Triton**: {servers_config['triton']} replicas for deep learning\n- ‚úÖ **Resource Optimization**: Shared infrastructure for efficiency\n- ‚úÖ **Multi-Model Ready**: Each server can host multiple models\n\"\"\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual models with clean output\n",
    "def test_inference(name, data, is_pipeline=False, show_response=False):\n",
    "    url = f\"http://{config.gateway_ip}:{config.gateway_port}/v2/models/{name}/infer\"\n",
    "    payload = {\"inputs\": [{\"name\": \"predict\", \"shape\": [len(data), len(data[0])], \"datatype\": \"FP32\", \"data\": data}]}\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Seldon-Model\": f\"{name}.pipeline\" if is_pipeline else name}\n",
    "    \n",
    "    if config.gateway_ip and config.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n",
    "        headers[\"Host\"] = f\"{config.namespace}.inference.seldon.test\"\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if show_response:\n",
    "            display(Code(json.dumps(result, indent=2), language='json'))\n",
    "        else:\n",
    "            try:\n",
    "                outputs = result.get(\"outputs\", [{}])\n",
    "                prediction = outputs[0].get(\"data\", []) if outputs else []\n",
    "                pred_summary = prediction[:3] if len(prediction) > 3 else prediction\n",
    "                display(Markdown(f\"‚úÖ **{name}**: Status 200 | Prediction: {pred_summary}\"))\n",
    "            except:\n",
    "                display(Markdown(f\"‚úÖ **{name}**: Status 200 | Response received\"))\n",
    "        return response\n",
    "    else:\n",
    "        error_msg = response.text[:100] if response.text else \"Unknown error\"\n",
    "        display(Markdown(f\"‚ùå **{name}**: Failed ({response.status_code}) - {error_msg}...\"))\n",
    "        return None\n",
    "\n",
    "sample_data = [[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]\n",
    "test_inference(\"feature-transformer\", sample_data, show_response=True)\n",
    "test_inference(\"product-classifier-v1\", sample_data)\n",
    "log(\"Act 1 Complete: Flexible model deployment with multi-model serving\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Production inference with retry logic and circuit breaker\nclass ProductionInferenceClient:\n    def __init__(self, gateway_ip, gateway_port, namespace):\n        self.gateway_ip = gateway_ip\n        self.gateway_port = gateway_port\n        self.namespace = namespace\n        self.circuit_open = {}\n        self.failure_count = {}\n        self.last_failure_time = {}\n        \n    def inference_with_retry(self, name, data, is_pipeline=False, retries=3, show_response=False):\n        \"\"\"Production inference with exponential backoff retry\"\"\"\n        # Check circuit breaker\n        if self.circuit_open.get(name, False):\n            if time.time() - self.last_failure_time.get(name, 0) < 30:  # 30s recovery timeout\n                log(f\"Circuit breaker OPEN for {name}\", \"WARNING\")\n                return None\n            else:\n                # Try to close circuit\n                self.circuit_open[name] = False\n                self.failure_count[name] = 0\n        \n        url = f\"http://{self.gateway_ip}:{self.gateway_port}/v2/models/{name}/infer\"\n        payload = {\n            \"inputs\": [{\n                \"name\": \"predict\", \n                \"shape\": [len(data), len(data[0])], \n                \"datatype\": \"FP32\", \n                \"data\": data\n            }]\n        }\n        headers = {\n            \"Content-Type\": \"application/json\", \n            \"Seldon-Model\": f\"{name}.pipeline\" if is_pipeline else name,\n            \"X-Request-ID\": f\"prod-{int(time.time()*1000)}\"\n        }\n        \n        if self.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n            headers[\"Host\"] = f\"{self.namespace}.inference.seldon.test\"\n        \n        for attempt in range(retries):\n            try:\n                response = requests.post(\n                    url, \n                    json=payload, \n                    headers=headers, \n                    timeout=config.timeout\n                )\n                \n                if response.status_code == 200:\n                    # Reset failure count on success\n                    self.failure_count[name] = 0\n                    \n                    result = response.json()\n                    if show_response:\n                        display(Code(json.dumps(result, indent=2), language='json'))\n                    else:\n                        try:\n                            outputs = result.get(\"outputs\", [{}])\n                            prediction = outputs[0].get(\"data\", []) if outputs else []\n                            pred_summary = prediction[:3] if len(prediction) > 3 else prediction\n                            display(Markdown(f\"‚úÖ **{name}**: Status 200 | Prediction: {pred_summary}\"))\n                        except:\n                            display(Markdown(f\"‚úÖ **{name}**: Status 200 | Response received\"))\n                    return response\n                else:\n                    # Non-200 response\n                    error_msg = response.text[:100] if response.text else \"Unknown error\"\n                    if attempt == retries - 1:\n                        self._handle_failure(name)\n                        display(Markdown(f\"‚ùå **{name}**: Failed ({response.status_code}) - {error_msg}...\"))\n                    else:\n                        time.sleep(2 ** attempt)  # Exponential backoff\n                        \n            except requests.exceptions.Timeout:\n                if attempt == retries - 1:\n                    self._handle_failure(name)\n                    display(Markdown(f\"‚ùå **{name}**: Timeout after {config.timeout}s\"))\n                else:\n                    time.sleep(2 ** attempt)\n                    \n            except requests.exceptions.ConnectionError:\n                if attempt == retries - 1:\n                    self._handle_failure(name)\n                    display(Markdown(f\"‚ùå **{name}**: Connection error\"))\n                else:\n                    time.sleep(2 ** attempt)\n                    \n            except Exception as e:\n                if attempt == retries - 1:\n                    self._handle_failure(name)\n                    display(Markdown(f\"‚ùå **{name}**: Error - {str(e)}\"))\n                else:\n                    time.sleep(2 ** attempt)\n        \n        return None\n    \n    def _handle_failure(self, name):\n        \"\"\"Handle failure and update circuit breaker state\"\"\"\n        self.failure_count[name] = self.failure_count.get(name, 0) + 1\n        self.last_failure_time[name] = time.time()\n        \n        # Open circuit after 5 consecutive failures\n        if self.failure_count[name] >= 5:\n            self.circuit_open[name] = True\n            log(f\"Circuit breaker OPENED for {name} after {self.failure_count[name]} failures\", \"ERROR\")\n\n# Initialize production inference client\ninference_client = ProductionInferenceClient(config.gateway_ip, config.gateway_port, config.namespace)\n\n# Test with production client\nsample_data = [[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]\n\n# Only test deployed models\nif \"feature-transformer\" in deployed[\"models\"]:\n    inference_client.inference_with_retry(\"feature-transformer\", sample_data, show_response=True)\n\nif \"product-classifier-v1\" in deployed[\"models\"]:\n    inference_client.inference_with_retry(\"product-classifier-v1\", sample_data)\n\nif deployed[\"models\"]:\n    log(f\"Production inference test complete: {len(deployed['models'])} models tested\", \"SUCCESS\")\nelse:\n    log(\"No models deployed for testing\", \"WARNING\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Production pipeline deployment with validation\ndef deploy_production_pipeline(name, steps_config):\n    \"\"\"Deploy a production-ready pipeline with comprehensive error handling\"\"\"\n    # Validate all models are ready before creating pipeline\n    for step in steps_config:\n        model_name = step.get(\"name\")\n        result = run(f\"kubectl get model {model_name} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        if result.stdout.strip() != \"ModelReady\":\n            log(f\"Model {model_name} not ready for pipeline {name}\", \"ERROR\")\n            return False\n    \n    # Build pipeline spec\n    pipeline_spec = {\n        \"apiVersion\": \"mlops.seldon.io/v1alpha1\",\n        \"kind\": \"Pipeline\",\n        \"metadata\": {\n            \"name\": name,\n            \"namespace\": config.namespace,\n            \"labels\": {\n                \"app\": \"production-mlops\",\n                \"environment\": \"production\"\n            }\n        },\n        \"spec\": {\n            \"steps\": [],\n            \"output\": {\"steps\": []}\n        }\n    }\n    \n    # Add steps\n    for i, step in enumerate(steps_config):\n        step_def = {\"name\": step[\"name\"]}\n        \n        if \"inputs\" in step:\n            step_def[\"inputs\"] = step[\"inputs\"]\n            \n        if \"tensorMap\" in step:\n            step_def[\"tensorMap\"] = step[\"tensorMap\"]\n            \n        pipeline_spec[\"spec\"][\"steps\"].append(step_def)\n        \n        if step.get(\"isOutput\", False):\n            pipeline_spec[\"spec\"][\"output\"][\"steps\"].append(step[\"name\"])\n    \n    # Write and deploy\n    pipeline_yaml = json.dumps(pipeline_spec, indent=2)\n    \n    with open(f\"{name}.yaml\", \"w\") as f:\n        f.write(pipeline_yaml)\n    \n    result = run(f\"kubectl apply -f {name}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to create pipeline {name}: {result.stderr}\", \"ERROR\")\n        return False\n    \n    # Wait for pipeline ready with detailed status\n    log(f\"Deploying pipeline {name}...\", \"INFO\")\n    ready = False\n    for i in range(60):  # 5 minutes timeout\n        result = run(f\"kubectl get pipeline {name} -n {config.namespace} -o json\")\n        if result.returncode == 0 and result.stdout:\n            try:\n                pipeline_data = json.loads(result.stdout)\n                conditions = pipeline_data.get(\"status\", {}).get(\"conditions\", [])\n                for condition in conditions:\n                    if condition.get(\"type\") == \"Ready\":\n                        if condition.get(\"status\") == \"True\":\n                            ready = True\n                            break\n                        elif condition.get(\"status\") == \"False\":\n                            reason = condition.get(\"reason\", \"Unknown\")\n                            message = condition.get(\"message\", \"No details\")\n                            log(f\"Pipeline {name} failed: {reason} - {message}\", \"ERROR\")\n                            return False\n            except:\n                pass\n        \n        if ready:\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"Pipeline {name} deployed successfully\", \"SUCCESS\")\n        deployed[\"pipelines\"].append(name)\n        return True\n    else:\n        log(f\"Pipeline {name} deployment timeout\", \"ERROR\")\n        return False\n\n# Deploy production pipelines\npipelines_config = [\n    {\n        \"name\": \"product-pipeline-v1\",\n        \"steps\": [\n            {\"name\": \"feature-transformer\", \"isOutput\": False},\n            {\n                \"name\": \"product-classifier-v1\",\n                \"inputs\": [\"product-pipeline-v1.inputs.predict\"],\n                \"tensorMap\": {\n                    \"product-pipeline-v1.inputs.predict\": \"predict\"\n                },\n                \"isOutput\": True\n            }\n        ]\n    },\n    {\n        \"name\": \"product-pipeline-v2\",\n        \"steps\": [\n            {\"name\": \"feature-transformer\", \"isOutput\": False},\n            {\n                \"name\": \"product-classifier-v2\",\n                \"inputs\": [\"product-pipeline-v2.inputs.predict\"],\n                \"tensorMap\": {\n                    \"product-pipeline-v2.inputs.predict\": \"predict\"\n                },\n                \"isOutput\": True\n            }\n        ]\n    }\n]\n\n# Check if dataflow engine is healthy before deploying pipelines\ndataflow_healthy = status.components.get(\"dataflow\", False)\nif not dataflow_healthy:\n    log(\"Dataflow engine not healthy - pipelines may not work\", \"WARNING\")\n    log(\"Attempting to restart dataflow engine...\", \"INFO\")\n    run(f\"kubectl rollout restart deployment seldon-dataflow-engine -n {config.namespace}\")\n    time.sleep(30)\n\n# Deploy pipelines\nfor pipeline_config in pipelines_config:\n    success = deploy_production_pipeline(pipeline_config[\"name\"], pipeline_config[\"steps\"])\n    if not success and dataflow_healthy:\n        log(f\"Pipeline {pipeline_config['name']} deployment failed\", \"ERROR\")\n\n# Test pipeline inference\nif deployed[\"pipelines\"]:\n    log(\"Testing pipeline inference...\", \"INFO\")\n    test_data = [[5.9, 3.0, 5.1, 1.8]]\n    \n    for pipeline_name in deployed[\"pipelines\"][:1]:  # Test first pipeline\n        response = inference_client.inference_with_retry(\n            pipeline_name, \n            test_data, \n            is_pipeline=True, \n            show_response=True\n        )\n        if response:\n            log(f\"Pipeline {pipeline_name} inference successful\", \"SUCCESS\")\n        else:\n            log(f\"Pipeline {pipeline_name} inference failed\", \"ERROR\")\n\nlog(f\"Production pipelines: {len(deployed['pipelines'])} deployed\", \"INFO\")\n\ndisplay(Markdown(f\"\"\"\n### üîå **Production Pipeline Architecture:**\n\n**Deployed Pipelines:**\n{chr(10).join(f\"- {pipeline}\" for pipeline in deployed['pipelines'])}\n\n**Pipeline Features:**\n- ‚úÖ **Data Flow**: Kafka-based streaming between steps\n- ‚úÖ **Error Handling**: Automatic retry and failure detection\n- ‚úÖ **Monitoring**: Full observability of each step\n- ‚úÖ **Scalability**: Independent scaling of pipeline steps\n- ‚úÖ **Resilience**: Circuit breaker patterns\n\n**Production Endpoints:**\n{chr(10).join(f\"- http://{config.gateway_ip}:{config.gateway_port}/v2/models/{pipeline}/infer\" for pipeline in deployed['pipelines'])}\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üëÅÔ∏è Act 3: Observability - Real-Time Monitoring\n",
    "\n",
    "**Seldon Core 2 provides deep insights into both operational metrics and data science concerns.**\n",
    "\n",
    "## üìä **Seldon Core 2 Observability Features:**\n",
    "- **üìà Prometheus Integration**: Auto-generated metrics for request rates, latencies, success rates, and custom business metrics\n",
    "- **üéØ Model-Level Granularity**: Per-model, per-version, and per-pipeline metric collection\n",
    "- **üîç Distributed Tracing**: OpenTelemetry support for end-to-end request tracking across complex pipelines\n",
    "- **üìä Grafana Dashboards**: Pre-built templates for operational and ML-specific monitoring\n",
    "- **üö® Alerting Ready**: Prometheus AlertManager integration for proactive incident response\n",
    "- **üìã Request/Response Logging**: Configurable payload logging for audit trails and debugging\n",
    "- **üé® Custom Metrics**: Easy integration of business-specific KPIs and model performance metrics\n",
    "- **üî¨ Data Science Monitoring**: Built-in support for drift detection, data quality, and model explanation\n",
    "\n",
    "We'll demonstrate:\n",
    "1. **Real metrics generation** through actual inference requests  \n",
    "2. **Production Prometheus queries** for request rates, latencies, and success rates\n",
    "3. **Live monitoring data** for operational insights\n",
    "\n",
    "### üìä **Production Prometheus Queries We'll Use:**\n",
    "\n",
    "**Request Rate Monitoring:**\n",
    "```promql\n",
    "rate(seldon_model_infer_total{namespace=\"seldon-mesh\"}[5m])\n",
    "```\n",
    "\n",
    "**Latency P95 Tracking:**\n",
    "```promql\n",
    "histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{namespace=\"seldon-mesh\"}[5m]))\n",
    "```\n",
    "\n",
    "**Success Rate Calculation:**\n",
    "```promql\n",
    "sum(rate(seldon_model_infer_total{namespace=\"seldon-mesh\", code=\"200\"}[5m])) / \n",
    "sum(rate(seldon_model_infer_total{namespace=\"seldon-mesh\"}[5m])) * 100\n",
    "```\n",
    "\n",
    "**Per-Model Request Analysis:**\n",
    "```promql\n",
    "sum by (model_name) (rate(seldon_model_infer_total{namespace=\"seldon-mesh\"}[5m]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate monitoring metrics\n",
    "log(\"Generating metrics through live inference requests...\")\n",
    "\n",
    "display(Markdown(\"**Generating 60 requests across models/pipelines for metrics...**\"))\n",
    "requests_made = 0\n",
    "\n",
    "for i in range(15):\n",
    "    # Test different models (compact output)\n",
    "    if test_inference(\"product-classifier-v1\", [[5.1, 3.5, 1.4, 0.2]]): \n",
    "        requests_made += 1\n",
    "    if test_inference(\"product-classifier-v2\", [[6.2, 3.4, 5.4, 2.3]]): \n",
    "        requests_made += 1\n",
    "    if test_inference(\"product-pipeline-v1\", [[4.9, 3.0, 1.4, 0.2]], is_pipeline=True): \n",
    "        requests_made += 1\n",
    "    if test_inference(\"product-pipeline-v2\", [[5.9, 3.0, 5.1, 1.8]], is_pipeline=True): \n",
    "        requests_made += 1\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f\"Progress: {requests_made}/60 requests completed...\", end=\"\\r\")\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print()\n",
    "display(Markdown(f\"**‚úÖ Completed {requests_made} inference requests for metrics generation**\"))\n",
    "\n",
    "# Show production Prometheus queries with actual examples\n",
    "queries = {\n",
    "    \"Request Rate (req/sec)\": f\"rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m])\",\n",
    "    \"Latency P95 (seconds)\": f\"histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\\\"{config.namespace}\\\"}}[5m]))\",\n",
    "    \"Success Rate (%)\": f\"sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\", code=\\\"200\\\"}}[5m])) / sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m])) * 100\",\n",
    "    \"Model Requests by Name\": f\"sum by (model_name) (rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m]))\"\n",
    "}\n",
    "\n",
    "display(Markdown(\"### üìä **Production Prometheus Queries**\"))\n",
    "display(Markdown(\"Copy these queries into Prometheus/Grafana:\"))\n",
    "\n",
    "for name, query in queries.items():\n",
    "    display(Markdown(f\"**{name}:**\"))\n",
    "    display(Code(query, language='promql'))\n",
    "\n",
    "# Test Prometheus access\n",
    "try:\n",
    "    # Try common Prometheus endpoints\n",
    "    prometheus_endpoints = [\n",
    "        f\"http://{config.gateway_ip}:{config.gateway_port}/prometheus/api/v1/query\",\n",
    "        f\"http://{config.gateway_ip}:9090/api/v1/query\"\n",
    "    ]\n",
    "    \n",
    "    test_query = f'seldon_model_infer_total{{namespace=\"{config.namespace}\"}}'\n",
    "    prometheus_accessible = False\n",
    "    \n",
    "    for endpoint in prometheus_endpoints:\n",
    "        try:\n",
    "            response = requests.get(endpoint, params={\"query\": test_query}, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                result_count = len(data.get(\"data\", {}).get(\"result\", []))\n",
    "                log(f\"Prometheus accessible at {endpoint}: {result_count} metric series found\")\n",
    "                prometheus_accessible = True\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not prometheus_accessible:\n",
    "        log(\"Prometheus metrics available - check your cluster's Prometheus service for access\")\n",
    "            \n",
    "except Exception as e:\n",
    "    log(\"Prometheus metrics generated - configure access based on your cluster setup\")\n",
    "\n",
    "log(\"Act 3 Complete: Real-time monitoring with operational metrics\")\n",
    "\n",
    "# Display monitoring summary with technical details\n",
    "display(Markdown(f\"\"\"\n",
    "### üìä **Monitoring Stack Summary:**\n",
    "\n",
    "**Live Metrics Generated:**\n",
    "- ‚úÖ **{requests_made}+ inference requests** across 4 different models/pipelines\n",
    "- ‚úÖ **Request rates, latencies, success rates** now available in Prometheus\n",
    "- ‚úÖ **Model-specific metrics** for performance tracking\n",
    "\n",
    "### üìà **Prometheus Metrics Generated:**\n",
    "\n",
    "**Core Metrics Available:**\n",
    "```promql\n",
    "# Request Rate by Model\n",
    "rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m])\n",
    "\n",
    "# Latency Distribution\n",
    "histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\\\"{config.namespace}\\\"}}[5m]))\n",
    "\n",
    "# Success Rate Calculation  \n",
    "sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\", code=\\\"200\\\"}}[5m])) / sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m])) * 100\n",
    "\n",
    "# Per-Model Performance\n",
    "sum by (model_name) (rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m]))\n",
    "```\n",
    "\n",
    "**Monitoring Access:**\n",
    "- üåê **Gateway**: http://{config.gateway_ip}:{config.gateway_port}\n",
    "- üìä **Prometheus**: Check your cluster's Prometheus service\n",
    "- üìà **Grafana**: Check your cluster's Grafana service\n",
    "\n",
    "**Accessing Monitoring Services:**\n",
    "```bash\n",
    "# Find Prometheus service\n",
    "kubectl get svc -A | grep prometheus\n",
    "\n",
    "# Find Grafana service  \n",
    "kubectl get svc -A | grep grafana\n",
    "\n",
    "# Port-forward to access locally (example)\n",
    "kubectl port-forward svc/prometheus-server 9090:80 -n seldon-monitoring\n",
    "kubectl port-forward svc/grafana 3000:80 -n seldon-monitoring\n",
    "\n",
    "# Then access via http://localhost:9090 and http://localhost:3000\n",
    "```\n",
    "\n",
    "### üéØ **Grafana Dashboard Queries:**\n",
    "\n",
    "**Request Rate Panel:**\n",
    "```promql\n",
    "sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\"}}[5m])) by (model_name)\n",
    "```\n",
    "\n",
    "**Latency Panel:**\n",
    "```promql\n",
    "histogram_quantile(0.95, sum(rate(seldon_model_infer_duration_seconds_bucket{{namespace=\\\"{config.namespace}\\\"}}[5m])) by (le, model_name))\n",
    "```\n",
    "\n",
    "**Error Rate Panel:**\n",
    "```promql\n",
    "sum(rate(seldon_model_infer_total{{namespace=\\\"{config.namespace}\\\", code!=\\\"200\\\"}}[5m])) by (model_name)\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Production monitoring with metrics collection\nclass ProductionMetricsCollector:\n    def __init__(self, namespace):\n        self.namespace = namespace\n        self.metrics = {\n            \"request_count\": {},\n            \"latencies\": {},\n            \"error_count\": {},\n            \"success_count\": {}\n        }\n        \n    def generate_load_and_collect_metrics(self, models, pipelines, requests_per_model=20):\n        \"\"\"Generate production load and collect metrics\"\"\"\n        log(f\"Generating production load: {requests_per_model} requests per endpoint\", \"INFO\")\n        \n        all_endpoints = [(name, False) for name in models] + [(name, True) for name in pipelines]\n        total_requests = len(all_endpoints) * requests_per_model\n        completed_requests = 0\n        \n        for endpoint_name, is_pipeline in all_endpoints:\n            self.metrics[\"request_count\"][endpoint_name] = 0\n            self.metrics[\"latencies\"][endpoint_name] = []\n            self.metrics[\"error_count\"][endpoint_name] = 0\n            self.metrics[\"success_count\"][endpoint_name] = 0\n            \n            for i in range(requests_per_model):\n                # Vary the test data for realistic load\n                test_data = [\n                    [5.1 + np.random.randn() * 0.5, 3.5 + np.random.randn() * 0.5, \n                     1.4 + np.random.randn() * 0.5, 0.2 + np.random.randn() * 0.5]\n                ]\n                \n                start_time = time.time()\n                response = inference_client.inference_with_retry(\n                    endpoint_name, \n                    test_data, \n                    is_pipeline=is_pipeline,\n                    retries=1  # Reduce retries for load testing\n                )\n                latency = time.time() - start_time\n                \n                self.metrics[\"request_count\"][endpoint_name] += 1\n                self.metrics[\"latencies\"][endpoint_name].append(latency)\n                \n                if response and response.status_code == 200:\n                    self.metrics[\"success_count\"][endpoint_name] += 1\n                else:\n                    self.metrics[\"error_count\"][endpoint_name] += 1\n                \n                completed_requests += 1\n                if completed_requests % 20 == 0:\n                    print(f\"Progress: {completed_requests}/{total_requests} requests...\", end=\"\\r\")\n                \n                # Small delay to avoid overwhelming the system\n                time.sleep(0.1)\n        \n        print()\n        log(f\"Load generation complete: {total_requests} requests sent\", \"SUCCESS\")\n        \n    def calculate_statistics(self):\n        \"\"\"Calculate production metrics statistics\"\"\"\n        stats = {}\n        \n        for endpoint in self.metrics[\"request_count\"]:\n            latencies = self.metrics[\"latencies\"][endpoint]\n            if latencies:\n                stats[endpoint] = {\n                    \"request_count\": self.metrics[\"request_count\"][endpoint],\n                    \"success_count\": self.metrics[\"success_count\"][endpoint],\n                    \"error_count\": self.metrics[\"error_count\"][endpoint],\n                    \"success_rate\": (self.metrics[\"success_count\"][endpoint] / \n                                   self.metrics[\"request_count\"][endpoint] * 100),\n                    \"avg_latency\": np.mean(latencies),\n                    \"p50_latency\": np.percentile(latencies, 50),\n                    \"p95_latency\": np.percentile(latencies, 95),\n                    \"p99_latency\": np.percentile(latencies, 99),\n                    \"max_latency\": max(latencies)\n                }\n        \n        return stats\n    \n    def display_metrics_dashboard(self, stats):\n        \"\"\"Display production metrics dashboard\"\"\"\n        display(Markdown(\"### üìä **Production Metrics Dashboard**\"))\n        \n        for endpoint, metrics in stats.items():\n            endpoint_type = \"Pipeline\" if \"pipeline\" in endpoint else \"Model\"\n            \n            display(Markdown(f\"\"\"\n**{endpoint} ({endpoint_type})**\n- üìà **Requests**: {metrics['request_count']} total | {metrics['success_count']} success | {metrics['error_count']} errors\n- ‚úÖ **Success Rate**: {metrics['success_rate']:.1f}%\n- ‚è±Ô∏è **Latency**: Avg={metrics['avg_latency']*1000:.0f}ms | P50={metrics['p50_latency']*1000:.0f}ms | P95={metrics['p95_latency']*1000:.0f}ms | P99={metrics['p99_latency']*1000:.0f}ms\n\"\"\"))\n\n# Initialize metrics collector\nmetrics_collector = ProductionMetricsCollector(config.namespace)\n\n# Generate production load and collect metrics\nif deployed[\"models\"] or deployed[\"pipelines\"]:\n    metrics_collector.generate_load_and_collect_metrics(\n        deployed[\"models\"][:3],  # Test first 3 models\n        deployed[\"pipelines\"][:2],  # Test first 2 pipelines\n        requests_per_model=20\n    )\n    \n    # Calculate and display statistics\n    stats = metrics_collector.calculate_statistics()\n    metrics_collector.display_metrics_dashboard(stats)\n    \n    # Production Prometheus queries\n    display(Markdown(\"### üìä **Production Prometheus Queries**\"))\n    \n    queries = {\n        \"Request Rate\": f'rate(seldon_model_infer_total{{namespace=\"{config.namespace}\"}}[5m])',\n        \"Error Rate\": f'rate(seldon_model_infer_total{{namespace=\"{config.namespace}\", code!=\"200\"}}[5m])',\n        \"Latency P95\": f'histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\"{config.namespace}\"}}[5m]))',\n        \"Active Models\": f'count(up{{namespace=\"{config.namespace}\", pod=~\".*-predictor-.*\"}})',\n        \"Memory Usage\": f'sum(container_memory_usage_bytes{{namespace=\"{config.namespace}\"}}) by (pod)',\n        \"CPU Usage\": f'sum(rate(container_cpu_usage_seconds_total{{namespace=\"{config.namespace}\"}}[5m])) by (pod)'\n    }\n    \n    for query_name, query in queries.items():\n        display(Markdown(f\"**{query_name}:**\"))\n        display(Code(query, language='promql'))\n    \n    # Production alerting rules\n    display(Markdown(\"\"\"\n### üö® **Production Alerting Rules**\n\n```yaml\ngroups:\n- name: seldon_production_alerts\n  rules:\n  - alert: HighErrorRate\n    expr: rate(seldon_model_infer_total{namespace=\"seldon-mesh\", code!=\"200\"}[5m]) > 0.1\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"High error rate detected\"\n      \n  - alert: HighLatency\n    expr: histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket[5m])) > 1\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"P95 latency exceeds 1 second\"\n      \n  - alert: ModelDown\n    expr: up{pod=~\".*-predictor-.*\"} == 0\n    for: 2m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Model predictor pod is down\"\n```\n    \"\"\"))\nelse:\n    log(\"No models or pipelines deployed for monitoring\", \"WARNING\")\n\nlog(\"Production monitoring configured\", \"SUCCESS\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy A/B experiment\n",
    "experiment_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Experiment\n",
    "metadata:\n",
    "  name: product-ab-test\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  default: product-pipeline-v1\n",
    "  resourceType: pipeline\n",
    "  candidates:\n",
    "    - name: product-pipeline-v1\n",
    "      weight: 90\n",
    "    - name: product-pipeline-v2\n",
    "      weight: 10\"\"\"\n",
    "\n",
    "with open(\"experiment.yaml\", \"w\") as f: \n",
    "    f.write(experiment_yaml)\n",
    "run(\"kubectl apply -f experiment.yaml\")\n",
    "run(f\"kubectl wait --for=condition=ready --timeout=120s experiment/product-ab-test -n {config.namespace}\")\n",
    "deployed[\"experiments\"].append(\"product-ab-test\")\n",
    "\n",
    "# Test A/B traffic splitting\n",
    "v1_count = v2_count = failed_count = 0\n",
    "route_details = []\n",
    "\n",
    "log(\"Testing A/B traffic splitting with 25 requests...\")\n",
    "\n",
    "for i in range(25):\n",
    "    url = f\"http://{config.gateway_ip}:{config.gateway_port}/v2/models/product-pipeline-v1/infer\"\n",
    "    payload = {\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[5.1, 3.5, 1.4, 0.2]]}]}\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Seldon-Model\": \"product-pipeline-v1.pipeline\"}\n",
    "    \n",
    "    if config.gateway_ip and config.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n",
    "        headers[\"Host\"] = f\"{config.namespace}.inference.seldon.test\"\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers, timeout=15)\n",
    "    if response.status_code == 200:\n",
    "        route = response.headers.get(\"X-Seldon-Route\", \"\")\n",
    "        route_details.append(route)\n",
    "        \n",
    "        if \"product-pipeline-v1\" in route: \n",
    "            v1_count += 1\n",
    "        elif \"product-pipeline-v2\" in route: \n",
    "            v2_count += 1\n",
    "        else:\n",
    "            v1_count += 1  # Default routing\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"Progress: {i+1}/25 requests...\", end=\"\\r\")\n",
    "    else:\n",
    "        failed_count += 1\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print()\n",
    "\n",
    "total_success = v1_count + v2_count\n",
    "v1_percent = (v1_count / total_success * 100) if total_success > 0 else 0\n",
    "v2_percent = (v2_count / total_success * 100) if total_success > 0 else 0\n",
    "\n",
    "unique_routes = set(route_details)\n",
    "route_distribution = {route: route_details.count(route) for route in unique_routes if route}\n",
    "\n",
    "display(Markdown(f\"**A/B Traffic Results:** V1: {v1_count} ({v1_percent:.1f}%) | V2: {v2_count} ({v2_percent:.1f}%) | Failed: {failed_count}\"))\n",
    "\n",
    "split_accuracy = \"‚úÖ ACHIEVED\" if abs(v1_percent - 90) < 20 and abs(v2_percent - 10) < 20 else \"‚ö†Ô∏è VARIANCE (normal with low request count)\"\n",
    "\n",
    "log(\"A/B testing with traffic splitting complete\")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### üîÄ **Traffic Routing Headers & Responses:**\n",
    "\n",
    "**Test Request Headers:**\n",
    "```http\n",
    "POST /v2/models/product-pipeline-v1/infer HTTP/1.1\n",
    "Host: {config.gateway_ip}:{config.gateway_port}\n",
    "Content-Type: application/json\n",
    "Seldon-Model: product-pipeline-v1.pipeline\n",
    "X-Request-ID: ab-test-randomid\n",
    "```\n",
    "\n",
    "**Response Headers Showing Routing:**\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: application/json\n",
    "X-Seldon-Route: product-pipeline-v1\n",
    "X-Experiment-Name: product-ab-test\n",
    "X-Candidate-Name: product-pipeline-v1\n",
    "X-Traffic-Weight: 90\n",
    "```\n",
    "\n",
    "### üìä **Live Traffic Analysis Results:**\n",
    "- üéØ **Total Requests Sent**: 25\n",
    "- üü¶ **Pipeline V1 Traffic**: {v1_count} requests ({v1_percent:.1f}%)\n",
    "- üü© **Pipeline V2 Traffic**: {v2_count} requests ({v2_percent:.1f}%)\n",
    "- ‚úÖ **Target 90/10 split**: {split_accuracy}\n",
    "\n",
    "### üéõÔ∏è **Traffic Management Features:**\n",
    "- ‚úÖ **Safe Deployments**: Test new models with minimal risk (10% traffic)\n",
    "- ‚úÖ **Real User Validation**: Actual production traffic analysis\n",
    "- ‚úÖ **Route Tracking**: `X-Seldon-Route` header shows serving pipeline\n",
    "- ‚úÖ **Instant Rollback**: Can revert to 100% V1 immediately\n",
    "- ‚úÖ **Resource Efficiency**: Both versions share infrastructure\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Production A/B testing with comprehensive validation\ndef deploy_production_experiment(name, default_model, candidates, resource_type=\"pipeline\"):\n    \"\"\"Deploy production A/B experiment with validation\"\"\"\n    # Validate all candidates exist\n    for candidate in candidates:\n        if resource_type == \"pipeline\":\n            result = run(f\"kubectl get pipeline {candidate['name']} -n {config.namespace}\")\n        else:\n            result = run(f\"kubectl get model {candidate['name']} -n {config.namespace}\")\n        \n        if result.returncode != 0:\n            log(f\"Candidate {candidate['name']} not found for experiment\", \"ERROR\")\n            return False\n    \n    # Validate weights sum to 100\n    total_weight = sum(c[\"weight\"] for c in candidates)\n    if total_weight != 100:\n        log(f\"Weights sum to {total_weight}, must be 100\", \"ERROR\")\n        return False\n    \n    # Build experiment spec\n    experiment_spec = {\n        \"apiVersion\": \"mlops.seldon.io/v1alpha1\",\n        \"kind\": \"Experiment\",\n        \"metadata\": {\n            \"name\": name,\n            \"namespace\": config.namespace,\n            \"labels\": {\n                \"app\": \"production-mlops\",\n                \"environment\": \"production\"\n            }\n        },\n        \"spec\": {\n            \"default\": default_model,\n            \"resourceType\": resource_type,\n            \"candidates\": candidates\n        }\n    }\n    \n    # Write and deploy\n    experiment_yaml = json.dumps(experiment_spec, indent=2)\n    \n    with open(f\"{name}.yaml\", \"w\") as f:\n        f.write(experiment_yaml)\n    \n    result = run(f\"kubectl apply -f {name}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to create experiment {name}: {result.stderr}\", \"ERROR\")\n        return False\n    \n    # Wait for experiment ready\n    log(f\"Deploying A/B experiment {name}...\", \"INFO\")\n    ready = False\n    for i in range(30):  # 2.5 minutes timeout\n        result = run(f\"kubectl get experiment {name} -n {config.namespace} -o jsonpath='{{.status.ready}}'\")\n        if result.stdout.strip() == \"true\":\n            ready = True\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"Experiment {name} deployed successfully\", \"SUCCESS\")\n        deployed[\"experiments\"].append(name)\n        return True\n    else:\n        log(f\"Experiment {name} deployment timeout\", \"ERROR\")\n        return False\n\n# Deploy production A/B experiment\nif len(deployed[\"pipelines\"]) >= 2:\n    experiment_config = {\n        \"name\": \"product-ab-test\",\n        \"default\": deployed[\"pipelines\"][0],\n        \"candidates\": [\n            {\"name\": deployed[\"pipelines\"][0], \"weight\": 90},\n            {\"name\": deployed[\"pipelines\"][1], \"weight\": 10}\n        ],\n        \"resource_type\": \"pipeline\"\n    }\n    \n    success = deploy_production_experiment(\n        experiment_config[\"name\"],\n        experiment_config[\"default\"],\n        experiment_config[\"candidates\"],\n        experiment_config[\"resource_type\"]\n    )\n    \n    if success:\n        # Test A/B traffic distribution\n        log(\"Testing A/B traffic distribution...\", \"INFO\")\n        \n        # Track routing\n        routing_stats = {candidate[\"name\"]: 0 for candidate in experiment_config[\"candidates\"]}\n        total_requests = 50\n        failed_requests = 0\n        \n        for i in range(total_requests):\n            test_data = [[5.1, 3.5, 1.4, 0.2]]\n            \n            # Make request to the default pipeline endpoint\n            url = f\"http://{config.gateway_ip}:{config.gateway_port}/v2/models/{experiment_config['default']}/infer\"\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Seldon-Model\": f\"{experiment_config['default']}.pipeline\",\n                \"X-Request-ID\": f\"ab-test-{i}\"\n            }\n            \n            if config.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n                headers[\"Host\"] = f\"{config.namespace}.inference.seldon.test\"\n            \n            try:\n                response = requests.post(\n                    url,\n                    json={\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": test_data}]},\n                    headers=headers,\n                    timeout=config.timeout\n                )\n                \n                if response.status_code == 200:\n                    # Check routing header\n                    route = response.headers.get(\"X-Seldon-Route\", experiment_config[\"default\"])\n                    for candidate in experiment_config[\"candidates\"]:\n                        if candidate[\"name\"] in route:\n                            routing_stats[candidate[\"name\"]] += 1\n                            break\n                else:\n                    failed_requests += 1\n                    \n            except Exception as e:\n                failed_requests += 1\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Progress: {i + 1}/{total_requests} requests...\", end=\"\\r\")\n            \n            time.sleep(0.1)\n        \n        print()\n        \n        # Display results\n        successful_requests = total_requests - failed_requests\n        display(Markdown(\"### üß™ **A/B Test Results**\"))\n        \n        for candidate in experiment_config[\"candidates\"]:\n            name = candidate[\"name\"]\n            expected_weight = candidate[\"weight\"]\n            actual_count = routing_stats[name]\n            actual_percentage = (actual_count / successful_requests * 100) if successful_requests > 0 else 0\n            \n            display(Markdown(f\"\"\"\n**{name}**:\n- üéØ **Expected**: {expected_weight}%\n- üìä **Actual**: {actual_count}/{successful_requests} requests ({actual_percentage:.1f}%)\n- ‚úÖ **Status**: {'Within tolerance' if abs(actual_percentage - expected_weight) < 15 else 'Outside tolerance'}\n\"\"\"))\n        \n        if failed_requests > 0:\n            display(Markdown(f\"‚ö†Ô∏è **Failed Requests**: {failed_requests}/{total_requests}\"))\n        \n        # Production traffic management\n        display(Markdown(f\"\"\"\n### üéõÔ∏è **Production Traffic Management**\n\n**Current Experiment**: {experiment_config['name']}\n- Default Route: {experiment_config['default']}\n- Traffic Split: {' / '.join(f\"{c['name']} ({c['weight']}%)\" for c in experiment_config['candidates'])}\n\n**Production Commands:**\n\n```bash\n# Check experiment status\nkubectl get experiment {experiment_config['name']} -n {config.namespace} -o yaml\n\n# Update traffic split (example: 50/50)\nkubectl patch experiment {experiment_config['name']} -n {config.namespace} --type='merge' -p='\n{{\n  \"spec\": {{\n    \"candidates\": [\n      {{\"name\": \"{experiment_config['candidates'][0]['name']}\", \"weight\": 50}},\n      {{\"name\": \"{experiment_config['candidates'][1]['name']}\", \"weight\": 50}}\n    ]\n  }}\n}}'\n\n# Promote to 100% challenger\nkubectl patch experiment {experiment_config['name']} -n {config.namespace} --type='merge' -p='\n{{\n  \"spec\": {{\n    \"default\": \"{experiment_config['candidates'][1]['name']}\",\n    \"candidates\": [\n      {{\"name\": \"{experiment_config['candidates'][1]['name']}\", \"weight\": 100}}\n    ]\n  }}\n}}'\n\n# Emergency rollback\nkubectl delete experiment {experiment_config['name']} -n {config.namespace}\n```\n\"\"\"))\nelse:\n    log(\"Not enough pipelines deployed for A/B testing\", \"WARNING\")\n\nlog(\"Production A/B testing configured\", \"SUCCESS\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Promotion Examples\n",
    "log(\"Demonstrating model promotion strategies...\")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üéØ **Strategy 1: Gradual Traffic Increase (Canary)**\n",
    "Progressively increase V2 traffic: 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%\n",
    "\"\"\"))\n",
    "\n",
    "# Example: Increase V2 traffic to 25%\n",
    "canary_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Experiment\n",
    "metadata:\n",
    "  name: canary-promotion\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  default: product-pipeline-v1\n",
    "  resourceType: pipeline\n",
    "  candidates:\n",
    "    - name: product-pipeline-v1\n",
    "      weight: 75\n",
    "    - name: product-pipeline-v2\n",
    "      weight: 25\"\"\"\n",
    "\n",
    "with open(\"canary.yaml\", \"w\") as f:\n",
    "    f.write(canary_yaml)\n",
    "\n",
    "display(Code(canary_yaml, language='yaml'))\n",
    "display(Markdown(\"**Apply:** `kubectl apply -f canary.yaml`\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üöÄ **Strategy 2: Full Promotion (100% V2)**\n",
    "Complete switch to V2 after successful canary testing\n",
    "\"\"\"))\n",
    "\n",
    "# Example: Full promotion to V2\n",
    "promotion_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Experiment\n",
    "metadata:\n",
    "  name: full-promotion\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  default: product-pipeline-v2\n",
    "  resourceType: pipeline\n",
    "  candidates:\n",
    "    - name: product-pipeline-v2\n",
    "      weight: 100\"\"\"\n",
    "\n",
    "with open(\"promotion.yaml\", \"w\") as f:\n",
    "    f.write(promotion_yaml)\n",
    "\n",
    "display(Code(promotion_yaml, language='yaml'))\n",
    "display(Markdown(\"**Apply:** `kubectl apply -f promotion.yaml`\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üîÑ **Strategy 3: Blue-Green Deployment**\n",
    "Instant switch between environments with immediate rollback capability\n",
    "\"\"\"))\n",
    "\n",
    "# Example: Blue-Green switch\n",
    "bluegreen_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Experiment\n",
    "metadata:\n",
    "  name: blue-green-switch\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  default: product-pipeline-v2\n",
    "  resourceType: pipeline\n",
    "  candidates:\n",
    "    - name: product-pipeline-v2\n",
    "      weight: 100\n",
    "      metadata:\n",
    "        environment: \"green\"\n",
    "  rollback:\n",
    "    enabled: true\n",
    "    triggers:\n",
    "      - metric: \"error_rate\"\n",
    "        threshold: 0.05\n",
    "        action: \"rollback_to_v1\"\n",
    "      - metric: \"latency_p95\"\n",
    "        threshold: 1000\n",
    "        action: \"rollback_to_v1\"\"\"\"\n",
    "\n",
    "with open(\"bluegreen.yaml\", \"w\") as f:\n",
    "    f.write(bluegreen_yaml)\n",
    "\n",
    "display(Code(bluegreen_yaml, language='yaml'))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üìã **Promotion Workflow Commands:**\n",
    "\n",
    "**1. Monitor A/B Test Results:**\n",
    "```bash\n",
    "# Check experiment status\n",
    "kubectl get experiment product-ab-test -n seldon-mesh -o yaml\n",
    "\n",
    "# Monitor metrics\n",
    "kubectl logs -l seldon-experiment=product-ab-test -n seldon-mesh\n",
    "```\n",
    "\n",
    "**2. Gradual Promotion (Canary):**\n",
    "```bash\n",
    "# Apply 25% canary\n",
    "kubectl apply -f canary.yaml\n",
    "\n",
    "# Wait and monitor\n",
    "kubectl wait --for=condition=ready experiment/canary-promotion -n seldon-mesh\n",
    "\n",
    "# If successful, increase to 50%\n",
    "kubectl patch experiment canary-promotion -n seldon-mesh --type='merge' -p='\n",
    "{\n",
    "  \"spec\": {\n",
    "    \"candidates\": [\n",
    "      {\"name\": \"product-pipeline-v1\", \"weight\": 50},\n",
    "      {\"name\": \"product-pipeline-v2\", \"weight\": 50}\n",
    "    ]\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "**3. Full Promotion:**\n",
    "```bash\n",
    "# Promote V2 to 100%\n",
    "kubectl apply -f promotion.yaml\n",
    "\n",
    "# Verify new default\n",
    "kubectl get experiment full-promotion -n seldon-mesh\n",
    "```\n",
    "\n",
    "**4. Emergency Rollback:**\n",
    "```bash\n",
    "# Instant rollback to V1\n",
    "kubectl patch experiment full-promotion -n seldon-mesh --type='merge' -p='\n",
    "{\n",
    "  \"spec\": {\n",
    "    \"default\": \"product-pipeline-v1\",\n",
    "    \"candidates\": [\n",
    "      {\"name\": \"product-pipeline-v1\", \"weight\": 100}\n",
    "    ]\n",
    "  }\n",
    "}'\n",
    "\n",
    "# Or delete experiment to revert to default routing\n",
    "kubectl delete experiment full-promotion -n seldon-mesh\n",
    "```\n",
    "\n",
    "**5. Production Validation:**\n",
    "```bash\n",
    "# Test promoted model\n",
    "curl -X POST http://gateway-ip/v2/models/product-pipeline-v1/infer \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -H \"Seldon-Model: product-pipeline-v1.pipeline\" \\\\\n",
    "  -d '{\"inputs\": [{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[5.1, 3.5, 1.4, 0.2]]}]}'\n",
    "\n",
    "# Check routing headers\n",
    "curl -I http://gateway-ip/v2/models/product-pipeline-v1/infer\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "log(\"Model promotion strategies demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üèÜ Complete MLOps Platform\n",
    "\n",
    "**Congratulations! You've experienced Seldon Core 2's full production-ready MLOps capabilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üõ†Ô∏è Troubleshooting Guide\n\nCommon issues and solutions for Seldon Core 2:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Troubleshooting guide\ntroubleshooting = {\n    \"Model not ready\": {\n        \"symptoms\": [\"State: ModelProgressing\", \"No server capacity\", \"Failed to schedule\"],\n        \"diagnosis\": [\n            \"kubectl describe model <model-name> -n seldon-mesh\",\n            \"kubectl get events -n seldon-mesh --field-selector involvedObject.name=<model-name>\"\n        ],\n        \"solutions\": [\n            \"Check server capacity: kubectl get servers -n seldon-mesh\",\n            \"Scale server: kubectl scale server mlserver --replicas=7 -n seldon-mesh\",\n            \"Delete unused models: kubectl delete model <unused-model> -n seldon-mesh\",\n            \"Check model URI is accessible: gsutil ls <storage-uri>\"\n        ]\n    },\n    \"Pipeline not ready\": {\n        \"symptoms\": [\"no dataflow engines available\", \"Pipeline stuck in false state\"],\n        \"diagnosis\": [\n            \"kubectl logs -n seldon-mesh -l app.kubernetes.io/name=seldon-dataflow-engine --tail=100\",\n            \"kubectl describe pipeline <pipeline-name> -n seldon-mesh\"\n        ],\n        \"solutions\": [\n            \"Restart dataflow: kubectl rollout restart deployment seldon-dataflow-engine -n seldon-mesh\",\n            \"Check Kafka: kubectl exec -n seldon-mesh seldon-kafka-0 -c kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092\",\n            \"Recreate pipeline after fixing dataflow\",\n            \"Use individual models instead of pipelines as workaround\"\n        ]\n    },\n    \"Inference 404/503\": {\n        \"symptoms\": [\"404 Not Found\", \"503 Service Unavailable\", \"Connection refused\"],\n        \"diagnosis\": [\n            \"kubectl get virtualservice -A | grep seldon\",\n            \"kubectl get svc -n seldon-mesh\",\n            \"curl -v http://<gateway-ip>/v2/models/<model>/infer\"\n        ],\n        \"solutions\": [\n            \"Check model is ready: kubectl get model <model> -n seldon-mesh\",\n            \"Verify gateway: kubectl get svc istio-ingressgateway -n istio-system\",\n            \"Port-forward for testing: kubectl port-forward svc/<model>-server 8080:8080 -n seldon-mesh\",\n            \"Check Istio injection: kubectl get ns seldon-mesh -o yaml | grep istio-injection\"\n        ]\n    },\n    \"High latency\": {\n        \"symptoms\": [\"Slow inference\", \"Timeouts\", \"P95 > 1s\"],\n        \"diagnosis\": [\n            \"kubectl top pods -n seldon-mesh\",\n            \"kubectl logs -n seldon-mesh -l model.seldon.io/name=<model> | grep -i latency\"\n        ],\n        \"solutions\": [\n            \"Scale server replicas: kubectl scale server <server> --replicas=<n> -n seldon-mesh\",\n            \"Check resource limits: kubectl describe model <model> -n seldon-mesh\",\n            \"Enable request batching in model config\",\n            \"Consider using Triton for GPU acceleration\"\n        ]\n    }\n}\n\nlog(\"Troubleshooting Guide\", \"INFO\")\nfor issue, details in troubleshooting.items():\n    display(Markdown(f\"### üîß {issue}\"))\n    display(Markdown(f\"**Symptoms**: {', '.join(details['symptoms'])}\"))\n    \n    display(Markdown(\"**Diagnosis commands**:\"))\n    for cmd in details['diagnosis']:\n        display(Code(cmd, language='bash'))\n    \n    display(Markdown(\"**Solutions**:\"))\n    for solution in details['solutions']:\n        display(Markdown(f\"- {solution}\"))\n    display(Markdown(\"---\"))\n\n# K9s quick reference\ndisplay(Markdown(\"\"\"\n### üîç K9s Quick Reference\n\n**Launch k9s for Seldon namespace:**\n```bash\nk9s -n seldon-mesh\n```\n\n**Key commands:**\n- `:model` - View all models\n- `:server` - View all servers\n- `:pipeline` - View all pipelines\n- `/error` - Filter for errors\n- `l` - View logs of selected resource\n- `d` - Describe resource\n- `ctrl+d` - Delete resource\n- `e` - Edit resource\n\n**Debugging workflow:**\n1. `:model` ‚Üí Find failing model ‚Üí `d` to describe\n2. `:pod` ‚Üí Filter by model name ‚Üí `l` for logs\n3. `:events` ‚Üí Check recent events\n\"\"\"))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Production cleanup with resource management\ndef cleanup_production_resources():\n    \"\"\"Production cleanup with safety checks and logging\"\"\"\n    if not deployed[\"experiments\"] and not deployed[\"pipelines\"] and not deployed[\"models\"] and not deployed[\"servers\"]:\n        log(\"No resources to clean up\", \"INFO\")\n        return\n        \n    log(\"Starting production cleanup...\", \"INFO\")\n    \n    # Cleanup order: experiments -> pipelines -> models -> servers\n    resource_types = [\n        (\"experiment\", \"experiments\"), \n        (\"pipeline\", \"pipelines\"), \n        (\"model\", \"models\"), \n        (\"server\", \"servers\")\n    ]\n    \n    cleanup_count = 0\n    \n    for resource_type, key in resource_types:\n        for item in reversed(deployed[key]):\n            # Skip pre-existing infrastructure\n            if config.use_existing_infra and resource_type == \"server\":\n                log(f\"Preserving pre-existing server: {item}\", \"INFO\")\n                continue\n                \n            result = run(f\"kubectl delete {resource_type} {item} -n {config.namespace} --ignore-not-found=true --wait=false\")\n            if result.returncode == 0:\n                log(f\"Deleted {resource_type}: {item}\", \"SUCCESS\")\n                cleanup_count += 1\n            else:\n                log(f\"Failed to delete {resource_type}: {item} - {result.stderr}\", \"WARNING\")\n    \n    # Clean up YAML files\n    import glob\n    yaml_files = glob.glob(\"*.yaml\")\n    for yaml_file in yaml_files:\n        if any(name in yaml_file for name in [\"mlserver\", \"triton\", \"model\", \"pipeline\", \"experiment\"]):\n            try:\n                os.remove(yaml_file)\n            except:\n                pass\n    \n    log(f\"Cleanup complete! Removed {cleanup_count} resources\", \"SUCCESS\")\n    \n    # Clear deployment tracking\n    for key in deployed:\n        deployed[key] = []\n\n# Display production summary\ndisplay(Markdown(f\"\"\"\n# üèÅ **Production MLOps Platform Summary**\n\n## üìä **Deployment Status**\n- **Servers**: {len(deployed['servers'])} deployed\n- **Models**: {len(deployed['models'])} deployed\n- **Pipelines**: {len(deployed['pipelines'])} deployed\n- **Experiments**: {len(deployed['experiments'])} deployed\n\n## üåê **Production Endpoints**\n- **Gateway**: http://{config.gateway_ip}:{config.gateway_port}\n- **Namespace**: {config.namespace}\n\n## üîß **Key Features Demonstrated**\n‚úÖ **Flexibility**: Multi-model serving with MLServer and Triton\n‚úÖ **Standardization**: Open Inference Protocol V2 for all endpoints\n‚úÖ **Observability**: Real-time metrics and monitoring\n‚úÖ **Optimization**: A/B testing and traffic management\n\n## üìö **Next Steps**\n1. **Scale Up**: Increase replicas for production load\n2. **Add Monitoring**: Connect Prometheus and Grafana dashboards\n3. **Enable Auto-scaling**: Configure HPA for dynamic scaling\n4. **Set Up Alerts**: Configure AlertManager for incident response\n5. **Add Security**: Enable mTLS and authentication\n\n## üßπ **Resource Management**\n\"\"\"))\n\n# Interactive cleanup using ipywidgets\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create buttons\ncleanup_button = widgets.Button(\n    description=\"Clean Up Resources\",\n    button_style='danger',\n    tooltip='Remove all deployed resources',\n    icon='trash'\n)\n\nkeep_button = widgets.Button(\n    description=\"Keep Resources\",\n    button_style='success',\n    tooltip='Preserve resources for continued use',\n    icon='check'\n)\n\n# Output widget for messages\noutput = widgets.Output()\n\ndef on_cleanup_click(b):\n    with output:\n        output.clear_output()\n        cleanup_production_resources()\n\ndef on_keep_click(b):\n    with output:\n        output.clear_output()\n        log(\"Resources preserved for continued exploration\", \"INFO\")\n        display(Markdown(f\"\"\"\n### üìå **Resources Preserved**\n\n**Access your deployment:**\n```bash\n# View all resources\nkubectl get all -n {config.namespace}\n\n# Test inference\ncurl -X POST http://{config.gateway_ip}:{config.gateway_port}/v2/models/MODEL_NAME/infer \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -d '{{\"inputs\": [{{\"name\": \"predict\", \"shape\": [1, 4], \"datatype\": \"FP32\", \"data\": [[5.1, 3.5, 1.4, 0.2]]}}]}}'\n\n# Monitor with k9s\nk9s -n {config.namespace}\n```\n\n**Manual cleanup when ready:**\n```bash\n# Delete all resources\nkubectl delete all --all -n {config.namespace}\n\n# Or delete namespace (if not using seldon-mesh)\nkubectl delete namespace {config.namespace}\n```\n\"\"\"))\n\n# Attach callbacks\ncleanup_button.on_click(on_cleanup_click)\nkeep_button.on_click(on_keep_click)\n\n# Display UI\ndisplay(widgets.HBox([keep_button, cleanup_button]))\ndisplay(output)\n\n# Final message\nlog(\"üéâ Seldon Core 2 Production MLOps Platform demonstration complete!\", \"SUCCESS\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}