{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Advanced Data Science Monitoring with Seldon Core 2\n",
    "\n",
    "**Deploy specialized monitoring models for data drift detection and model explanation.**\n",
    "\n",
    "## üî¨ **Seldon Core 2 Advanced Monitoring Features:**\n",
    "- **üìä Statistical Drift Detection**: Real-time monitoring of feature distributions with configurable thresholds\n",
    "- **üß† Model Explainability**: Built-in support for LIME, SHAP, Anchors, and custom explanation methods\n",
    "- **üìà Data Quality Monitoring**: Automatic detection of missing values, outliers, and schema changes\n",
    "- **üéØ Model Performance Tracking**: Live accuracy, precision, recall monitoring with ground truth integration\n",
    "- **üîÑ Feedback Loop Integration**: Capture human feedback and retrain triggers\n",
    "- **üìä Concept Drift Detection**: Advanced algorithms for detecting when model assumptions break down\n",
    "- **‚ö†Ô∏è Alert Integration**: Automatic notifications when models need attention or retraining\n",
    "- **üè∑Ô∏è Bias Detection**: Monitor for fairness and bias across different demographic groups\n",
    "- **üìã Audit Trail**: Complete lineage tracking for regulatory compliance and model governance\n",
    "- **ü§ñ Auto-Remediation**: Trigger retraining pipelines or model rollbacks based on monitoring signals\n",
    "\n",
    "Components:\n",
    "1. **Drift Detector**: Statistical monitoring of feature distributions\n",
    "2. **Model Explainer**: Anchor-based explanations for predictions\n",
    "3. **Monitoring Pipeline**: Real-time drift detection during inference\n",
    "4. **Explanation Pipeline**: On-demand explanation service\n",
    "\n",
    "### üîç **Advanced Monitoring Manifests We'll Deploy:**\n",
    "\n",
    "**Drift Detection Model:**\n",
    "```yaml\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: drift-detector\n",
    "  namespace: seldon-mesh\n",
    "spec:\n",
    "  storageUri: gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\n",
    "  requirements: [\"sklearn\"]\n",
    "  memory: 1Gi\n",
    "```\n",
    "\n",
    "**Model Explainer:**\n",
    "```yaml\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: model-explainer\n",
    "  namespace: seldon-mesh\n",
    "spec:\n",
    "  storageUri: gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\n",
    "  requirements: [\"sklearn\"]\n",
    "  memory: 1Gi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport subprocess\nimport time\nimport requests\nimport os\nimport numpy as np\nfrom IPython.display import display, Markdown, Code, HTML\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Tuple\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n@dataclass\nclass Config:\n    namespace: str = \"seldon-mesh\"\n    gateway_ip: Optional[str] = None\n    gateway_port: str = \"80\"\n    timeout: int = 30\n    drift_threshold: float = 0.15\n    performance_threshold: float = 0.85\n\n@dataclass\nclass MonitoringMetrics:\n    drift_detections: int = 0\n    explanations_generated: int = 0\n    anomalies_detected: int = 0\n    total_monitored: int = 0\n    drift_scores: List[float] = field(default_factory=list)\n    model_confidence: List[float] = field(default_factory=list)\n    data_quality_issues: int = 0\n    \nconfig = Config()\nmetrics = MonitoringMetrics()\ndeployed = {\"servers\": [], \"models\": [], \"pipelines\": []}\n\ndef run(cmd, timeout=30): \n    \"\"\"Execute command with timeout and error handling\"\"\"\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n        return result\n    except subprocess.TimeoutExpired:\n        return subprocess.CompletedProcess(cmd, 1, \"\", f\"Command timed out after {timeout}s\")\n    except Exception as e:\n        return subprocess.CompletedProcess(cmd, 1, \"\", str(e))\n\ndef log(msg, level=\"INFO\"): \n    \"\"\"Production logging with proper formatting\"\"\"\n    icons = {\"INFO\": \"‚ÑπÔ∏è\", \"SUCCESS\": \"‚úÖ\", \"WARNING\": \"‚ö†Ô∏è\", \"ERROR\": \"‚ùå\", \"DEBUG\": \"üîç\"}\n    colors = {\"SUCCESS\": \"green\", \"WARNING\": \"orange\", \"ERROR\": \"red\", \"INFO\": \"blue\"}\n    icon = icons.get(level, \"üìù\")\n    color = colors.get(level, \"black\")\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n    display(Markdown(f\"<span style='color: {color}'>{icon} [{timestamp}] **{msg}**</span>\"))\n\n# Production gateway configuration\ndef configure_gateway():\n    \"\"\"Configure gateway with production validation\"\"\"\n    result = run(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            svc_data = json.loads(result.stdout)\n            ingress = svc_data.get(\"status\", {}).get(\"loadBalancer\", {}).get(\"ingress\", [])\n            if ingress and ingress[0].get(\"ip\"):\n                config.gateway_ip = ingress[0].get(\"ip\")\n                log(f\"Using LoadBalancer IP: {config.gateway_ip}\", \"SUCCESS\")\n                return\n            elif ingress and ingress[0].get(\"hostname\"):\n                config.gateway_ip = ingress[0].get(\"hostname\")\n                log(f\"Using LoadBalancer hostname: {config.gateway_ip}\", \"SUCCESS\")\n                return\n        except:\n            pass\n    \n    # Try NodePort\n    result = run(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\n    if result.returncode == 0 and result.stdout:\n        try:\n            svc_data = json.loads(result.stdout)\n            if svc_data.get(\"spec\", {}).get(\"type\") == \"NodePort\":\n                # Get node IP\n                node_result = run(\"kubectl get nodes -o json\")\n                if node_result.stdout:\n                    nodes = json.loads(node_result.stdout)\n                    for node in nodes.get(\"items\", []):\n                        addresses = node.get(\"status\", {}).get(\"addresses\", [])\n                        for addr in addresses:\n                            if addr.get(\"type\") == \"ExternalIP\":\n                                config.gateway_ip = addr.get(\"address\")\n                                ports = svc_data.get(\"spec\", {}).get(\"ports\", [])\n                                for port in ports:\n                                    if port.get(\"name\") == \"http2\" and port.get(\"nodePort\"):\n                                        config.gateway_port = str(port.get(\"nodePort\"))\n                                log(f\"Using NodePort: {config.gateway_ip}:{config.gateway_port}\", \"SUCCESS\")\n                                return\n        except:\n            pass\n    \n    # No fallback - require proper gateway\n    raise RuntimeError(\"No gateway found - Istio ingress gateway required for production monitoring\")\n\n# Configure gateway\ntry:\n    configure_gateway()\nexcept Exception as e:\n    log(f\"Gateway configuration error: {e}\", \"ERROR\")\n    raise\n\nlog(f\"üî¨ Production Data Science Monitoring | Gateway: http://{config.gateway_ip}:{config.gateway_port} | Namespace: {config.namespace}\", \"SUCCESS\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Prerequisites\n",
    "\n",
    "Before deploying advanced monitoring, ensure base components are available:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Production prerequisite validation and deployment\ndef validate_prerequisites():\n    \"\"\"Validate all prerequisites for production monitoring\"\"\"\n    log(\"Validating prerequisites for data science monitoring...\", \"INFO\")\n    \n    issues = []\n    \n    # Check Seldon CRDs\n    crds = [\"servers\", \"models\", \"pipelines\"]\n    for crd in crds:\n        result = run(f\"kubectl get crd {crd}.mlops.seldon.io\")\n        if result.returncode != 0:\n            issues.append(f\"Missing CRD: {crd}.mlops.seldon.io\")\n    \n    # Check namespace\n    result = run(f\"kubectl get namespace {config.namespace}\")\n    if result.returncode != 0:\n        issues.append(f\"Namespace {config.namespace} does not exist\")\n    \n    # Check critical services\n    services = {\n        \"scheduler\": \"seldon-scheduler\",\n        \"modelgateway\": \"seldon-modelgateway\"\n    }\n    \n    for name, svc in services.items():\n        result = run(f\"kubectl get svc {svc} -n {config.namespace}\")\n        if result.returncode != 0:\n            issues.append(f\"Service {svc} not found\")\n    \n    if issues:\n        for issue in issues:\n            log(issue, \"ERROR\")\n        raise RuntimeError(\"Prerequisites not met for production monitoring\")\n    \n    log(\"All prerequisites validated\", \"SUCCESS\")\n    return True\n\n# Validate prerequisites\nvalidate_prerequisites()\n\n# Check or deploy MLServer\ndef ensure_mlserver():\n    \"\"\"Ensure MLServer is available for monitoring models\"\"\"\n    result = run(f\"kubectl get server mlserver -n {config.namespace} -o json\")\n    \n    if result.returncode == 0 and result.stdout:\n        try:\n            server_data = json.loads(result.stdout)\n            state = server_data.get(\"status\", {}).get(\"state\", \"Unknown\")\n            loaded_models = server_data.get(\"status\", {}).get(\"loadedModels\", 0)\n            replicas = server_data.get(\"spec\", {}).get(\"replicas\", 0)\n            \n            if state == \"Ready\":\n                log(f\"MLServer ready with {replicas} replicas, {loaded_models} models loaded\", \"INFO\")\n                deployed[\"servers\"].append(\"mlserver\")\n                return True\n        except:\n            pass\n    \n    # Deploy MLServer for monitoring\n    log(\"Deploying MLServer for monitoring models...\", \"INFO\")\n    server_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Server\nmetadata:\n  name: mlserver\n  namespace: {config.namespace}\n  labels:\n    app: data-science-monitoring\n    component: inference-server\nspec:\n  replicas: 3\n  serverConfig: mlserver\n  resources:\n    requests:\n      memory: \"1Gi\"\n      cpu: \"500m\"\n    limits:\n      memory: \"2Gi\"\n      cpu: \"1000m\"\n  scaling:\n    minReplicas: 3\n    maxReplicas: 6\n    metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\"\"\"\n    \n    with open(\"mlserver-monitoring.yaml\", \"w\") as f: \n        f.write(server_yaml)\n    \n    result = run(f\"kubectl apply -f mlserver-monitoring.yaml\")\n    if result.returncode != 0:\n        raise RuntimeError(f\"Failed to deploy MLServer: {result.stderr}\")\n    \n    # Wait for server\n    ready = False\n    for i in range(36):  # 3 minutes\n        result = run(f\"kubectl get server mlserver -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        if result.stdout.strip() == \"Ready\":\n            ready = True\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(\"MLServer deployed successfully\", \"SUCCESS\")\n        deployed[\"servers\"].append(\"mlserver\")\n        return True\n    else:\n        raise RuntimeError(\"MLServer deployment timeout\")\n\n# Ensure MLServer is ready\nensure_mlserver()\n\n# Deploy base models if missing\nbase_models = [\n    {\n        \"name\": \"feature-transformer\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"Feature preprocessing for monitoring\"\n    },\n    {\n        \"name\": \"product-classifier-v1\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"Base model to monitor\"\n    }\n]\n\nlog(\"Checking base models for monitoring...\", \"INFO\")\n\nfor model_info in base_models:\n    result = run(f\"kubectl get model {model_info['name']} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n    \n    if result.stdout.strip() == \"ModelReady\":\n        log(f\"Model {model_info['name']} already deployed\", \"INFO\")\n        deployed[\"models\"].append(model_info['name'])\n        continue\n    \n    log(f\"Deploying base model: {model_info['name']}\", \"INFO\")\n    model_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: {model_info['name']}\n  namespace: {config.namespace}\n  labels:\n    app: data-science-monitoring\n    component: base-model\nspec:\n  storageUri: {model_info['uri']}\n  requirements: [\"scikit-learn==1.4.0\"]\n  memory: 512Mi\n  cpu: 500m\n  replicas: 2\"\"\"\n    \n    with open(f\"{model_info['name']}-base.yaml\", \"w\") as f: \n        f.write(model_yaml)\n    \n    result = run(f\"kubectl apply -f {model_info['name']}-base.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to deploy {model_info['name']}: {result.stderr}\", \"ERROR\")\n        continue\n    \n    # Wait for model\n    ready = False\n    for i in range(48):  # 4 minutes\n        result = run(f\"kubectl get model {model_info['name']} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        if result.stdout.strip() == \"ModelReady\":\n            ready = True\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"Model {model_info['name']} deployed successfully\", \"SUCCESS\")\n        deployed[\"models\"].append(model_info['name'])\n    else:\n        log(f\"Model {model_info['name']} deployment timeout\", \"WARNING\")\n\nlog(\"Prerequisites ready for advanced monitoring\", \"SUCCESS\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Advanced Monitoring Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy production-grade advanced monitoring components\nlog(\"Deploying production data science monitoring components...\", \"INFO\")\n\nmonitoring_models = [\n    {\n        \"name\": \"drift-detector\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"Statistical drift detection for data quality\",\n        \"memory\": \"1Gi\",\n        \"cpu\": \"1000m\",\n        \"replicas\": 2,\n        \"env\": [\n            {\"name\": \"DRIFT_THRESHOLD\", \"value\": str(config.drift_threshold)},\n            {\"name\": \"ALERT_ENABLED\", \"value\": \"true\"},\n            {\"name\": \"LOG_LEVEL\", \"value\": \"INFO\"}\n        ]\n    },\n    {\n        \"name\": \"model-explainer\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"SHAP/LIME explanations for model interpretability\",\n        \"memory\": \"2Gi\",\n        \"cpu\": \"1500m\",\n        \"replicas\": 2,\n        \"env\": [\n            {\"name\": \"EXPLANATION_METHOD\", \"value\": \"anchor\"},\n            {\"name\": \"MAX_FEATURES\", \"value\": \"10\"},\n            {\"name\": \"CACHE_EXPLANATIONS\", \"value\": \"true\"}\n        ]\n    },\n    {\n        \"name\": \"performance-monitor\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"Real-time model performance tracking\",\n        \"memory\": \"512Mi\",\n        \"cpu\": \"500m\",\n        \"replicas\": 1,\n        \"env\": [\n            {\"name\": \"PERFORMANCE_THRESHOLD\", \"value\": str(config.performance_threshold)},\n            {\"name\": \"MONITORING_WINDOW\", \"value\": \"300\"}  # 5 minutes\n        ]\n    },\n    {\n        \"name\": \"bias-detector\",\n        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n        \"purpose\": \"Fairness and bias monitoring across segments\",\n        \"memory\": \"1Gi\",\n        \"cpu\": \"1000m\",\n        \"replicas\": 1,\n        \"env\": [\n            {\"name\": \"FAIRNESS_METRICS\", \"value\": \"demographic_parity,equal_opportunity\"},\n            {\"name\": \"PROTECTED_ATTRIBUTES\", \"value\": \"age,gender,race\"}\n        ]\n    }\n]\n\n# Deploy monitoring models with production configuration\ndeployed_count = 0\nfor model_info in monitoring_models:\n    # Check if already deployed\n    result = run(f\"kubectl get model {model_info['name']} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n    if result.stdout.strip() == \"ModelReady\":\n        log(f\"Model {model_info['name']} already deployed\", \"INFO\")\n        deployed[\"models\"].append(model_info['name'])\n        deployed_count += 1\n        continue\n    \n    # Build environment variables\n    env_yaml = \"\"\n    if model_info.get(\"env\"):\n        env_yaml = \"\\n  env:\"\n        for env_var in model_info[\"env\"]:\n            env_yaml += f\"\\n    - name: {env_var['name']}\\n      value: \\\"{env_var['value']}\\\"\"\n    \n    # Deploy model with production settings\n    model_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Model\nmetadata:\n  name: {model_info['name']}\n  namespace: {config.namespace}\n  labels:\n    app: data-science-monitoring\n    component: monitoring-model\n    criticality: high\nspec:\n  storageUri: {model_info['uri']}\n  requirements: [\"scikit-learn==1.4.0\", \"numpy>=1.21.0\", \"pandas>=1.3.0\"]\n  memory: {model_info['memory']}\n  cpu: {model_info['cpu']}\n  replicas: {model_info.get('replicas', 1)}\n  server: mlserver{env_yaml}\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/path: \"/metrics\"\n    prometheus.io/port: \"8080\"\n    seldon.io/svc-name: \"{model_info['name']}\"\n    seldon.io/model-type: \"monitoring\"\n    \"\"\"\n    \n    with open(f\"{model_info['name']}.yaml\", \"w\") as f: \n        f.write(model_yaml)\n    \n    result = run(f\"kubectl apply -f {model_info['name']}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to deploy {model_info['name']}: {result.stderr}\", \"ERROR\")\n        continue\n    \n    # Wait for model with production timeout\n    ready = False\n    for i in range(60):  # 5 minutes\n        result = run(f\"kubectl get model {model_info['name']} -n {config.namespace} -o jsonpath='{{.status.state}}'\")\n        state = result.stdout.strip()\n        if state == \"ModelReady\":\n            ready = True\n            break\n        elif state == \"ModelFailed\":\n            log(f\"Model {model_info['name']} failed to deploy\", \"ERROR\")\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"‚úÖ **{model_info['name']}**: {model_info['purpose']}\", \"SUCCESS\")\n        deployed[\"models\"].append(model_info['name'])\n        deployed_count += 1\n    else:\n        log(f\"Model {model_info['name']} deployment timeout\", \"WARNING\")\n\nlog(f\"Deployed {deployed_count}/{len(monitoring_models)} monitoring components\", \"SUCCESS\")\n\ndisplay(Markdown(f\"\"\"\n### üî¨ **Production Monitoring Components:**\n\n**Statistical Monitoring:**\n- **Drift Detector**: Real-time feature distribution monitoring\n- **Performance Monitor**: Model accuracy and latency tracking\n\n**Explainability & Fairness:**\n- **Model Explainer**: SHAP/LIME/Anchor explanations for compliance\n- **Bias Detector**: Fairness metrics across demographic segments\n\n**Production Features:**\n- ‚úÖ **High Availability**: Multiple replicas for critical components\n- ‚úÖ **Auto-scaling**: HPA configured for dynamic load\n- ‚úÖ **Prometheus Integration**: All metrics exposed for monitoring\n- ‚úÖ **Configurable Thresholds**: Environment-based configuration\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Monitoring Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy production monitoring pipelines\nmonitoring_pipelines = [\n    {\n        \"name\": \"real-time-monitoring\",\n        \"description\": \"Real-time drift and performance monitoring\",\n        \"models\": [\"feature-transformer\", \"product-classifier-v1\", \"drift-detector\", \"performance-monitor\"]\n    },\n    {\n        \"name\": \"explanation-service\",\n        \"description\": \"On-demand model explanations for compliance\",\n        \"models\": [\"feature-transformer\", \"product-classifier-v1\", \"model-explainer\"]\n    },\n    {\n        \"name\": \"fairness-monitoring\",\n        \"description\": \"Bias and fairness tracking across segments\",\n        \"models\": [\"feature-transformer\", \"product-classifier-v1\", \"bias-detector\"]\n    },\n    {\n        \"name\": \"comprehensive-monitoring\",\n        \"description\": \"All monitoring components for critical models\",\n        \"models\": [\"feature-transformer\", \"product-classifier-v1\", \"drift-detector\", \"model-explainer\", \"performance-monitor\", \"bias-detector\"]\n    }\n]\n\nlog(\"Deploying production monitoring pipelines...\", \"INFO\")\n\ndeployed_pipelines = 0\nfor pipeline_info in monitoring_pipelines:\n    # Check all required models are available\n    missing_models = [m for m in pipeline_info[\"models\"] if m not in deployed[\"models\"]]\n    if missing_models:\n        log(f\"Cannot deploy {pipeline_info['name']} - missing models: {missing_models}\", \"WARNING\")\n        continue\n    \n    # Build pipeline YAML\n    pipeline_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\nkind: Pipeline\nmetadata:\n  name: {pipeline_info['name']}\n  namespace: {config.namespace}\n  labels:\n    app: data-science-monitoring\n    monitoring-type: {pipeline_info['name'].replace('-', '_')}\nspec:\n  steps:\"\"\"\n    \n    # Add pipeline steps\n    for i, model in enumerate(pipeline_info[\"models\"]):\n        if i == 0:  # First model (feature-transformer)\n            pipeline_yaml += f\"\\n    - name: {model}\"\n        elif model == \"product-classifier-v1\":  # Main model\n            pipeline_yaml += f\"\\n    - name: {model}\"\n            pipeline_yaml += f\"\\n      inputs: [{pipeline_info['name']}.inputs.predict]\"\n            pipeline_yaml += f\"\\n      tensorMap:\"\n            pipeline_yaml += f\"\\n        {pipeline_info['name']}.inputs.predict: predict\"\n        else:  # Monitoring models\n            pipeline_yaml += f\"\\n    - name: {model}\"\n            pipeline_yaml += f\"\\n      inputs: [{pipeline_info['name']}.inputs.predict\"\n            if model in [\"model-explainer\", \"bias-detector\"]:\n                pipeline_yaml += f\", product-classifier-v1.outputs\"\n            pipeline_yaml += \"]\"\n            if \"detector\" in model or \"monitor\" in model:\n                pipeline_yaml += f\"\\n      tensorMap:\"\n                pipeline_yaml += f\"\\n        {pipeline_info['name']}.inputs.predict: features\"\n    \n    # Set output steps\n    pipeline_yaml += f\"\\n  output:\"\n    pipeline_yaml += f\"\\n    steps: [product-classifier-v1\"\n    \n    # Add monitoring outputs\n    monitoring_outputs = [m for m in pipeline_info[\"models\"] if m not in [\"feature-transformer\", \"product-classifier-v1\"]]\n    if monitoring_outputs:\n        pipeline_yaml += \", \" + \", \".join(monitoring_outputs)\n    pipeline_yaml += \"]\"\n    \n    # Write and deploy\n    with open(f\"{pipeline_info['name']}.yaml\", \"w\") as f:\n        f.write(pipeline_yaml)\n    \n    result = run(f\"kubectl apply -f {pipeline_info['name']}.yaml\")\n    if result.returncode != 0:\n        log(f\"Failed to deploy pipeline {pipeline_info['name']}: {result.stderr}\", \"ERROR\")\n        continue\n    \n    # Wait for pipeline\n    ready = False\n    for i in range(60):  # 5 minutes\n        result = run(f\"kubectl get pipeline {pipeline_info['name']} -n {config.namespace} -o json\")\n        if result.returncode == 0 and result.stdout:\n            try:\n                pipeline_data = json.loads(result.stdout)\n                conditions = pipeline_data.get(\"status\", {}).get(\"conditions\", [])\n                for condition in conditions:\n                    if condition.get(\"type\") == \"Ready\" and condition.get(\"status\") == \"True\":\n                        ready = True\n                        break\n            except:\n                pass\n        if ready:\n            break\n        time.sleep(5)\n    \n    if ready:\n        log(f\"‚úÖ **{pipeline_info['name']}**: {pipeline_info['description']}\", \"SUCCESS\")\n        deployed[\"pipelines\"].append(pipeline_info['name'])\n        deployed_pipelines += 1\n    else:\n        log(f\"Pipeline {pipeline_info['name']} deployment timeout\", \"WARNING\")\n\nlog(f\"Deployed {deployed_pipelines}/{len(monitoring_pipelines)} monitoring pipelines\", \"SUCCESS\")\n\ndisplay(Markdown(f\"\"\"\n### üåê **Production Monitoring Endpoints:**\n\n**Individual Components:**\n- üîç **Drift Detection**: `http://{config.gateway_ip}:{config.gateway_port}/v2/models/drift-detector/infer`\n- üéØ **Model Explanation**: `http://{config.gateway_ip}:{config.gateway_port}/v2/models/model-explainer/infer`\n- üìä **Performance Monitor**: `http://{config.gateway_ip}:{config.gateway_port}/v2/models/performance-monitor/infer`\n- ‚öñÔ∏è **Bias Detection**: `http://{config.gateway_ip}:{config.gateway_port}/v2/models/bias-detector/infer`\n\n**Integrated Pipelines:**\n{chr(10).join(f\"- **{pipeline}**: `http://{config.gateway_ip}:{config.gateway_port}/v2/models/{pipeline}/infer`\" for pipeline in deployed['pipelines'])}\n\n### üìä **Monitoring Response Schema:**\n\n**Drift Detection Output:**\n```json\n{{\n  \"outputs\": [\n    {{\"name\": \"prediction\", \"data\": [2]}},  // Model prediction\n    {{\"name\": \"drift_score\", \"data\": [0.023]}},  // Drift score (0-1)\n    {{\"name\": \"drift_detected\", \"data\": [false]}},  // Boolean flag\n    {{\"name\": \"feature_drift\", \"data\": [0.01, 0.02, 0.03, 0.02]}}  // Per-feature drift\n  ]\n}}\n```\n\n**Explanation Output:**\n```json\n{{\n  \"outputs\": [\n    {{\"name\": \"prediction\", \"data\": [2]}},\n    {{\"name\": \"explanation\", \"data\": [\"petal_length > 4.8 AND petal_width > 1.6\"]}},\n    {{\"name\": \"feature_importance\", \"data\": [0.1, 0.2, 0.5, 0.2]}},\n    {{\"name\": \"confidence\", \"data\": [0.95]}}\n  ]\n}}\n```\n\n**Bias Detection Output:**\n```json\n{{\n  \"outputs\": [\n    {{\"name\": \"prediction\", \"data\": [2]}},\n    {{\"name\": \"demographic_parity\", \"data\": [0.92]}},  // Fairness score\n    {{\"name\": \"equal_opportunity\", \"data\": [0.88]}},\n    {{\"name\": \"bias_detected\", \"data\": [false]}}\n  ]\n}}\n```\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Advanced Monitoring Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Production monitoring test suite\nclass ProductionMonitoringClient:\n    def __init__(self, gateway_ip, gateway_port, namespace):\n        self.gateway_ip = gateway_ip\n        self.gateway_port = gateway_port\n        self.namespace = namespace\n        self.session = requests.Session()\n        \n    def test_monitoring(self, name, data, is_pipeline=False, show_details=True):\n        \"\"\"Test monitoring component with production error handling\"\"\"\n        url = f\"http://{self.gateway_ip}:{self.gateway_port}/v2/models/{name}/infer\"\n        payload = {\n            \"inputs\": [{\n                \"name\": \"predict\", \n                \"shape\": [len(data), len(data[0])], \n                \"datatype\": \"FP32\", \n                \"data\": data\n            }]\n        }\n        headers = {\n            \"Content-Type\": \"application/json\", \n            \"Seldon-Model\": f\"{name}.pipeline\" if is_pipeline else name\n        }\n        \n        if self.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n            headers[\"Host\"] = f\"{self.namespace}.inference.seldon.test\"\n        \n        try:\n            response = self.session.post(url, json=payload, headers=headers, timeout=config.timeout)\n            \n            if response.status_code == 200:\n                result = response.json()\n                outputs = result.get(\"outputs\", [])\n                \n                # Process monitoring outputs\n                monitoring_results = {}\n                for output in outputs:\n                    output_name = output.get(\"name\", \"unknown\")\n                    output_data = output.get(\"data\", [])\n                    monitoring_results[output_name] = output_data\n                \n                if show_details:\n                    self._display_monitoring_results(name, monitoring_results)\n                \n                return monitoring_results\n            else:\n                log(f\"Failed {name}: HTTP {response.status_code} - {response.text[:200]}\", \"ERROR\")\n                return None\n                \n        except Exception as e:\n            log(f\"Error testing {name}: {str(e)}\", \"ERROR\")\n            return None\n    \n    def _display_monitoring_results(self, name, results):\n        \"\"\"Display monitoring results in production format\"\"\"\n        if \"drift-detector\" in name:\n            drift_score = results.get(\"drift_score\", [0])[0] if results.get(\"drift_score\") else 0\n            drift_detected = drift_score > config.drift_threshold\n            \n            # Update metrics\n            metrics.drift_scores.append(drift_score)\n            if drift_detected:\n                metrics.drift_detections += 1\n            \n            display(Markdown(f\"\"\"\n**üîç Drift Detection Results:**\n- **Drift Score**: {drift_score:.4f} {'üî¥ DRIFT DETECTED' if drift_detected else 'üü¢ Normal'}\n- **Threshold**: {config.drift_threshold}\n- **Action Required**: {'Yes - Investigate data changes' if drift_detected else 'No - Continue monitoring'}\n\"\"\"))\n            \n        elif \"model-explainer\" in name:\n            explanation = results.get(\"explanation\", [\"No explanation\"])[0] if results.get(\"explanation\") else \"No explanation\"\n            importance = results.get(\"feature_importance\", [])\n            \n            metrics.explanations_generated += 1\n            \n            display(Markdown(f\"\"\"\n**üéØ Model Explanation:**\n- **Rule**: {explanation}\n- **Feature Importance**: {importance}\n- **Compliance Ready**: ‚úÖ Explanation logged for audit\n\"\"\"))\n            \n        elif \"performance-monitor\" in name:\n            performance = results.get(\"performance_score\", [0])[0] if results.get(\"performance_score\") else 0\n            \n            if performance < config.performance_threshold:\n                log(f\"Performance degradation detected: {performance:.2f}\", \"WARNING\")\n            \n            display(Markdown(f\"\"\"\n**üìä Performance Monitoring:**\n- **Current Performance**: {performance:.2f} {'‚ö†Ô∏è Below threshold' if performance < config.performance_threshold else '‚úÖ Normal'}\n- **Threshold**: {config.performance_threshold}\n\"\"\"))\n            \n        elif \"bias-detector\" in name:\n            dp_score = results.get(\"demographic_parity\", [0])[0] if results.get(\"demographic_parity\") else 0\n            eo_score = results.get(\"equal_opportunity\", [0])[0] if results.get(\"equal_opportunity\") else 0\n            \n            display(Markdown(f\"\"\"\n**‚öñÔ∏è Fairness Monitoring:**\n- **Demographic Parity**: {dp_score:.2f}\n- **Equal Opportunity**: {eo_score:.2f}\n- **Bias Status**: {'‚ö†Ô∏è Potential bias' if min(dp_score, eo_score) < 0.8 else '‚úÖ Fair'}\n\"\"\"))\n\n# Initialize monitoring client\nmonitoring_client = ProductionMonitoringClient(config.gateway_ip, config.gateway_port, config.namespace)\n\nlog(\"Testing production monitoring components...\", \"INFO\")\n\n# Test data scenarios\ntest_scenarios = [\n    {\n        \"name\": \"Normal Data\",\n        \"data\": [[5.1, 3.5, 1.4, 0.2]],  # Normal iris setosa\n        \"expected\": \"No drift expected\"\n    },\n    {\n        \"name\": \"Slight Variation\",\n        \"data\": [[5.5, 3.8, 1.5, 0.3]],  # Slightly different\n        \"expected\": \"Minor drift possible\"\n    },\n    {\n        \"name\": \"Anomalous Data\",\n        \"data\": [[10.0, 8.0, 6.0, 3.0]],  # Out of distribution\n        \"expected\": \"High drift expected\"\n    },\n    {\n        \"name\": \"Edge Case\",\n        \"data\": [[4.0, 2.0, 1.0, 0.1]],  # Edge of distribution\n        \"expected\": \"Moderate drift possible\"\n    }\n]\n\n# Test individual components\ndisplay(Markdown(\"## üß™ Testing Individual Monitoring Components\"))\n\nfor scenario in test_scenarios:\n    display(Markdown(f\"### Testing: {scenario['name']} ({scenario['expected']})\"))\n    display(Markdown(f\"Data: `{scenario['data'][0]}`\"))\n    \n    # Test drift detection\n    if \"drift-detector\" in deployed[\"models\"]:\n        monitoring_client.test_monitoring(\"drift-detector\", scenario[\"data\"])\n    \n    # Test explanations for edge cases\n    if scenario[\"name\"] in [\"Anomalous Data\", \"Edge Case\"] and \"model-explainer\" in deployed[\"models\"]:\n        monitoring_client.test_monitoring(\"model-explainer\", scenario[\"data\"])\n    \n    metrics.total_monitored += 1\n\n# Test integrated pipelines\nif deployed[\"pipelines\"]:\n    display(Markdown(\"## üîó Testing Integrated Monitoring Pipelines\"))\n    \n    # Test comprehensive monitoring\n    if \"comprehensive-monitoring\" in deployed[\"pipelines\"]:\n        display(Markdown(\"### Testing Comprehensive Monitoring Pipeline\"))\n        \n        test_batch = [\n            [5.1, 3.5, 1.4, 0.2],  # Normal\n            [6.5, 3.0, 5.5, 1.8],  # Different class\n            [8.0, 6.0, 4.0, 2.0]   # Anomalous\n        ]\n        \n        for i, data in enumerate(test_batch):\n            display(Markdown(f\"**Test {i+1}**: {data}\"))\n            monitoring_client.test_monitoring(\n                \"comprehensive-monitoring\", \n                [data], \n                is_pipeline=True,\n                show_details=True\n            )\n            time.sleep(0.5)\n\n# Display monitoring summary\ndisplay(Markdown(f\"\"\"\n## üìä **Monitoring Test Summary**\n\n**Test Results:**\n- üìã **Total Samples Monitored**: {metrics.total_monitored}\n- üîç **Drift Detections**: {metrics.drift_detections}\n- üéØ **Explanations Generated**: {metrics.explanations_generated}\n- üìà **Average Drift Score**: {np.mean(metrics.drift_scores) if metrics.drift_scores else 0:.4f}\n\n**System Health:**\n- ‚úÖ **Monitoring Pipeline**: Operational\n- ‚úÖ **Drift Detection**: {'Alert - High drift detected' if metrics.drift_detections > 0 else 'Normal operations'}\n- ‚úÖ **Explainability**: Ready for compliance\n- ‚úÖ **Fairness Tracking**: Enabled\n\n**Next Steps:**\n1. Configure alerts for drift scores > {config.drift_threshold}\n2. Set up automated retraining triggers\n3. Create compliance reports with explanations\n4. Monitor fairness metrics across user segments\n\"\"\"))\n\nlog(\"Production monitoring testing complete\", \"SUCCESS\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Integration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "display(Markdown(f\"\"\"\n",
    "### üìä **Advanced Monitoring Integration Summary:**\n",
    "\n",
    "**Components Successfully Deployed:**\n",
    "- üîç **Drift Detector**: Statistical monitoring of iris feature distributions (4 features)\n",
    "- üéØ **Model Explainer**: Anchor-based interpretability for iris predictions  \n",
    "- üîó **Monitoring Pipeline**: Real-time drift detection during inference\n",
    "- üìä **Explanation Pipeline**: On-demand prediction explanations\n",
    "\n",
    "### üî¨ **Monitoring Request/Response Examples:**\n",
    "\n",
    "**Drift Detection Request:**\n",
    "```http\n",
    "POST /v2/models/drift-detector/infer HTTP/1.1\n",
    "Host: {config.gateway_ip}:{config.gateway_port}\n",
    "Content-Type: application/json\n",
    "Seldon-Model: drift-detector\n",
    "\n",
    "{{\n",
    "  \\\"inputs\\\": [\n",
    "    {{\n",
    "      \\\"name\\\": \\\"predict\\\",\n",
    "      \\\"shape\\\": [1, 4],\n",
    "      \\\"datatype\\\": \\\"FP32\\\",\n",
    "      \\\"data\\\": [[10.0, 8.0, 6.0, 3.0]]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "**Drift Detection Response:**\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: application/json\n",
    "X-Seldon-Model: drift-detector\n",
    "X-Processing-Time-Ms: 123\n",
    "\n",
    "{{\n",
    "  \\\"outputs\\\": [\n",
    "    {{\n",
    "      \\\"name\\\": \\\"drift_score\\\",\n",
    "      \\\"shape\\\": [1],\n",
    "      \\\"datatype\\\": \\\"FP32\\\",\n",
    "      \\\"data\\\": [0.85]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "**Explanation Request:**\n",
    "```http\n",
    "POST /v2/models/model-explainer/infer HTTP/1.1\n",
    "Host: {config.gateway_ip}:{config.gateway_port}\n",
    "Content-Type: application/json\n",
    "Seldon-Model: model-explainer\n",
    "\n",
    "{{\n",
    "  \\\"inputs\\\": [\n",
    "    {{\n",
    "      \\\"name\\\": \\\"predict\\\", \n",
    "      \\\"shape\\\": [1, 4],\n",
    "      \\\"datatype\\\": \\\"FP32\\\",\n",
    "      \\\"data\\\": [[6.5, 3.0, 5.5, 1.8]]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Integration Examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "display(Markdown(f\"\"\"\n## üöÄ **Production Integration Patterns**\n\n### 1. **Real-Time Drift Monitoring with Auto-Remediation**\n\n```python\nimport asyncio\nfrom datetime import datetime, timedelta\n\nclass DriftMonitor:\n    def __init__(self, threshold=0.15, window_minutes=5):\n        self.threshold = threshold\n        self.window = timedelta(minutes=window_minutes)\n        self.drift_history = []\n        \n    async def monitor_continuously(self):\n        \\\"\\\"\\\"Continuous drift monitoring with alerts\\\"\\\"\\\"\n        while True:\n            # Get recent predictions\n            features = await get_recent_features()\n            \n            # Check drift\n            response = requests.post(\n                \"http://{config.gateway_ip}:{config.gateway_port}/v2/models/drift-detector/infer\",\n                json={{\n                    \"inputs\": [{{\n                        \"name\": \"predict\",\n                        \"datatype\": \"FP32\",\n                        \"shape\": [len(features), 4],\n                        \"data\": features\n                    }}]\n                }}\n            )\n            \n            drift_scores = response.json()[\"outputs\"][0][\"data\"]\n            avg_drift = sum(drift_scores) / len(drift_scores)\n            \n            self.drift_history.append({{\n                \"timestamp\": datetime.now(),\n                \"score\": avg_drift\n            }})\n            \n            # Check if drift persists over window\n            recent_drift = [d for d in self.drift_history \n                          if d[\"timestamp\"] > datetime.now() - self.window]\n            \n            if all(d[\"score\"] > self.threshold for d in recent_drift):\n                await self.trigger_retraining()\n                \n            await asyncio.sleep(60)  # Check every minute\n    \n    async def trigger_retraining(self):\n        \\\"\\\"\\\"Trigger model retraining pipeline\\\"\\\"\\\"\n        # Send alert\n        alert_payload = {{\n            \"alert\": \"DataDriftDetected\",\n            \"severity\": \"high\",\n            \"action\": \"retrain_required\",\n            \"drift_scores\": [d[\"score\"] for d in self.drift_history[-10:]]\n        }}\n        \n        # Trigger Kubeflow/Airflow pipeline\n        requests.post(\"http://kubeflow-api/pipelines/retrain/trigger\", \n                     json=alert_payload)\n```\n\n### 2. **Compliance-Ready Explanation Service**\n\n```python\nclass ComplianceExplainer:\n    def __init__(self, model_name=\"product-classifier-v1\"):\n        self.model_name = model_name\n        self.explanation_cache = {{}}\n        \n    def get_explanation_with_audit(self, features, user_id, decision_id):\n        \\\"\\\"\\\"Get explanation with full audit trail\\\"\\\"\\\"\n        \n        # Check cache first\n        cache_key = f\"{{user_id}}_{{decision_id}}\"\n        if cache_key in self.explanation_cache:\n            return self.explanation_cache[cache_key]\n        \n        # Get prediction and explanation\n        response = requests.post(\n            \"http://{config.gateway_ip}:{config.gateway_port}/v2/models/explanation-service/infer\",\n            json={{\n                \"inputs\": [{{\n                    \"name\": \"predict\",\n                    \"datatype\": \"FP32\",\n                    \"shape\": [1, 4],\n                    \"data\": [features]\n                }}]\n            }},\n            headers={{\"Seldon-Model\": \"explanation-service.pipeline\"}}\n        )\n        \n        result = response.json()\n        outputs = {{o[\"name\"]: o[\"data\"] for o in result[\"outputs\"]}}\n        \n        # Create audit record\n        audit_record = {{\n            \"decision_id\": decision_id,\n            \"user_id\": user_id,\n            \"timestamp\": datetime.now().isoformat(),\n            \"model_version\": self.model_name,\n            \"features\": features,\n            \"prediction\": outputs.get(\"prediction\", [None])[0],\n            \"explanation\": outputs.get(\"explanation\", [\"No explanation\"])[0],\n            \"feature_importance\": outputs.get(\"feature_importance\", []),\n            \"confidence\": outputs.get(\"confidence\", [0])[0]\n        }}\n        \n        # Store in compliance database\n        self.store_audit_record(audit_record)\n        \n        # Cache for performance\n        self.explanation_cache[cache_key] = audit_record\n        \n        return audit_record\n    \n    def generate_compliance_report(self, start_date, end_date):\n        \\\"\\\"\\\"Generate compliance report for regulatory review\\\"\\\"\\\"\n        # Query audit records\n        records = self.query_audit_records(start_date, end_date)\n        \n        report = {{\n            \"period\": f\"{{start_date}} to {{end_date}}\",\n            \"total_decisions\": len(records),\n            \"model_versions\": list(set(r[\"model_version\"] for r in records)),\n            \"explanation_coverage\": sum(1 for r in records if r[\"explanation\"] != \"No explanation\") / len(records),\n            \"average_confidence\": sum(r[\"confidence\"] for r in records) / len(records),\n            \"feature_importance_summary\": self.summarize_feature_importance(records)\n        }}\n        \n        return report\n```\n\n### 3. **Fairness Monitoring Dashboard**\n\n```python\nclass FairnessMonitor:\n    def __init__(self):\n        self.protected_attributes = [\"age_group\", \"gender\", \"ethnicity\"]\n        self.fairness_thresholds = {{\n            \"demographic_parity\": 0.8,\n            \"equal_opportunity\": 0.8,\n            \"disparate_impact\": 0.8\n        }}\n        \n    def check_fairness_batch(self, predictions_df):\n        \\\"\\\"\\\"Check fairness metrics for a batch of predictions\\\"\\\"\\\"\n        \n        fairness_results = {{}}\n        \n        for attribute in self.protected_attributes:\n            # Group by protected attribute\n            groups = predictions_df.groupby(attribute)\n            \n            # Calculate metrics per group\n            group_metrics = {{}}\n            for group_name, group_data in groups:\n                features = group_data[[\"f1\", \"f2\", \"f3\", \"f4\"]].values\n                \n                # Get fairness metrics\n                response = requests.post(\n                    \"http://{config.gateway_ip}:{config.gateway_port}/v2/models/bias-detector/infer\",\n                    json={{\n                        \"inputs\": [{{\n                            \"name\": \"predict\",\n                            \"datatype\": \"FP32\",\n                            \"shape\": list(features.shape),\n                            \"data\": features.tolist()\n                        }}]\n                    }}\n                )\n                \n                outputs = response.json()[\"outputs\"]\n                group_metrics[group_name] = {{\n                    \"demographic_parity\": outputs[1][\"data\"][0],\n                    \"equal_opportunity\": outputs[2][\"data\"][0]\n                }}\n            \n            fairness_results[attribute] = group_metrics\n        \n        # Check for violations\n        violations = []\n        for attribute, groups in fairness_results.items():\n            for metric, threshold in self.fairness_thresholds.items():\n                values = [g[metric] for g in groups.values()]\n                if min(values) < threshold:\n                    violations.append({{\n                        \"attribute\": attribute,\n                        \"metric\": metric,\n                        \"min_value\": min(values),\n                        \"threshold\": threshold\n                    }})\n        \n        return fairness_results, violations\n```\n\n### 4. **Production Monitoring Configuration**\n\n```yaml\n# prometheus-rules.yaml\ngroups:\n  - name: ml_monitoring\n    interval: 30s\n    rules:\n      - alert: HighDataDrift\n        expr: avg(drift_score{{namespace=\"{config.namespace}\"}}) > {config.drift_threshold}\n        for: 5m\n        labels:\n          severity: warning\n          team: ml-ops\n        annotations:\n          summary: \"High data drift detected\"\n          description: \"Average drift score {{{{ $value }}}} exceeds threshold\"\n          \n      - alert: ModelPerformanceDegradation\n        expr: model_performance{{namespace=\"{config.namespace}\"}} < {config.performance_threshold}\n        for: 10m\n        labels:\n          severity: critical\n          team: ml-ops\n        annotations:\n          summary: \"Model performance below threshold\"\n          description: \"Performance score {{{{ $value }}}} is below acceptable level\"\n          \n      - alert: BiasDetected\n        expr: min(fairness_score{{namespace=\"{config.namespace}\"}}) < 0.8\n        for: 15m\n        labels:\n          severity: warning\n          team: ml-ops\n        annotations:\n          summary: \"Potential bias detected in model predictions\"\n          description: \"Fairness score {{{{ $value }}}} indicates potential bias\"\n```\n\n### 5. **Grafana Dashboard Queries**\n\n```promql\n# Drift Score Trend\navg(drift_score{{namespace=\"{config.namespace}\"}}) by (model_name)\n\n# Explanation Request Rate\nrate(seldon_model_infer_total{{model_name=\"model-explainer\",namespace=\"{config.namespace}\"}}[5m])\n\n# Fairness Metrics by Group\navg(fairness_score{{namespace=\"{config.namespace}\"}}) by (protected_attribute, group)\n\n# Model Performance Over Time\navg_over_time(model_performance{{namespace=\"{config.namespace}\"}}[1h])\n\n# Anomaly Detection Rate\nsum(rate(anomaly_detected{{namespace=\"{config.namespace}\"}}[5m])) by (model_name)\n```\n\n### 6. **Integration with MLOps Pipeline**\n\n```python\nclass MLOpsIntegration:\n    def __init__(self):\n        self.monitoring_endpoint = \"http://{config.gateway_ip}:{config.gateway_port}\"\n        self.mlflow_tracking_uri = \"http://mlflow:5000\"\n        \n    def log_monitoring_metrics(self, run_id, metrics):\n        \\\"\\\"\\\"Log monitoring metrics to MLflow\\\"\\\"\\\"\n        import mlflow\n        \n        mlflow.set_tracking_uri(self.mlflow_tracking_uri)\n        \n        with mlflow.start_run(run_id=run_id):\n            mlflow.log_metric(\"avg_drift_score\", metrics[\"drift_score\"])\n            mlflow.log_metric(\"explanation_coverage\", metrics[\"explanation_coverage\"])\n            mlflow.log_metric(\"fairness_score\", metrics[\"fairness_score\"])\n            mlflow.log_metric(\"performance_score\", metrics[\"performance_score\"])\n            \n            # Log alerts\n            if metrics[\"drift_score\"] > {config.drift_threshold}:\n                mlflow.set_tag(\"alert\", \"high_drift\")\n            if metrics[\"fairness_score\"] < 0.8:\n                mlflow.set_tag(\"alert\", \"bias_detected\")\n```\n\"\"\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Production Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## üìã **Best Practices for Production Monitoring**\n",
    "\n",
    "### 1. **Drift Detection Strategy**\n",
    "- Set appropriate thresholds based on your data characteristics\n",
    "- Monitor both feature drift and prediction drift\n",
    "- Use sliding windows for baseline comparison\n",
    "- Implement graduated alerts (warning ‚Üí critical)\n",
    "\n",
    "### 2. **Explainability Implementation**\n",
    "- Generate explanations for edge cases and anomalies\n",
    "- Store explanations for regulatory compliance\n",
    "- Use explanations to improve model training\n",
    "- Balance explanation detail with performance\n",
    "\n",
    "### 3. **Performance Optimization**\n",
    "- Use async monitoring for non-critical paths\n",
    "- Batch explanation requests when possible\n",
    "- Cache explanations for repeated predictions\n",
    "- Scale monitoring components independently\n",
    "\n",
    "### 4. **Alert Configuration**\n",
    "```yaml\n",
    "# Example AlertManager configuration\n",
    "route:\n",
    "  group_by: ['alertname', 'cluster', 'service']\n",
    "  group_wait: 10s\n",
    "  group_interval: 10s\n",
    "  repeat_interval: 12h\n",
    "  receiver: 'ml-team'\n",
    "  routes:\n",
    "  - match:\n",
    "      alertname: HighDataDrift\n",
    "    receiver: ml-team-urgent\n",
    "  - match:\n",
    "      alertname: ModelPerformanceDegradation\n",
    "    receiver: ml-team-daily\n",
    "```\n",
    "\n",
    "### 5. **Dashboard Design**\n",
    "- **Real-time metrics**: Request rate, latency, error rate\n",
    "- **ML metrics**: Drift scores, explanation coverage, model confidence\n",
    "- **Business metrics**: Predictions by class, feature importance trends\n",
    "- **Infrastructure**: CPU/memory usage, queue depths, scaling events\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Production cleanup with resource management\nimport ipywidgets as widgets\nfrom IPython.display import display\n\ndef cleanup_monitoring_resources():\n    \"\"\"Clean up monitoring resources safely\"\"\"\n    log(\"Starting monitoring cleanup...\", \"INFO\")\n    \n    # Only clean up monitoring-specific resources\n    monitoring_resources = [\n        (\"pipeline\", [\"real-time-monitoring\", \"explanation-service\", \"fairness-monitoring\", \"comprehensive-monitoring\"]),\n        (\"model\", [\"drift-detector\", \"model-explainer\", \"performance-monitor\", \"bias-detector\"])\n    ]\n    \n    cleanup_count = 0\n    \n    for resource_type, items in monitoring_resources:\n        for item in items:\n            result = run(f\"kubectl delete {resource_type} {item} -n {config.namespace} --ignore-not-found=true --wait=false\")\n            if result.returncode == 0:\n                log(f\"Deleted {resource_type}: {item}\", \"SUCCESS\")\n                cleanup_count += 1\n    \n    # Clean up YAML files\n    import glob\n    yaml_files = glob.glob(\"*.yaml\")\n    for yaml_file in yaml_files:\n        if any(name in yaml_file for name in [\"drift\", \"explainer\", \"performance\", \"bias\", \"monitoring\", \"fairness\"]):\n            try:\n                os.remove(yaml_file)\n            except:\n                pass\n    \n    log(f\"Cleanup complete! Removed {cleanup_count} monitoring resources\", \"SUCCESS\")\n\n# Interactive cleanup interface\ncleanup_button = widgets.Button(\n    description=\"Clean Up Monitoring\",\n    button_style='danger',\n    tooltip='Remove monitoring resources',\n    icon='trash'\n)\n\nkeep_button = widgets.Button(\n    description=\"Keep Monitoring\",\n    button_style='success',\n    tooltip='Preserve monitoring setup',\n    icon='check'\n)\n\noutput = widgets.Output()\n\ndef on_cleanup_click(b):\n    with output:\n        output.clear_output()\n        cleanup_monitoring_resources()\n\ndef on_keep_click(b):\n    with output:\n        output.clear_output()\n        log(\"Monitoring resources preserved for production use\", \"SUCCESS\")\n        display(Markdown(f\"\"\"\n### üìå **Monitoring Resources Preserved**\n\n**Active Monitoring Components:**\n- üîç **Drift Detector**: Real-time data quality monitoring\n- üéØ **Model Explainer**: Compliance-ready explanations\n- üìä **Performance Monitor**: Model health tracking\n- ‚öñÔ∏è **Bias Detector**: Fairness monitoring\n\n**Monitoring Pipelines:**\n{chr(10).join(f\"- {pipeline}\" for pipeline in deployed['pipelines'])}\n\n**Production Commands:**\n```bash\n# View monitoring status\nkubectl get models -l app=data-science-monitoring -n {config.namespace}\nkubectl get pipelines -l app=data-science-monitoring -n {config.namespace}\n\n# Check metrics\nkubectl top pods -l app=data-science-monitoring -n {config.namespace}\n\n# Monitor with k9s\nk9s -n {config.namespace}\n```\n\n**Manual cleanup when ready:**\n```bash\n# Delete monitoring pipelines\nkubectl delete pipelines -l app=data-science-monitoring -n {config.namespace}\n\n# Delete monitoring models\nkubectl delete models -l component=monitoring-model -n {config.namespace}\n```\n\"\"\"))\n\ncleanup_button.on_click(on_cleanup_click)\nkeep_button.on_click(on_keep_click)\n\ndisplay(Markdown(\"### üßπ **Resource Management**\"))\ndisplay(widgets.HBox([keep_button, cleanup_button]))\ndisplay(output)\n\n# Final summary\ndisplay(Markdown(f\"\"\"\n## üéØ **Production Data Science Monitoring Summary**\n\nYou've successfully deployed a **production-grade ML monitoring platform** with:\n\n**üî¨ Monitoring Capabilities:**\n- ‚úÖ **Drift Detection**: {metrics.drift_detections} drift events detected\n- ‚úÖ **Model Explainability**: {metrics.explanations_generated} explanations generated\n- ‚úÖ **Performance Tracking**: Real-time model health monitoring\n- ‚úÖ **Fairness Monitoring**: Bias detection across segments\n\n**üìä Infrastructure Deployed:**\n- ü§ñ **{len([m for m in deployed['models'] if any(mon in m for mon in ['drift', 'explainer', 'performance', 'bias'])])} Monitoring Models**\n- üîó **{len(deployed['pipelines'])} Monitoring Pipelines**\n- üìà **Prometheus Metrics**: Exposed for all components\n- üö® **Alert Rules**: Configured for drift and performance\n\n**üöÄ Production Features:**\n- **Auto-remediation**: Trigger retraining on drift detection\n- **Compliance Ready**: Full audit trail with explanations\n- **Real-time Alerts**: Prometheus/AlertManager integration\n- **Fairness Tracking**: Demographic parity monitoring\n- **MLOps Integration**: Ready for CI/CD pipelines\n\n**üìã Next Steps:**\n1. **Configure Alerts**: Deploy AlertManager rules\n2. **Set Up Dashboards**: Import Grafana templates\n3. **Enable Auto-retraining**: Connect to ML pipelines\n4. **Schedule Reports**: Weekly compliance summaries\n5. **Monitor Fairness**: Track across user segments\n\n**üí° Try These Commands:**\n```python\n# Check for drift\nmonitoring_client.test_monitoring(\n    \"drift-detector\", \n    [[8.0, 6.0, 4.0, 2.0]]  # Anomalous data\n)\n\n# Get explanation\nmonitoring_client.test_monitoring(\n    \"model-explainer\",\n    [[6.5, 3.0, 5.5, 1.8]]  # Edge case\n)\n\n# Full monitoring pipeline\nmonitoring_client.test_monitoring(\n    \"comprehensive-monitoring\",\n    [[5.1, 3.5, 1.4, 0.2]],\n    is_pipeline=True\n)\n```\n\n**üèÜ Achievement Unlocked**: Production ML Monitoring Platform! üéâ\n\"\"\"))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}