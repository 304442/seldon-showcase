{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ LLM GPU Deployment with Seldon Core 2\n",
    "**Production-Ready Large Language Model Serving with GPU Acceleration**\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "This notebook demonstrates how to deploy and serve Large Language Models (LLMs) using Seldon Core 2 with GPU acceleration. We'll cover:\n",
    "\n",
    "- üöÄ **GPU Cluster Management**: Efficiently manage costly GPU resources\n",
    "- üß† **LLM Deployment Options**: API-based, local GPU, and optimized prompt runtime\n",
    "- ‚ö° **Performance Optimization**: Model caching and prompt runtime for faster inference\n",
    "- üí∞ **Cost Management**: Auto-scaling down GPU nodes when not in use\n",
    "- üîß **Production Patterns**: HPA + native server autoscaling\n",
    "\n",
    "## ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "- GCloud CLI configured with access to `dev-sherif` project\n",
    "- `kubectl` and `kubectx` installed\n",
    "- Access to GPU cluster: `llm-demos-alex`\n",
    "- Seldon Core 2 installed on the cluster\n",
    "\n",
    "## üí∏ Important: GPU Cost Management\n",
    "\n",
    "**GPU nodes are expensive!** Always:\n",
    "1. Scale up nodes only when needed\n",
    "2. Scale down immediately after use\n",
    "3. Monitor costs in GCP console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Cluster Management Scripts\n",
    "\n",
    "First, let's set up the cluster management functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from IPython.display import display, Markdown, Code\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class GPUClusterConfig:\n",
    "    \"\"\"GPU cluster configuration\"\"\"\n",
    "    cluster_name: str = \"llm-demos-alex\"\n",
    "    region: str = \"europe-west4\"\n",
    "    project: str = \"dev-sherif\"\n",
    "    context: str = \"gke_dev-sherif_europe-west4_llm-demos-alex\"\n",
    "    \n",
    "    # Node pool configurations\n",
    "    pool_1: str = \"pool-1\"  # Standard nodes\n",
    "    pool_4: str = \"pool-4\"  # GPU nodes (optional)\n",
    "    pool_7: str = \"pool-7\"  # GPU nodes (primary)\n",
    "    \n",
    "    # Sizes when scaled up\n",
    "    pool_1_size_up: int = 6\n",
    "    pool_4_size_up: int = 0  # Currently not used\n",
    "    pool_7_size_up: int = 1  # 1 GPU node\n",
    "    \n",
    "    # All pools scale to 0 when down\n",
    "    pool_size_down: int = 0\n",
    "\n",
    "config = GPUClusterConfig()\n",
    "\n",
    "def run_command(cmd: str, check: bool = True) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run command with proper error handling\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if check and result.returncode != 0:\n",
    "        print(f\"‚ùå Command failed: {cmd}\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    return result\n",
    "\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"Pretty logging\"\"\"\n",
    "    icons = {\"INFO\": \"‚ÑπÔ∏è\", \"SUCCESS\": \"‚úÖ\", \"WARNING\": \"‚ö†Ô∏è\", \"ERROR\": \"‚ùå\"}\n",
    "    colors = {\"INFO\": \"blue\", \"SUCCESS\": \"green\", \"WARNING\": \"orange\", \"ERROR\": \"red\"}\n",
    "    icon = icons.get(level, \"üìù\")\n",
    "    color = colors.get(level, \"black\")\n",
    "    display(Markdown(f\"<span style='color: {color}'>{icon} **{msg}**</span>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ GPU Cluster Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_cluster():\n",
    "    \"\"\"Connect to the GPU cluster\"\"\"\n",
    "    log(\"Connecting to GPU cluster...\", \"INFO\")\n",
    "    \n",
    "    # Get cluster credentials\n",
    "    cmd = f\"gcloud container clusters get-credentials {config.cluster_name} --region {config.region} --project {config.project}\"\n",
    "    result = run_command(cmd)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        log(\"Successfully connected to cluster\", \"SUCCESS\")\n",
    "        \n",
    "        # Switch context\n",
    "        result = run_command(f\"kubectl config use-context {config.context}\")\n",
    "        if result.returncode == 0:\n",
    "            log(f\"Switched to context: {config.context}\", \"SUCCESS\")\n",
    "        else:\n",
    "            log(\"Failed to switch context\", \"ERROR\")\n",
    "    else:\n",
    "        log(\"Failed to connect to cluster\", \"ERROR\")\n",
    "        log(\"Make sure you have access to the dev-sherif project\", \"WARNING\")\n",
    "\n",
    "def scale_gpu_up():\n",
    "    \"\"\"Scale up GPU nodes - COSTLY OPERATION!\"\"\"\n",
    "    log(\"‚ö†Ô∏è SCALING UP GPU NODES - THIS WILL INCUR COSTS!\", \"WARNING\")\n",
    "    \n",
    "    # Confirm context\n",
    "    current_context = run_command(\"kubectl config current-context\", check=False)\n",
    "    if config.context not in current_context.stdout:\n",
    "        log(f\"Wrong context! Expected {config.context}\", \"ERROR\")\n",
    "        return\n",
    "    \n",
    "    # Scale up pools\n",
    "    pools = [\n",
    "        (config.pool_1, config.pool_1_size_up, \"Standard nodes\"),\n",
    "        (config.pool_7, config.pool_7_size_up, \"GPU nodes\")\n",
    "    ]\n",
    "    \n",
    "    for pool_name, size, desc in pools:\n",
    "        if size > 0:\n",
    "            log(f\"Scaling {desc} ({pool_name}) to {size} nodes...\", \"INFO\")\n",
    "            cmd = f\"gcloud container clusters resize {config.cluster_name} --node-pool {pool_name} --num-nodes {size} --zone {config.region} --project {config.project} --quiet\"\n",
    "            result = run_command(cmd)\n",
    "            if result.returncode == 0:\n",
    "                log(f\"{desc} scaled to {size}\", \"SUCCESS\")\n",
    "            else:\n",
    "                log(f\"Failed to scale {desc}\", \"ERROR\")\n",
    "    \n",
    "    log(\"Waiting for nodes to be ready...\", \"INFO\")\n",
    "    time.sleep(60)\n",
    "    \n",
    "    # Check node status\n",
    "    result = run_command(\"kubectl get nodes | grep Ready | wc -l\")\n",
    "    ready_nodes = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 0\n",
    "    log(f\"Ready nodes: {ready_nodes}\", \"INFO\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    result = run_command(\"kubectl get nodes -o json | jq -r '.items[] | select(.status.allocatable[\\\"nvidia.com/gpu\\\"] != null) | .metadata.name'\")\n",
    "    if result.stdout.strip():\n",
    "        log(f\"GPU nodes available: {result.stdout.strip()}\", \"SUCCESS\")\n",
    "    else:\n",
    "        log(\"No GPU nodes found!\", \"WARNING\")\n",
    "\n",
    "def scale_gpu_down():\n",
    "    \"\"\"Scale down ALL nodes to save costs\"\"\"\n",
    "    log(\"üí∞ SCALING DOWN ALL NODES TO SAVE COSTS\", \"WARNING\")\n",
    "    \n",
    "    # Confirm context\n",
    "    current_context = run_command(\"kubectl config current-context\", check=False)\n",
    "    if config.context not in current_context.stdout:\n",
    "        log(f\"Wrong context! Expected {config.context}\", \"ERROR\")\n",
    "        return\n",
    "    \n",
    "    # Scale down all pools\n",
    "    pools = [config.pool_1, config.pool_4, config.pool_7]\n",
    "    \n",
    "    for pool_name in pools:\n",
    "        log(f\"Scaling down {pool_name} to 0 nodes...\", \"INFO\")\n",
    "        cmd = f\"gcloud container clusters resize {config.cluster_name} --node-pool {pool_name} --num-nodes 0 --zone {config.region} --project {config.project} --quiet\"\n",
    "        result = run_command(cmd)\n",
    "        if result.returncode == 0:\n",
    "            log(f\"{pool_name} scaled to 0\", \"SUCCESS\")\n",
    "        else:\n",
    "            log(f\"Failed to scale down {pool_name}\", \"ERROR\")\n",
    "    \n",
    "    log(\"All nodes scaled down - cluster is now cost-effective\", \"SUCCESS\")\n",
    "\n",
    "def check_cluster_status():\n",
    "    \"\"\"Check current cluster status\"\"\"\n",
    "    log(\"Checking cluster status...\", \"INFO\")\n",
    "    \n",
    "    # Check nodes\n",
    "    result = run_command(\"kubectl get nodes\")\n",
    "    if result.returncode == 0:\n",
    "        display(Code(result.stdout, language='text'))\n",
    "    \n",
    "    # Check GPU resources\n",
    "    result = run_command(\"kubectl describe nodes | grep -E 'nvidia.com/gpu|Allocatable:' -A 5 | grep nvidia\")\n",
    "    if result.stdout:\n",
    "        log(\"GPU resources found:\", \"SUCCESS\")\n",
    "        display(Code(result.stdout, language='text'))\n",
    "    else:\n",
    "        log(\"No GPU resources available\", \"WARNING\")\n",
    "\n",
    "# Create cluster management interface\n",
    "display(Markdown(\"\"\"\n",
    "### üéÆ Cluster Management Commands\n",
    "\n",
    "Run these cells in order:\n",
    "1. **Connect**: `connect_to_cluster()`\n",
    "2. **Scale Up**: `scale_gpu_up()` - ‚ö†Ô∏è INCURS COSTS\n",
    "3. **Check Status**: `check_cluster_status()`\n",
    "4. **Scale Down**: `scale_gpu_down()` - üí∞ SAVES COSTS\n",
    "\n",
    "**Remember**: Always scale down when finished!\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 1: Connect to GPU Cluster\n",
    "\n",
    "First, let's connect to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GPU cluster\n",
    "connect_to_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 2: Scale Up GPU Nodes (When Needed)\n",
    "\n",
    "**‚ö†Ô∏è WARNING**: This will incur GPU costs! Only run when you need to deploy LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO SCALE UP - THIS COSTS MONEY!\n",
    "# scale_gpu_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "check_cluster_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ LLM Deployment Options\n",
    "\n",
    "We'll demonstrate three approaches:\n",
    "1. **API-based Model** - No GPU required, uses external API\n",
    "2. **Local GPU Model** - Runs on GPU nodes\n",
    "3. **Optimized Prompt Runtime** - Cached model with prompt runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM deployment configuration\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    namespace: str = \"llm-demo\"\n",
    "    model_name: str = \"llama2-7b\"\n",
    "    api_model: str = \"gpt-3.5-turbo\"  # For API-based deployment\n",
    "    \n",
    "llm_config = LLMConfig()\n",
    "\n",
    "# Create namespace\n",
    "run_command(f\"kubectl create namespace {llm_config.namespace} --dry-run=client -o yaml | kubectl apply -f -\")\n",
    "run_command(f\"kubectl label namespace {llm_config.namespace} istio-injection=enabled --overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: API-Based LLM (No GPU Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy API-based LLM server\n",
    "api_server_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: llm-api-server\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  replicas: 2\n",
    "  serverConfig: mlserver\n",
    "  extraEnv:\n",
    "  - name: OPENAI_API_KEY\n",
    "    valueFrom:\n",
    "      secretKeyRef:\n",
    "        name: openai-secret\n",
    "        key: api-key\n",
    "\"\"\"\n",
    "\n",
    "# Deploy API-based model\n",
    "api_model_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: loan-approval-api\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  storageUri: gs://seldon-models/llm/loan-approval-api\n",
    "  requirements:\n",
    "  - openai\n",
    "  - langchain\n",
    "  memory: 2Gi\n",
    "  env:\n",
    "  - name: MODEL_TYPE\n",
    "    value: \"api\"\n",
    "  - name: API_MODEL\n",
    "    value: \"{llm_config.api_model}\"\n",
    "\"\"\"\n",
    "\n",
    "log(\"Deploying API-based LLM (no GPU required)...\", \"INFO\")\n",
    "\n",
    "# Save and apply configurations\n",
    "with open('/tmp/llm-api-server.yaml', 'w') as f:\n",
    "    f.write(api_server_yaml)\n",
    "with open('/tmp/llm-api-model.yaml', 'w') as f:\n",
    "    f.write(api_model_yaml)\n",
    "\n",
    "# Note: You need to create the secret with your API key\n",
    "display(Markdown(\"\"\"\n",
    "### üîë Create API Key Secret\n",
    "\n",
    "Before deploying, create a secret with your OpenAI API key:\n",
    "\n",
    "```bash\n",
    "kubectl create secret generic openai-secret \\\n",
    "  --from-literal=api-key=YOUR_OPENAI_API_KEY \\\n",
    "  -n llm-demo\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Local GPU LLM Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy GPU-based LLM server\n",
    "gpu_server_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: llm-gpu-server\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  replicas: 1\n",
    "  serverConfig: triton\n",
    "  resources:\n",
    "    requests:\n",
    "      nvidia.com/gpu: 1\n",
    "      memory: 16Gi\n",
    "      cpu: 4\n",
    "    limits:\n",
    "      nvidia.com/gpu: 1\n",
    "      memory: 32Gi\n",
    "      cpu: 8\n",
    "  nodeSelector:\n",
    "    cloud.google.com/gke-accelerator: \"nvidia-tesla-t4\"\n",
    "  tolerations:\n",
    "  - key: nvidia.com/gpu\n",
    "    operator: Exists\n",
    "    effect: NoSchedule\n",
    "\"\"\"\n",
    "\n",
    "# Deploy local GPU model\n",
    "gpu_model_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: llama2-7b-gpu\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  storageUri: gs://seldon-models/llm/llama2-7b-chat\n",
    "  requirements:\n",
    "  - transformers\n",
    "  - torch\n",
    "  - accelerate\n",
    "  memory: 16Gi\n",
    "  env:\n",
    "  - name: MODEL_TYPE\n",
    "    value: \"local\"\n",
    "  - name: LOAD_IN_8BIT\n",
    "    value: \"true\"\n",
    "  - name: DEVICE_MAP\n",
    "    value: \"auto\"\n",
    "\"\"\"\n",
    "\n",
    "log(\"Deploying GPU-based LLM (requires GPU nodes)...\", \"INFO\")\n",
    "\n",
    "# Save configurations\n",
    "with open('/tmp/llm-gpu-server.yaml', 'w') as f:\n",
    "    f.write(gpu_server_yaml)\n",
    "with open('/tmp/llm-gpu-model.yaml', 'w') as f:\n",
    "    f.write(gpu_model_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Optimized Prompt Runtime (Best Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy optimized prompt runtime\n",
    "prompt_runtime_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: llama2-prompt-runtime\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  storageUri: gs://seldon-models/llm/llama2-7b-chat\n",
    "  requirements:\n",
    "  - transformers\n",
    "  - torch\n",
    "  - accelerate\n",
    "  memory: 16Gi\n",
    "  runtime: prompt-runtime  # Special runtime for optimized prompt handling\n",
    "  env:\n",
    "  - name: MODEL_TYPE\n",
    "    value: \"prompt-optimized\"\n",
    "  - name: LOAD_IN_8BIT\n",
    "    value: \"true\"\n",
    "  - name: CACHE_MODEL\n",
    "    value: \"true\"\n",
    "  - name: MAX_BATCH_SIZE\n",
    "    value: \"8\"\n",
    "  - name: MAX_SEQUENCE_LENGTH\n",
    "    value: \"2048\"\n",
    "\"\"\"\n",
    "\n",
    "# Deploy loan approval pipeline with prompt runtime\n",
    "pipeline_yaml = f\"\"\"\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: loan-approval-pipeline\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  steps:\n",
    "  - name: prompt-builder\n",
    "    implementation: PROMPT_BUILDER\n",
    "    parameters:\n",
    "      template: |\n",
    "        You are a loan approval assistant. Based on the following application details, \n",
    "        provide a decision (APPROVED/DENIED) and explanation.\n",
    "        \n",
    "        Application Details:\n",
    "        {{application_details}}\n",
    "        \n",
    "        Decision:\n",
    "  - name: llama2-prompt-runtime\n",
    "    inputs: [prompt-builder.outputs]\n",
    "  - name: response-parser\n",
    "    implementation: RESPONSE_PARSER\n",
    "    inputs: [llama2-prompt-runtime.outputs]\n",
    "    parameters:\n",
    "      extract_fields:\n",
    "      - decision\n",
    "      - explanation\n",
    "  output:\n",
    "    steps: [response-parser]\n",
    "\"\"\"\n",
    "\n",
    "log(\"Deploying optimized prompt runtime pipeline...\", \"INFO\")\n",
    "\n",
    "with open('/tmp/prompt-runtime.yaml', 'w') as f:\n",
    "    f.write(prompt_runtime_yaml)\n",
    "with open('/tmp/loan-pipeline.yaml', 'w') as f:\n",
    "    f.write(pipeline_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Deploy Selected Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose deployment option\n",
    "deployment_option = \"api\"  # Change to \"gpu\" or \"prompt-runtime\" as needed\n",
    "\n",
    "if deployment_option == \"api\":\n",
    "    log(\"Deploying API-based solution (no GPU required)\", \"INFO\")\n",
    "    run_command(\"kubectl apply -f /tmp/llm-api-server.yaml\")\n",
    "    run_command(\"kubectl apply -f /tmp/llm-api-model.yaml\")\n",
    "    \n",
    "elif deployment_option == \"gpu\":\n",
    "    log(\"Deploying GPU-based solution\", \"INFO\")\n",
    "    run_command(\"kubectl apply -f /tmp/llm-gpu-server.yaml\")\n",
    "    run_command(\"kubectl apply -f /tmp/llm-gpu-model.yaml\")\n",
    "    \n",
    "elif deployment_option == \"prompt-runtime\":\n",
    "    log(\"Deploying prompt runtime solution\", \"INFO\")\n",
    "    run_command(\"kubectl apply -f /tmp/llm-gpu-server.yaml\")  # Still needs GPU server\n",
    "    run_command(\"kubectl apply -f /tmp/prompt-runtime.yaml\")\n",
    "    run_command(\"kubectl apply -f /tmp/loan-pipeline.yaml\")\n",
    "\n",
    "# Wait for deployment\n",
    "log(\"Waiting for deployment to be ready...\", \"INFO\")\n",
    "time.sleep(60)\n",
    "\n",
    "# Check deployment status\n",
    "run_command(f\"kubectl get all -n {llm_config.namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loan_approval(application_data: dict, model_name: str = \"loan-approval-api\"):\n",
    "    \"\"\"Test loan approval inference\"\"\"\n",
    "    \n",
    "    # Get gateway endpoint\n",
    "    result = run_command(\"kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\")\n",
    "    gateway_ip = result.stdout.strip() or \"localhost\"\n",
    "    \n",
    "    url = f\"http://{gateway_ip}/v2/models/{model_name}/infer\"\n",
    "    \n",
    "    # Prepare request\n",
    "    payload = {\n",
    "        \"inputs\": [{\n",
    "            \"name\": \"application\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [json.dumps(application_data)]\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Seldon-Model\": model_name\n",
    "    }\n",
    "    \n",
    "    log(f\"Testing loan approval with {model_name}...\", \"INFO\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            log(f\"Inference successful! Latency: {latency:.0f}ms\", \"SUCCESS\")\n",
    "            \n",
    "            # Extract decision\n",
    "            try:\n",
    "                outputs = result.get(\"outputs\", [{}])[0]\n",
    "                decision_data = outputs.get(\"data\", [{}])[0]\n",
    "                display(Markdown(f\"\"\"\n",
    "### üìã Loan Decision\n",
    "\n",
    "**Application**: {application_data.get('applicant_name', 'Unknown')}\n",
    "**Decision**: {decision_data.get('decision', 'PENDING')}\n",
    "**Explanation**: {decision_data.get('explanation', 'No explanation provided')}\n",
    "**Processing Time**: {latency:.0f}ms\n",
    "\"\"\"))\n",
    "            except:\n",
    "                display(Code(json.dumps(result, indent=2), language='json'))\n",
    "        else:\n",
    "            log(f\"Inference failed: {response.status_code}\", \"ERROR\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"Error during inference: {str(e)}\", \"ERROR\")\n",
    "\n",
    "# Test application\n",
    "test_application = {\n",
    "    \"applicant_name\": \"John Doe\",\n",
    "    \"annual_income\": 75000,\n",
    "    \"credit_score\": 720,\n",
    "    \"loan_amount\": 250000,\n",
    "    \"loan_purpose\": \"home_purchase\",\n",
    "    \"employment_years\": 5,\n",
    "    \"debt_to_income_ratio\": 0.35\n",
    "}\n",
    "\n",
    "# Test the deployment\n",
    "test_loan_approval(test_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Production Patterns: Model HPA + Server Native Autoscaling\n",
    "\n",
    "The recommended approach for production LLM deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production autoscaling configuration\n",
    "autoscaling_yaml = f\"\"\"\n",
    "# Model-level HPA\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-model-hpa\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: mlops.seldon.io/v1alpha1\n",
    "    kind: Model\n",
    "    name: llama2-7b-gpu\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 3\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: gpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: inference_queue_size\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"10\"\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 60\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Pods\n",
    "        value: 1\n",
    "        periodSeconds: 120\n",
    "---\n",
    "# Server native autoscaling\n",
    "apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Server\n",
    "metadata:\n",
    "  name: llm-autoscaling-server\n",
    "  namespace: {llm_config.namespace}\n",
    "spec:\n",
    "  replicas: 2\n",
    "  serverConfig: triton\n",
    "  autoscaling:\n",
    "    enabled: true\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 5\n",
    "    metrics:\n",
    "    - type: gpu\n",
    "      targetUtilization: 80\n",
    "    - type: memory\n",
    "      targetUtilization: 70\n",
    "  resources:\n",
    "    requests:\n",
    "      nvidia.com/gpu: 1\n",
    "      memory: 16Gi\n",
    "    limits:\n",
    "      nvidia.com/gpu: 1\n",
    "      memory: 32Gi\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üöÄ Production Autoscaling Strategy\n",
    "\n",
    "**Recommended Pattern**: Model HPA + Server Native Autoscaling\n",
    "\n",
    "1. **Model-level HPA**:\n",
    "   - Scales based on GPU utilization and queue size\n",
    "   - Fast scale-up (50% increase per minute)\n",
    "   - Conservative scale-down (1 pod every 2 minutes)\n",
    "\n",
    "2. **Server Native Autoscaling**:\n",
    "   - Built-in server scaling based on resource metrics\n",
    "   - Handles infrastructure-level scaling\n",
    "   - Works in tandem with model HPA\n",
    "\n",
    "3. **Benefits**:\n",
    "   - Cost-efficient GPU utilization\n",
    "   - Responsive to load changes\n",
    "   - Prevents resource starvation\n",
    "   - Smooth scaling behavior\n",
    "\"\"\"))\n",
    "\n",
    "# Save autoscaling config\n",
    "with open('/tmp/llm-autoscaling.yaml', 'w') as f:\n",
    "    f.write(autoscaling_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Monitoring LLM Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-specific monitoring queries\n",
    "monitoring_queries = {\n",
    "    \"GPU Utilization\": f'nvidia_gpu_utilization{{namespace=\"{llm_config.namespace}\"}}',\n",
    "    \"GPU Memory Usage\": f'nvidia_gpu_memory_used_bytes{{namespace=\"{llm_config.namespace}\"}} / nvidia_gpu_memory_total_bytes{{namespace=\"{llm_config.namespace}\"}} * 100',\n",
    "    \"Model Inference Latency P95\": f'histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\"{llm_config.namespace}\"}}[5m]))',\n",
    "    \"Token Generation Rate\": f'rate(llm_tokens_generated_total{{namespace=\"{llm_config.namespace}\"}}[5m])',\n",
    "    \"Queue Size\": f'inference_queue_size{{namespace=\"{llm_config.namespace}\"}}',\n",
    "    \"Active Requests\": f'llm_active_requests{{namespace=\"{llm_config.namespace}\"}}'\n",
    "}\n",
    "\n",
    "display(Markdown(\"### üìä LLM Monitoring Queries\"))\n",
    "for name, query in monitoring_queries.items():\n",
    "    display(Markdown(f\"**{name}**:\"))\n",
    "    display(Code(query, language='promql'))\n",
    "\n",
    "# Check current metrics\n",
    "def check_llm_metrics():\n",
    "    \"\"\"Check current LLM metrics\"\"\"\n",
    "    log(\"Checking LLM metrics...\", \"INFO\")\n",
    "    \n",
    "    # GPU metrics\n",
    "    result = run_command(\"kubectl top nodes | grep gpu\")\n",
    "    if result.stdout:\n",
    "        display(Markdown(\"### GPU Node Resources\"))\n",
    "        display(Code(result.stdout, language='text'))\n",
    "    \n",
    "    # Pod metrics\n",
    "    result = run_command(f\"kubectl top pods -n {llm_config.namespace}\")\n",
    "    if result.stdout:\n",
    "        display(Markdown(\"### Pod Resources\"))\n",
    "        display(Code(result.stdout, language='text'))\n",
    "\n",
    "check_llm_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Step 3: Scale Down GPU Nodes (IMPORTANT!)\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL**: Always scale down GPU nodes when finished to avoid unnecessary costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS RUN THIS WHEN FINISHED!\n",
    "scale_gpu_down()\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### ‚úÖ Cluster Scaled Down\n",
    "\n",
    "GPU nodes have been scaled to 0 to save costs.\n",
    "\n",
    "**Next time you need GPUs**:\n",
    "1. Run `connect_to_cluster()`\n",
    "2. Run `scale_gpu_up()`\n",
    "3. Deploy your models\n",
    "4. Run `scale_gpu_down()` when finished\n",
    "\n",
    "**Cost Tracking**: Check costs in [GCP Console](https://console.cloud.google.com/kubernetes/clusters/details/europe-west4/llm-demos-alex/nodes?project=dev-sherif)\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "### Demo Repository\n",
    "Full loan approval demo with all three approaches:\n",
    "https://github.com/SeldonIO/customer-success/tree/master/tutorials/llm-module/demos/loan-approval-decision-system\n",
    "\n",
    "### Best Practices\n",
    "1. **Always use API models for development** - No GPU costs\n",
    "2. **Test with small batches** before scaling up\n",
    "3. **Monitor GPU memory** - LLMs can OOM easily\n",
    "4. **Use quantization** (8-bit) to fit larger models\n",
    "5. **Implement request queuing** for burst handling\n",
    "6. **Set up alerts** for GPU utilization and costs\n",
    "\n",
    "### Troubleshooting\n",
    "- **GPU not available**: Check node pool status and tolerations\n",
    "- **OOM errors**: Reduce batch size or enable 8-bit quantization\n",
    "- **Slow inference**: Check GPU utilization and queue depth\n",
    "- **High costs**: Ensure nodes are scaled down when not in use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}