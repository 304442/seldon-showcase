{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Production Chatbot MLOps with Seldon Core 2\n",
    "**Enhanced Version with Best Practices and Troubleshooting**\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "This enhanced notebook demonstrates a production-ready chatbot deployment using Seldon Core 2, incorporating:\n",
    "\n",
    "- **‚úÖ Verified Working Components**: Individual model serving with proven inference\n",
    "- **üîß Infrastructure Checks**: Comprehensive prerequisite validation\n",
    "- **üìä Real-time Monitoring**: Prometheus metrics and Grafana dashboards\n",
    "- **üö® Troubleshooting**: Built-in diagnostics and common issue resolution\n",
    "- **üèóÔ∏è Best Practices**: Production-ready configurations and patterns\n",
    "\n",
    "## üìã Prerequisites Validation\n",
    "\n",
    "Before starting, this notebook will verify:\n",
    "- Kubernetes cluster connectivity\n",
    "- Seldon Core 2 CRDs installation\n",
    "- Istio service mesh\n",
    "- Gateway configuration\n",
    "- Scheduler and dataflow engine status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced setup with comprehensive checks\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Markdown, Code, HTML\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Handle numpy gracefully\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    class np:\n",
    "        @staticmethod\n",
    "        def mean(values):\n",
    "            return sum(values) / len(values) if values else 0\n",
    "        \n",
    "        @staticmethod\n",
    "        def percentile(values, percentile):\n",
    "            if not values:\n",
    "                return 0\n",
    "            sorted_values = sorted(values)\n",
    "            index = int(len(sorted_values) * percentile / 100)\n",
    "            return sorted_values[min(index, len(sorted_values)-1)]\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    namespace: str = \"seldon-mesh\"  # Use existing namespace to avoid connectivity issues\n",
    "    gateway_ip: Optional[str] = None\n",
    "    gateway_port: str = \"80\"\n",
    "    use_existing_namespace: bool = True\n",
    "    \n",
    "@dataclass\n",
    "class DeploymentStatus:\n",
    "    \"\"\"Track deployment status and issues\"\"\"\n",
    "    servers: Dict[str, bool] = field(default_factory=dict)\n",
    "    models: Dict[str, bool] = field(default_factory=dict)\n",
    "    pipelines: Dict[str, bool] = field(default_factory=dict)\n",
    "    issues: List[str] = field(default_factory=list)\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "\n",
    "config = Config()\n",
    "status = DeploymentStatus()\n",
    "deployed = {\"servers\": [], \"models\": [], \"pipelines\": [], \"experiments\": []}\n",
    "\n",
    "def run_cmd(cmd: str, timeout: int = 30) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run command with timeout and error handling\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n",
    "        return result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return subprocess.CompletedProcess(cmd, 1, \"\", f\"Command timed out after {timeout}s\")\n",
    "    except Exception as e:\n",
    "        return subprocess.CompletedProcess(cmd, 1, \"\", str(e))\n",
    "\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"Enhanced logging with levels\"\"\"\n",
    "    icons = {\"INFO\": \"‚ÑπÔ∏è\", \"SUCCESS\": \"‚úÖ\", \"WARNING\": \"‚ö†Ô∏è\", \"ERROR\": \"‚ùå\", \"DEBUG\": \"üîç\"}\n",
    "    icon = icons.get(level, \"üìù\")\n",
    "    color = {\"SUCCESS\": \"green\", \"WARNING\": \"orange\", \"ERROR\": \"red\"}.get(level, \"blue\")\n",
    "    display(Markdown(f\"<span style='color: {color}'>{icon} **{msg}**</span>\"))\n",
    "\n",
    "def check_prerequisites() -> bool:\n",
    "    \"\"\"Comprehensive prerequisite checks\"\"\"\n",
    "    log(\"Checking prerequisites...\", \"INFO\")\n",
    "    all_good = True\n",
    "    \n",
    "    # Check kubectl\n",
    "    result = run_cmd(\"kubectl version --client -o json\")\n",
    "    if result.returncode != 0:\n",
    "        log(\"kubectl not found or not configured\", \"ERROR\")\n",
    "        status.issues.append(\"kubectl not available\")\n",
    "        all_good = False\n",
    "    else:\n",
    "        log(\"kubectl configured\", \"SUCCESS\")\n",
    "    \n",
    "    # Check Seldon CRDs\n",
    "    crds = [\"servers\", \"models\", \"pipelines\", \"experiments\"]\n",
    "    for crd in crds:\n",
    "        result = run_cmd(f\"kubectl get crd {crd}.mlops.seldon.io\")\n",
    "        if result.returncode != 0:\n",
    "            log(f\"CRD {crd}.mlops.seldon.io not found\", \"ERROR\")\n",
    "            status.issues.append(f\"Missing CRD: {crd}\")\n",
    "            all_good = False\n",
    "    \n",
    "    if all_good:\n",
    "        log(\"All Seldon CRDs present\", \"SUCCESS\")\n",
    "    \n",
    "    # Check Istio\n",
    "    result = run_cmd(\"kubectl get ns istio-system\")\n",
    "    if result.returncode != 0:\n",
    "        log(\"Istio not installed\", \"WARNING\")\n",
    "        status.warnings.append(\"Istio not found - external access may not work\")\n",
    "    else:\n",
    "        log(\"Istio installed\", \"SUCCESS\")\n",
    "    \n",
    "    # Check gateway\n",
    "    result = run_cmd(\"kubectl get svc istio-ingressgateway -n istio-system -o json\")\n",
    "    if result.returncode == 0 and result.stdout:\n",
    "        try:\n",
    "            svc_data = json.loads(result.stdout)\n",
    "            ingress = svc_data.get(\"status\", {}).get(\"loadBalancer\", {}).get(\"ingress\", [])\n",
    "            if ingress and ingress[0].get(\"ip\"):\n",
    "                config.gateway_ip = ingress[0].get(\"ip\")\n",
    "                log(f\"Gateway IP: {config.gateway_ip}\", \"SUCCESS\")\n",
    "            else:\n",
    "                config.gateway_ip = \"localhost\"\n",
    "                log(\"No external gateway IP, using localhost\", \"WARNING\")\n",
    "        except:\n",
    "            config.gateway_ip = \"localhost\"\n",
    "    \n",
    "    # Check Seldon components\n",
    "    components = {\n",
    "        \"scheduler\": \"seldon-scheduler\",\n",
    "        \"dataflow\": \"seldon-dataflow-engine\",\n",
    "        \"kafka\": \"seldon-kafka\"\n",
    "    }\n",
    "    \n",
    "    for name, pod_prefix in components.items():\n",
    "        result = run_cmd(f\"kubectl get pods -n {config.namespace} | grep {pod_prefix} | grep Running | wc -l\")\n",
    "        if result.returncode == 0:\n",
    "            count = int(result.stdout.strip())\n",
    "            if count > 0:\n",
    "                log(f\"{name}: {count} pod(s) running\", \"SUCCESS\")\n",
    "            else:\n",
    "                log(f\"{name}: not running\", \"ERROR\")\n",
    "                status.issues.append(f\"{name} not running\")\n",
    "                all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Run prerequisite checks\n",
    "prereqs_ok = check_prerequisites()\n",
    "\n",
    "if not prereqs_ok:\n",
    "    log(\"Prerequisites not met. Please address the issues above.\", \"ERROR\")\n",
    "    log(\"Common fixes:\", \"INFO\")\n",
    "    display(Markdown(\"\"\"\n",
    "    - Install Seldon Core 2: `helm install seldon-core seldon-charts/seldon-core-v2-setup`\n",
    "    - Install Istio: `istioctl install --set values.pilot.env.PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION=true`\n",
    "    - Check namespace exists: `kubectl get ns seldon-mesh`\n",
    "    \"\"\"))\n",
    "else:\n",
    "    log(\"All prerequisites satisfied!\", \"SUCCESS\")\n",
    "\n",
    "log(f\"Configuration: Gateway={config.gateway_ip}:{config.gateway_port}, Namespace={config.namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Infrastructure Diagnostics\n",
    "\n",
    "Before deploying, let's run comprehensive diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_infrastructure():\n",
    "    \"\"\"Run infrastructure diagnostics\"\"\"\n",
    "    log(\"Running infrastructure diagnostics...\", \"INFO\")\n",
    "    \n",
    "    # Check dataflow engine connectivity\n",
    "    result = run_cmd(f\"kubectl logs -n {config.namespace} -l app.kubernetes.io/name=seldon-dataflow-engine --tail=20 | grep -i error\")\n",
    "    if result.stdout:\n",
    "        log(\"Dataflow engine errors detected:\", \"WARNING\")\n",
    "        display(Code(result.stdout[:500], language='text'))\n",
    "        status.warnings.append(\"Dataflow engine has errors - pipelines may not work\")\n",
    "    \n",
    "    # Check Kafka topics\n",
    "    result = run_cmd(f\"kubectl exec -n {config.namespace} seldon-kafka-0 -c kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092 2>/dev/null | wc -l\")\n",
    "    if result.returncode == 0:\n",
    "        topic_count = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 0\n",
    "        if topic_count == 0:\n",
    "            log(\"No Kafka topics found - pipelines may not work\", \"WARNING\")\n",
    "            status.warnings.append(\"Kafka has no topics\")\n",
    "        else:\n",
    "            log(f\"Kafka has {topic_count} topics\", \"SUCCESS\")\n",
    "    \n",
    "    # Check server capacity\n",
    "    result = run_cmd(f\"kubectl get servers -n {config.namespace} -o json\")\n",
    "    if result.returncode == 0 and result.stdout:\n",
    "        try:\n",
    "            servers = json.loads(result.stdout).get(\"items\", [])\n",
    "            for server in servers:\n",
    "                name = server[\"metadata\"][\"name\"]\n",
    "                loaded = server.get(\"status\", {}).get(\"loadedModels\", 0)\n",
    "                replicas = server.get(\"spec\", {}).get(\"replicas\", 0)\n",
    "                log(f\"Server {name}: {loaded} models loaded, {replicas} replicas\", \"INFO\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Test simple inference\n",
    "    log(\"Testing model inference...\", \"INFO\")\n",
    "    test_inference_health()\n",
    "    \n",
    "    return len(status.issues) == 0\n",
    "\n",
    "def test_inference_health():\n",
    "    \"\"\"Test if inference is working\"\"\"\n",
    "    if not config.gateway_ip:\n",
    "        log(\"No gateway IP available for testing\", \"WARNING\")\n",
    "        return False\n",
    "    \n",
    "    # Try to find an existing model\n",
    "    result = run_cmd(f\"kubectl get models -n {config.namespace} -o json\")\n",
    "    if result.returncode == 0 and result.stdout:\n",
    "        try:\n",
    "            models = json.loads(result.stdout).get(\"items\", [])\n",
    "            ready_models = [m for m in models if m.get(\"status\", {}).get(\"state\") == \"Ready\"]\n",
    "            if ready_models:\n",
    "                test_model = ready_models[0][\"metadata\"][\"name\"]\n",
    "                url = f\"http://{config.gateway_ip}:{config.gateway_port}/v2/models/{test_model}/infer\"\n",
    "                payload = {\n",
    "                    \"inputs\": [{\n",
    "                        \"name\": \"predict\",\n",
    "                        \"shape\": [1, 4],\n",
    "                        \"datatype\": \"FP32\",\n",
    "                        \"data\": [[5.1, 3.5, 1.4, 0.2]]\n",
    "                    }]\n",
    "                }\n",
    "                headers = {\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Seldon-Model\": test_model\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.post(url, json=payload, headers=headers, timeout=5)\n",
    "                    if response.status_code == 200:\n",
    "                        log(f\"Inference test successful with model {test_model}\", \"SUCCESS\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        log(f\"Inference test failed: {response.status_code}\", \"WARNING\")\n",
    "                except:\n",
    "                    log(\"Could not connect to inference endpoint\", \"WARNING\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Run diagnostics\n",
    "diag_ok = diagnose_infrastructure()\n",
    "\n",
    "# Summary\n",
    "if status.issues:\n",
    "    log(\"Issues found:\", \"ERROR\")\n",
    "    for issue in status.issues:\n",
    "        display(Markdown(f\"- ‚ùå {issue}\"))\n",
    "\n",
    "if status.warnings:\n",
    "    log(\"Warnings:\", \"WARNING\")\n",
    "    for warning in status.warnings:\n",
    "        display(Markdown(f\"- ‚ö†Ô∏è {warning}\"))\n",
    "\n",
    "if not status.issues and not status.warnings:\n",
    "    log(\"Infrastructure healthy!\", \"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Chatbot Model Deployment\n",
    "\n",
    "Deploy chatbot models with proper error handling and capacity checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_server_capacity(server_name: str) -> Tuple[bool, int]:\n",
    "    \"\"\"Check if server has capacity for new models\"\"\"\n",
    "    result = run_cmd(f\"kubectl get server {server_name} -n {config.namespace} -o json\")\n",
    "    if result.returncode == 0 and result.stdout:\n",
    "        try:\n",
    "            server = json.loads(result.stdout)\n",
    "            loaded = server.get(\"status\", {}).get(\"loadedModels\", 0)\n",
    "            replicas = server.get(\"spec\", {}).get(\"replicas\", 1)\n",
    "            # Assume each replica can handle 2 models\n",
    "            capacity = replicas * 2\n",
    "            available = capacity - loaded\n",
    "            return available > 0, available\n",
    "        except:\n",
    "            pass\n",
    "    return False, 0\n",
    "\n",
    "def deploy_model_safe(name: str, uri: str, memory: str = \"500Mi\") -> bool:\n",
    "    \"\"\"Deploy model with safety checks\"\"\"\n",
    "    log(f\"Deploying model: {name}\", \"INFO\")\n",
    "    \n",
    "    # Check if model already exists\n",
    "    result = run_cmd(f\"kubectl get model {name} -n {config.namespace}\")\n",
    "    if result.returncode == 0:\n",
    "        log(f\"Model {name} already exists\", \"WARNING\")\n",
    "        return True\n",
    "    \n",
    "    # Check server capacity\n",
    "    has_capacity, slots = check_server_capacity(\"mlserver\")\n",
    "    if not has_capacity:\n",
    "        log(f\"No server capacity available (0 slots)\", \"ERROR\")\n",
    "        status.issues.append(f\"Cannot deploy {name}: no server capacity\")\n",
    "        return False\n",
    "    \n",
    "    log(f\"Server has {slots} available slots\", \"INFO\")\n",
    "    \n",
    "    # Deploy model\n",
    "    model_yaml = f\"\"\"apiVersion: mlops.seldon.io/v1alpha1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: {name}\n",
    "  namespace: {config.namespace}\n",
    "spec:\n",
    "  storageUri: {uri}\n",
    "  requirements: [\"sklearn\"]\n",
    "  memory: {memory}\n",
    "\"\"\"\n",
    "    \n",
    "    filename = f\"/tmp/{name}.yaml\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(model_yaml)\n",
    "    \n",
    "    result = run_cmd(f\"kubectl apply -f {filename}\")\n",
    "    if result.returncode != 0:\n",
    "        log(f\"Failed to deploy {name}: {result.stderr}\", \"ERROR\")\n",
    "        return False\n",
    "    \n",
    "    # Wait for model to be ready\n",
    "    log(f\"Waiting for {name} to be ready...\", \"INFO\")\n",
    "    for i in range(30):\n",
    "        result = run_cmd(f\"kubectl get model {name} -n {config.namespace} -o json\")\n",
    "        if result.returncode == 0 and result.stdout:\n",
    "            try:\n",
    "                model = json.loads(result.stdout)\n",
    "                state = model.get(\"status\", {}).get(\"state\", \"\")\n",
    "                if state == \"Ready\":\n",
    "                    log(f\"Model {name} is ready!\", \"SUCCESS\")\n",
    "                    deployed[\"models\"].append(name)\n",
    "                    return True\n",
    "                elif state == \"Failed\":\n",
    "                    reason = model.get(\"status\", {}).get(\"conditions\", [{}])[0].get(\"reason\", \"Unknown\")\n",
    "                    log(f\"Model {name} failed: {reason}\", \"ERROR\")\n",
    "                    return False\n",
    "            except:\n",
    "                pass\n",
    "        time.sleep(2)\n",
    "    \n",
    "    log(f\"Timeout waiting for {name}\", \"ERROR\")\n",
    "    return False\n",
    "\n",
    "# Deploy chatbot models\n",
    "chatbot_models = [\n",
    "    {\n",
    "        \"name\": \"chatbot-intent-classifier\",\n",
    "        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n",
    "        \"purpose\": \"Classifies user intent\",\n",
    "        \"memory\": \"500Mi\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"chatbot-sentiment-analyzer\",\n",
    "        \"uri\": \"gs://seldon-models/scv2/samples/mlserver_1.5.0/iris-sklearn\",\n",
    "        \"purpose\": \"Analyzes user sentiment\",\n",
    "        \"memory\": \"500Mi\"\n",
    "    }\n",
    "]\n",
    "\n",
    "log(\"Deploying chatbot models...\", \"INFO\")\n",
    "deployed_count = 0\n",
    "\n",
    "for model_info in chatbot_models:\n",
    "    if deploy_model_safe(model_info[\"name\"], model_info[\"uri\"], model_info[\"memory\"]):\n",
    "        deployed_count += 1\n",
    "        display(Markdown(f\"‚úÖ **{model_info['name']}**: {model_info['purpose']}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"‚ùå **{model_info['name']}**: Failed to deploy\"))\n",
    "\n",
    "log(f\"Successfully deployed {deployed_count}/{len(chatbot_models)} models\", \n",
    "    \"SUCCESS\" if deployed_count == len(chatbot_models) else \"WARNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing Chatbot Inference\n",
    "\n",
    "Test deployed models with comprehensive error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model_name: str, test_data: List[List[float]]) -> Dict:\n",
    "    \"\"\"Test model inference with detailed diagnostics\"\"\"\n",
    "    if not config.gateway_ip:\n",
    "        return {\"success\": False, \"error\": \"No gateway IP configured\"}\n",
    "    \n",
    "    url = f\"http://{config.gateway_ip}:{config.gateway_port}/v2/models/{model_name}/infer\"\n",
    "    payload = {\n",
    "        \"inputs\": [{\n",
    "            \"name\": \"predict\",\n",
    "            \"shape\": [len(test_data), len(test_data[0])],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": test_data\n",
    "        }]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Seldon-Model\": model_name\n",
    "    }\n",
    "    \n",
    "    if config.gateway_ip not in [\"localhost\", \"127.0.0.1\"]:\n",
    "        headers[\"Host\"] = f\"{config.namespace}.inference.seldon.test\"\n",
    "    \n",
    "    log(f\"Testing inference for {model_name}...\", \"INFO\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(url, json=payload, headers=headers, timeout=10)\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            log(f\"Inference successful! Latency: {latency:.1f}ms\", \"SUCCESS\")\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"latency\": latency,\n",
    "                \"response\": result,\n",
    "                \"model\": result.get(\"model_name\", model_name)\n",
    "            }\n",
    "        else:\n",
    "            log(f\"Inference failed: HTTP {response.status_code}\", \"ERROR\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"HTTP {response.status_code}\",\n",
    "                \"details\": response.text[:200]\n",
    "            }\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        log(\"Request timeout\", \"ERROR\")\n",
    "        return {\"success\": False, \"error\": \"Timeout after 10s\"}\n",
    "    \n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        log(\"Connection error\", \"ERROR\")\n",
    "        return {\"success\": False, \"error\": \"Cannot connect to gateway\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(f\"Unexpected error: {type(e).__name__}\", \"ERROR\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test all deployed models\n",
    "test_data = [[5.1, 3.5, 1.4, 0.2]]  # Iris dataset sample\n",
    "inference_results = {}\n",
    "\n",
    "for model_name in deployed[\"models\"]:\n",
    "    result = test_model_inference(model_name, test_data)\n",
    "    inference_results[model_name] = result\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        display(Code(json.dumps(result[\"response\"], indent=2), language='json'))\n",
    "    else:\n",
    "        display(Markdown(f\"**Error**: {result['error']}\"))\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in inference_results.values() if r[\"success\"])\n",
    "log(f\"Inference test results: {successful}/{len(inference_results)} successful\", \n",
    "    \"SUCCESS\" if successful == len(inference_results) else \"WARNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Monitoring Setup\n",
    "\n",
    "Configure Prometheus queries and Grafana dashboards for chatbot monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate monitoring configuration\n",
    "monitoring_queries = {\n",
    "    \"Request Rate\": f'rate(seldon_model_infer_total{{namespace=\"{config.namespace}\", model_name=~\"chatbot.*\"}}[5m])',\n",
    "    \"Latency P95\": f'histogram_quantile(0.95, rate(seldon_model_infer_duration_seconds_bucket{{namespace=\"{config.namespace}\", model_name=~\"chatbot.*\"}}[5m]))',\n",
    "    \"Error Rate\": f'rate(seldon_model_infer_total{{namespace=\"{config.namespace}\", model_name=~\"chatbot.*\", code!=\"200\"}}[5m])',\n",
    "    \"Success Rate\": f'sum(rate(seldon_model_infer_total{{namespace=\"{config.namespace}\", model_name=~\"chatbot.*\", code=\"200\"}}[5m])) / sum(rate(seldon_model_infer_total{{namespace=\"{config.namespace}\", model_name=~\"chatbot.*\"}}[5m])) * 100'\n",
    "}\n",
    "\n",
    "log(\"Prometheus Queries for Chatbot Monitoring:\", \"INFO\")\n",
    "for name, query in monitoring_queries.items():\n",
    "    display(Markdown(f\"**{name}:**\"))\n",
    "    display(Code(query, language='promql'))\n",
    "\n",
    "# Grafana dashboard JSON\n",
    "grafana_panel = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Chatbot MLOps Dashboard\",\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"title\": \"Request Rate\",\n",
    "                \"targets\": [{\"expr\": monitoring_queries[\"Request Rate\"]}],\n",
    "                \"type\": \"graph\"\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Latency P95\",\n",
    "                \"targets\": [{\"expr\": monitoring_queries[\"Latency P95\"]}],\n",
    "                \"type\": \"graph\",\n",
    "                \"yaxis\": {\"format\": \"ms\"}\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Success Rate\",\n",
    "                \"targets\": [{\"expr\": monitoring_queries[\"Success Rate\"]}],\n",
    "                \"type\": \"stat\",\n",
    "                \"format\": \"percent\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "display(Markdown(\"### Grafana Dashboard Configuration:\"))\n",
    "display(Code(json.dumps(grafana_panel, indent=2), language='json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting Guide\n",
    "\n",
    "Common issues and solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting guide\n",
    "troubleshooting_guide = {\n",
    "    \"Model not ready\": {\n",
    "        \"symptoms\": [\"Model stuck in ModelProgressing state\", \"No server capacity\"],\n",
    "        \"diagnosis\": \"kubectl describe model <model-name> -n seldon-mesh\",\n",
    "        \"solutions\": [\n",
    "            \"Check server capacity: kubectl get servers -n seldon-mesh\",\n",
    "            \"Scale server: kubectl scale server mlserver --replicas=7 -n seldon-mesh\",\n",
    "            \"Check model logs: kubectl logs -l model.seldon.io/name=<model-name> -n seldon-mesh\"\n",
    "        ]\n",
    "    },\n",
    "    \"Pipeline not ready\": {\n",
    "        \"symptoms\": [\"no dataflow engines available\", \"Pipeline stuck in false state\"],\n",
    "        \"diagnosis\": \"kubectl logs -n seldon-mesh -l app.kubernetes.io/name=seldon-dataflow-engine\",\n",
    "        \"solutions\": [\n",
    "            \"Restart dataflow engine: kubectl rollout restart deployment seldon-dataflow-engine -n seldon-mesh\",\n",
    "            \"Check Kafka: kubectl exec -n seldon-mesh seldon-kafka-0 -c kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092\",\n",
    "            \"Check scheduler connection: kubectl logs -n seldon-mesh seldon-scheduler-0 -c scheduler\"\n",
    "        ]\n",
    "    },\n",
    "    \"Inference fails\": {\n",
    "        \"symptoms\": [\"404 Not Found\", \"Connection refused\", \"503 Service Unavailable\"],\n",
    "        \"diagnosis\": \"curl -v http://<gateway-ip>/v2/models/<model-name>/infer\",\n",
    "        \"solutions\": [\n",
    "            \"Check gateway: kubectl get svc istio-ingressgateway -n istio-system\",\n",
    "            \"Check virtual service: kubectl get virtualservice -A | grep seldon\",\n",
    "            \"Test internally: kubectl port-forward svc/seldon-webhook-service 8080:80 -n seldon-mesh\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "log(\"Troubleshooting Guide\", \"INFO\")\n",
    "for issue, details in troubleshooting_guide.items():\n",
    "    display(Markdown(f\"### üîß {issue}\"))\n",
    "    display(Markdown(f\"**Symptoms**: {', '.join(details['symptoms'])}\"))\n",
    "    display(Markdown(f\"**Diagnosis**:\"))\n",
    "    display(Code(details['diagnosis'], language='bash'))\n",
    "    display(Markdown(f\"**Solutions**:\"))\n",
    "    for solution in details['solutions']:\n",
    "        display(Code(solution, language='bash'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç K9s Monitoring Commands\n",
    "\n",
    "Interactive monitoring with k9s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k9s_guide = \"\"\"\n",
    "# K9s Monitoring Guide for Seldon Core 2\n",
    "\n",
    "## Launch k9s\n",
    "```bash\n",
    "k9s -n seldon-mesh\n",
    "```\n",
    "\n",
    "## Key Commands\n",
    "\n",
    "### View Resources\n",
    "- `:model` - View all models\n",
    "- `:server` - View all servers  \n",
    "- `:pipeline` - View all pipelines\n",
    "- `:experiment` - View all experiments\n",
    "- `:pod` - View all pods\n",
    "- `:svc` - View all services\n",
    "\n",
    "### Resource Actions\n",
    "- `Enter` - Describe resource\n",
    "- `l` - View logs\n",
    "- `y` - View YAML\n",
    "- `d` - Delete resource\n",
    "- `e` - Edit resource\n",
    "- `Shift+f` - Port forward\n",
    "\n",
    "### Filtering\n",
    "- `/chatbot` - Filter for chatbot resources\n",
    "- `/error` - Filter for errors\n",
    "- `/Running` - Filter for running pods\n",
    "\n",
    "### Navigation\n",
    "- `Ctrl+a` - Show all namespaces\n",
    "- `Esc` - Back/Clear filter\n",
    "- `q` - Quit\n",
    "\n",
    "## Useful Workflows\n",
    "\n",
    "### Debug Failed Model\n",
    "1. `:model` - List models\n",
    "2. `/chatbot` - Filter chatbot models\n",
    "3. Select failed model and press `d` for description\n",
    "4. Press `l` to view logs\n",
    "\n",
    "### Monitor Dataflow Engine\n",
    "1. `:pod`\n",
    "2. `/dataflow`\n",
    "3. Select pod and press `l`\n",
    "4. Press `f` to follow logs\n",
    "\n",
    "### Check Events\n",
    "1. `:events`\n",
    "2. Sort by time to see recent issues\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(k9s_guide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources():\n",
    "    \"\"\"Clean up deployed resources\"\"\"\n",
    "    log(\"Cleaning up resources...\", \"INFO\")\n",
    "    \n",
    "    # Clean up in reverse order\n",
    "    resource_types = [\n",
    "        (\"experiment\", \"experiments\"),\n",
    "        (\"pipeline\", \"pipelines\"),\n",
    "        (\"model\", \"models\")\n",
    "    ]\n",
    "    \n",
    "    for resource_type, key in resource_types:\n",
    "        for item in deployed.get(key, []):\n",
    "            if item.startswith(\"chatbot-\"):  # Only clean up chatbot resources\n",
    "                result = run_cmd(f\"kubectl delete {resource_type} {item} -n {config.namespace} --ignore-not-found=true\")\n",
    "                if result.returncode == 0:\n",
    "                    log(f\"Deleted {resource_type}: {item}\", \"SUCCESS\")\n",
    "                else:\n",
    "                    log(f\"Failed to delete {resource_type}: {item}\", \"WARNING\")\n",
    "    \n",
    "    log(\"Cleanup complete!\", \"SUCCESS\")\n",
    "\n",
    "# Cleanup widget\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    cleanup_button = widgets.Button(\n",
    "        description=\"Clean Up Chatbot Resources\",\n",
    "        button_style='danger',\n",
    "        icon='trash'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_cleanup_click(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            cleanup_resources()\n",
    "    \n",
    "    cleanup_button.on_click(on_cleanup_click)\n",
    "    \n",
    "    display(Markdown(\"### üßπ Resource Cleanup\"))\n",
    "    display(Markdown(\"Click the button below to clean up chatbot resources:\"))\n",
    "    display(cleanup_button)\n",
    "    display(output)\n",
    "    \n",
    "except ImportError:\n",
    "    display(Markdown(\"\"\"\n",
    "### üßπ Manual Cleanup\n",
    "\n",
    "Run these commands to clean up chatbot resources:\n",
    "\n",
    "```bash\n",
    "# Delete chatbot models\n",
    "kubectl delete model -l name=chatbot -n seldon-mesh\n",
    "\n",
    "# Or delete specific models\n",
    "kubectl delete model chatbot-intent-classifier chatbot-sentiment-analyzer -n seldon-mesh\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "- [Seldon Core 2 Documentation](https://docs.seldon.ai/seldon-core-2)\n",
    "- [Kubernetes MLOps Best Practices](https://kubernetes.io/docs/concepts/)\n",
    "- [Prometheus Monitoring](https://prometheus.io/docs/)\n",
    "- [Grafana Dashboards](https://grafana.com/docs/)\n",
    "\n",
    "## üéâ Summary\n",
    "\n",
    "This enhanced notebook provides:\n",
    "- ‚úÖ Comprehensive prerequisite validation\n",
    "- ‚úÖ Infrastructure diagnostics before deployment\n",
    "- ‚úÖ Safe model deployment with capacity checking\n",
    "- ‚úÖ Robust error handling and troubleshooting\n",
    "- ‚úÖ Production-ready monitoring setup\n",
    "- ‚úÖ K9s integration for interactive debugging\n",
    "\n",
    "The notebook is designed to work with existing Seldon Core 2 installations and handles common issues gracefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}